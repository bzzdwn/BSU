\part{Линейная алгебра}
\chapter{Векторное пространство}
\section{Определение и простейшие свойства векторного пространства.}
$\bullet$ \textit{Пусть $P$ --- некоторое поле. Непустое множество $V$ называется \textbf{векторным пространством} над полем $P$, если на $V$ задана бинарная операция $V\times V \to V$, обычно называемая \textbf{сложением}, и операция $P\times V \to V$, называемая \textbf{умножением элемента множества $V$ на элемент поля $P$}, для которых выполняются следующие условия:}
\begin{enumerate} 
	\item \textit{ $(V, +)$ --- абелева группа.}
	\item $\forall$ $\alpha$, $\beta$ $\in$ $P$, \textit{и} $\forall$ $a$, $b$ $\in$ $V$:
	\begin{itemize} 
		\item $a$ $\cdot$ 1 = $a$
		\item ($\alpha$ + $\beta$) $\cdot$ $a$ = $\alpha$ $\cdot$ $a$ + $\beta$ $\cdot$ $a$
		\item $\alpha$ $\cdot$ $(a + b)$ = $\alpha$ $\cdot$ $a$ + $\alpha$ $\cdot$ $b$
		\item $\alpha$ $\cdot$ ($\beta$ $\cdot$ $a$) = ($\alpha$ $\cdot$ $\beta$)  $\cdot$ $a$
	\end{itemize}
\end{enumerate}
\textit{При этом элементы множества $V$ называются \textbf{векторами}, а элементы поля $P$ --- \textbf{скалярами}.}\\\\
$\bullet$ \textit{Векторное пространство над полем $\mathbb{R}$ называется \textbf{действительным}, а над полем $\mathbb{C}$ --- \textbf{комплексным}.}\\\\
\textit{\textbf{Простейшие свойства векторных пространств:}}
\begin{enumerate}
	\item \textit{Свойства абелевой группы:}\begin{itemize} 
		\item В $V$ $\exists!$ $0_v$ и $\forall a\in V\ \exists!\ (-a)$ ;
		\item $a+x=b$ имеет единственное решение $b+(-a) = b-a = x$;
		\item $a+c = b+c \Rightarrow a=b$.
	\end{itemize}
	\item $0_p$ $\cdot$ $a$ = $0_v$, $\forall$ $a$ $\in$ $V$, 
	\\$\alpha$ $\cdot$ $0_v$ = $0_v$, $\forall$ $\alpha$ $\in$ $P$.
	
	\begin{Proof}
		$0_p a + 0_p a = (0_p + 0_p) a = 0_p a + 0_v = 0_v$.
	\end{Proof}
	
	\item \textit{Если $\alpha$ $\cdot$ $a$ = $0_v$, то}
	$\left[ 
	\begin{gathered} 
		a = 0_v,\\
		\alpha = 0_p.\\
	\end{gathered} 
	\right.$
	\\\\
	\begin{Proof} Пусть $\alpha\in P,$ $\ne$ $0_p$ $\Rightarrow$ $\exists$ $\alpha^{-1}$ $\in$ $P:$ $\alpha$ $\cdot$ $\alpha^{-1}$ = 1
		\\ $a$ $\cdot$ 1 = ($\alpha^{-1}$ $\cdot$ $\alpha$) $\cdot$ $a$ = $\alpha^{-1}$ $\cdot$ ($\alpha$ $\cdot$ $a$) = $\alpha^{-1}$ $\cdot$ $0_v$ = $0_v$ \end{Proof}
	\item $\alpha(-a) = (-\alpha)a = -(\alpha a)$.
	
	\begin{Proof} $\alpha (-a) + \alpha a = \alpha (a + (-a)) = \alpha 0_v = 0_v\Rightarrow (-\alpha a)$ является противоположным для $(\alpha a)$. \end{Proof}
	
	\item $\alpha (a-b) = \alpha a - \alpha b$; \\ $a(\alpha - \beta) = \alpha a - \beta a$.
	
	\begin{Proof}
		$\alpha(a-b) = \alpha(a + (-b)) = \alpha a + \alpha (-b) = \alpha a + (- (\alpha b)) = \alpha a - \alpha b$.
	\end{Proof}
	
	\item \textit{$\forall\alpha,\beta\in P, \forall a \in V, \ne 0$\\Если $\alpha$ $\cdot$ $a$ = $\beta$ $\cdot$ $a$, $a$ $\ne$ $0_v$, то $\alpha$ = $\beta$;}
	\textit{\\ Если $\alpha$ $\cdot$ $a$ = $\alpha$ $\cdot$ $b$, $\alpha$ $\ne$ $0_v$, то $a = b$.}
	
	\begin{Proof} Пусть $\alpha a = \beta a \Rightarrow -\alpha a = -\beta a.\\0_v = \alpha a + (-\alpha a) = \alpha a + (-\beta a) = \alpha a - \beta a = (\alpha - \beta) a\Rightarrow \alpha - \beta = 0\Rightarrow \alpha = \beta$\end{Proof}
\end{enumerate}



















\section{Линейная зависимость и независимость системы векторов.}
Пусть $V$ --- векторное пространство над полем $P$. Пусть $\asys\in V$ --- некоторая конечная система векторов (то есть упорядоченная последовательность необязательно различных элементов из $V$).\\\\ $\bullet$ \textit{Пусть $\alpha_1, \dots, \alpha_n\in P$. Тогда вектор $\alpha_1a_1 + \ldots + \alpha_n a_n$ называется \textbf{линейной комбинацией} векторов системы $A$ с коэффициентами $\alpha_1, \dots, \alpha_n$}. \textit{Множество всех линейных комбинаций векторов системы $A$ называется \textbf{линейной оболочкой} системы $A$ (Обозначение: $L(A) = L(a_1, \dots, a_n)$)}.\\\\
$\bullet$ \textit{Если некоторый вектор $b$ является линейной комбинацией векторов системы $A$, то есть $b\in L(A)$, то говорят, что $b$ \textbf{линейно выражается} через систему $A$. \\Если каждый вектор из $B(b_1,\dots,b_n)$ линейно выражается через систему $А$, то система $B$ \textbf{линейно выражается} через систему $A$}.\\\\
$\bullet$ \textit{Если система $A$ линейно выражается через систему $B$ и наоборот, $B$ через $A$, то системы $A$ и $B$ \textbf{эквивалентны}. Обозначение: $A\sim B$.}
\newtheorem*{Th*}{Лемма}\begin{Th*} Если система векторов $\bsys$ линейно выражается через систему $\asys$, то $L(B)\subseteq L(A)$. \end{Th*}
\begin{Proof} Пусть $b$ --- произвольный вектор из $L(B)$. Тогда $b$ представим в виде: $b = \beta_1 b_1 +\beta_2 b_2 +\dots + \beta_k b_k$. Так как $B$ линейно выражается через $A$, то каждый вектор из $B$ представим в виде:
	\begin{center}
		\begin{equation*}
			\begin{cases}
				b_1 = \alpha_{11}a_1 + \ldots + \alpha_{1n}a_n,
				\\
				b_2 = \alpha_{21}a_1 + \ldots + \alpha_{2n}a_n,
				\\
				\dotfill
				\\
				b_k = \alpha_{k1}a_1 + \ldots + \alpha_{kn}a_n;
				\\
			\end{cases}
		\end{equation*}
	\end{center}
	Следовательно, подставим: $b = \beta_1(\alpha_{11} a_1 + \ldots + \alpha_{1n} a_n) + \beta_2(\alpha_{21} a_1 + \ldots + \alpha_{2n} a_n) + \ldots + \beta_k (\alpha_{k1} a_1 + \ldots + \alpha_{kn} a_n) = a_1 (\alpha_{11} \beta_1 + \ldots + \alpha_{k1} \beta_k) + a_2 (\alpha_{12} \beta_1 + \ldots + \alpha_{k2} \beta_k) + \ldots + a_n (\alpha_{1n} \beta_1 + \ldots + \alpha_{kn} \beta_k)$ --- линейная комбинация $\Rightarrow b\in L(A)\Rightarrow L(B) \subseteq L(A).$ $\quad$
\end{Proof}
\newtheorem*{Cor1*}{Следствие}\begin{Cor1*} Системы $A \sim B \Longleftrightarrow L(A) = L(B)$.
\end{Cor1*}
\newtheorem*{Cor2*}{Следствие}\begin{Cor2*}Если вектор $b\in A$ линейно выражается через остальные векторы системы $A$, то $L(A) = L(A\backslash \{b\})$.\end{Cor2*}
$\bullet$ \textit{Линейная комбинация $\alpha_1 a_1 + \ldots + \alpha_n a_n$  называется \textbf{тривиальной}, если все коэффициенты $\alpha_i$ = 0, иначе \textbf{нетривиальной}.}\\\\
Тривиальная комбинация любых векторов всегда равна нулевому вектору.\\\\
$\bullet$ \textit{Если существует нетривиальная линейная комбинация векторов системы $A$, равная нулевому вектору, то такая система векторов называется \textbf{линейно зависимой}.} \\\\То есть конечная система векторов $A$ является линейно зависимой, если существуют скаляры $\alpha_1,\dots,\alpha_n$, не обращающиеся в нуль одновременно, такие, что $\alpha_1a_1 + \ldots + \alpha_n a_n = 0_v$.\\\\
$\bullet$ \textit{Если только тривиальная линейная комбинация векторов равна нулевому вектору, то система векторов называется \textbf{линейно независимой}.}\\\\
То есть система линейно независима, если из $\alpha_1 a_1 + \ldots + \alpha_n a_n$ следует, что $\alpha_1 = \alpha_2 = \ldots = \alpha_n = 0_p$.\\\\
\textit{\textbf{Свойства линейной зависимости и независимости системы векторов:}}
\begin{enumerate}
	\item \textit{Если система векторов линейно независима, то любая её подсистема также линейно независима. Если какая-либо подсистема системы векторов линейно зависимая, то и вся система также линейно зависима.}
	
	\begin{Proof}
		Пусть подсистема $B_1(a_1,\dots,a_k)$ системы $\asys$ линейно зависима. Тогда существует нетривиальная линейная комбинация $\alpha_1 a_1 + \ldots + \alpha_k a_k = 0_v$. Следовательно, линейная комбинация $\alpha_1 a_1 + \ldots + \alpha_k a_k + 0\cdot a_{k+1} + \ldots + 0\cdot a_n$ является нетривиальной и равной нулевому вектору.
	\end{Proof}
	\item \begin{enumerate}
		\item \textit{Система, состоящая из одного вектора, линейно зависимая $\Longleftrightarrow$ этот вектор нулевой.}
		\item \textit{Система, состоящая более чем из одного вектора, линейно зависима $\Longleftrightarrow$ один из векторов этой системы линейно выражается через} другие.
	\end{enumerate} 
	
	
	\begin{Proof} \begin{enumerate} \item $\Rightarrow$) Пусть система $А$ линейно зависима. Тогда $\exists\ \alpha\in P, \ne 0: \alpha a = 0_v \Rightarrow a = 0_v$.\\
			$\Leftarrow$) Пусть $A$ = ($0_v$) $\Rightarrow \forall \alpha_1\in P, \ne 0\  \alpha_1 0_v = 0_v \Rightarrow $ система $А$ линейно зависимая.
			\item $\Rightarrow$) Пусть $A = (a_1, \dots, a_n)$ линейно зависима. То есть существует нетривиальная линейная комбинация $ \alpha_1 a_1 + \ldots + \alpha_n a_n = 0_v$. Пусть $\exists \alpha_k \in P, \ne 0 \Rightarrow \exists \alpha_k^{-1}: a_k = -\dfrac{\alpha_1}{\alpha_k} a_1 - \ldots - \dfrac{\alpha_{k-1}} {\alpha_k} a_{k-1} - \dfrac{\alpha_{k+1}}{\alpha_k}a_{k+1} - \ldots - \dfrac{\alpha_n}{\alpha_k} a_n\Rightarrow a_k$ линейно выражается через\\\\ остальные векторы.\\
			$\Leftarrow$) Пусть $a_k$ линейно выражается через остальные векторы системы $A$. Тогда $a_k$ имеет вид:\\ $a_k= \alpha_1 a_1 + \ldots + \alpha_{k-1} a_{k-1} + \alpha_{k+1} a_{k+1} + \ldots + \alpha_n a_n$. Тогда линейная комбинация $\alpha_1 a_1 + \ldots + (-1)\cdot a_k + \ldots + \alpha_n a_n$ является нетривиальной и равной $0_v$. Следовательно, $A$ линейно зависимая.
		\end{enumerate}
	\end{Proof}
	\newtheorem*{Cor3*}{Следствие} \begin{Cor3*} Система, содержащая нулевой вектор, всегда линейно зависимая. \end{Cor3*} 
	
	\newtheorem*{Cor4*}{Следствие} \begin{Cor4*} Система, содержащая равные векторы, всегда линейно зависима. \end{Cor4*}
	
	\item \textit{Если система $\asys$ линейно независимая, а система векторов $B(a_1, \dots, a_n, b)$ линейно зависимая, то вектор $b$ линейно выражается через систему $A$.}
	
	\begin{Proof} Так как $B$ линейно зависима, то $\exists\ \alpha_1,\dots,\alpha_n,\beta$, не обращающиеся в нуль одновременно, такие, что $\alpha_1a_1 + \ldots + \alpha_n a_n + \beta b = 0_v$.
		Если $\beta = 0$, то линейная комбинация $\alpha_1a_1 + \ldots + \alpha_n a_n = 0_v$ является нетривиальной. Следовательно, то, что система $A$ линейно зависимая, --- противоречие. Значит $\beta \ne 0 \Rightarrow b = \dfrac{\alpha_1}{\beta}a_1 + \ldots + \dfrac{\alpha_n}{\beta}a_n$.
	\end{Proof}
\end{enumerate}












\section{Базис и размерность векторных пространств.}
$\bullet$ \textit{Бесконечная система векторов называется \textbf{линейно независимой}, если независима любая её конечная подсистема, и \textbf{линейно зависимой}, если существует конечная зависимая подсистема. Векторное пространство называется \textbf{бесконечномерным}, если в нём существует бесконечная линейно независимая система векторов. И \textbf{конечномерным}, если в ней все линейно независимые системы векторов конечны.}\\\\
$\bullet$ \textit{Система векторов $\bsys$ векторного пространства $V$ называется \textbf{базисом пространства} $V$, если:}
\begin{itemize}
	\item \textit{$B$ --- линейно независимая система.}
	\item \textit{Линейная оболочка $L(B)$ = $V$, то есть любой вектор из $V$ линейно выражается через систему $B$.}
\end{itemize}
В курсе линейной алгебры изучаются только конечномерные векторные пространства.
\newtheorem*{Th3*}{Теорема} \begin{Th3*} В конечномерном пространстве линейно независимая система векторов либо является базисом, либо может быть дополнена до базиса. \end{Th3*}
\begin{Proof}
	Пусть $\asys$ --- линейно независимая система векторов. Если $A$ не базис, то $\exists$ вектор $b$, который не выражается через $A$.\\
	Рассмотрим систему ($a_1$, \dots, $a_n$, $b$). Если она линейно зависимая, то по свойствам линейной зависимости $b$ выражается через $A$, что является противоречием. Следовательно, ($a_1$, \dots, $a_n$, $b$) линейно независимая.\\ Если система $A$ не является базисом, то существует такой вектор, присоединив который к этой системе, мы получим линейно независимую систему векторов. Так как пространство конечномерно, то бесконечно присоединять векторы к системе с сохранением линейной независимости невозможно. Следовательно, через конечное число шагов процесс остановится, и мы получим линейно независимую систему, через которую выражается любой вектор пространства, то есть базис. При этом $A$ --- его подсистема.
\end{Proof}
\newtheorem*{Cor5*}{Следствие} \begin{Cor5*} В ненулевом конечномерном векторном пространстве существует конечный базис. \end{Cor5*}
\begin{Proof}
	Если $a \neq 0_v$, то система векторов $A$ линейно независима и может быть дополнена до базиса.
\end{Proof}
\newtheorem*{Th2*}{Теорема}\begin{Th2*} Все базисы ненулевого конечномерного векторного пространства состоят из одного и того же числа векторов. \end{Th2*}
\begin{Proof}
	Пусть $A$($a_1$, \dots, $a_k$), $\bsys$ --- базисы пространства $V$ и пусть $k\geqslant n$.\\\\
	Рассмотрим систему $(a_1, b_1, \dots, b_n)$. Так как $B$ --- базис, то вектор $a_1$ линейно выражается через $B$. Тогда $L(a_1, b_1, \dots, b_n) = L(B) = V$.\\
	Так как $a_1$ линейно выражается через $B$, то $(a_1, b_1, \dots, b_n)$ линейно зависима, то есть существует нетривиальная линейная комбинация векторов этой системы равная нулевому вектору: $\alpha_1a_1 + \beta_1b_1 + \ldots + \beta_n b_n = 0_v$. Если все $b_i = 0$, то $\alpha_1a_1 = 0_v$. А так как $a_1$ --- вектор из базиса (значит ненулевой), то $\alpha_1 = 0$. Получаем противоречие с нетривиальностью.\\
	Следовательно, $\exists$ $\beta_i \neq 0$. Пусть $\beta_1 \neq 0$ (иначе $\beta_i$ можно перенумеровать). Тогда $b_1$ линейно выражается через $(a_1,b_2,\dots, b_n)$ $\Rightarrow$ $L$($a_1, b_1, b_2, \dots, b_n$) = $L$($a_1, b_2, \dots, b_n$) = $V$.\\
	Продолжая подобные рассуждения ещё $(n-1)$ раз, получим, что $L(a_1, b_2, \dots, b_n) =\\ L(a_1, a_2, \dots, b_n) = \ldots = L(a_1, a_2, \dots, a_n) = V\Rightarrow$ векторы $a_{n+1}, \dots, a_k$ линейно выражаются через векторы $a_1, \dots, a_n \Rightarrow$ система векторов $A$ линейно зависимая, что является противоречием с тем, что система $A$ --- базис $\Rightarrow$ $k = n.$
\end{Proof}\\\\
$\bullet$ \textit{Количество векторов в базисе конечномерного векторного пространства называется \textbf{размерностью векторного пространства} (Обозначение: $\dim V$). Векторное пространство размерности $n$ называется \textbf{n-мерным}. Нулевое пространство принято считать \textbf{нульмерным}.}
\newtheorem*{Th4*}{Теорема}\begin{Th4*} Пусть $V$ --- $n$-мерное векторное пространство. Тогда\begin{enumerate}
		\item система векторов, состоящая более чем из $n$ векторов, линейно заивисима.
		\item линейно независимая система векторов, состоящая из $n$ векторов, является базисом.
		\item линейно независимая система векторов, состоящая менее чем из $n$ векторов, может быть дополнена до базиса.
\end{enumerate} \end{Th4*}
\begin{Proof}
	Любая линейно независимая система векторов либо базис, либо может быть дополнена до базиса, но в $n$-мерном векторном пространстве все базисы состоят из $n$ векторов. Следовательно:\begin{enumerate}
		\item в пространстве $V$ нет линейно независимых систем векторов, состоящих более чем из $n$ векторов.
		\item если система содержит $n$ векторов, то любое её дополнение будет линейно зависимым.
		\item линейно независимая система векторов, состоящая из менее чем $n$ векторов, не является базисом.
\end{enumerate}\end{Proof}

Из этой теоремы следует, что базис --- максимально независимая система векторов.









\section{Координаты вектора. Изоморфизм векторных пространств.}
Пусть $V$ --- $n$-мерное векторное пространство над полем $P$, $\asys$ --- его базис. Тогда произвольный вектор $b\in V$ линейно выражается через систему $A$, то есть представим в виде линейной комбинации векторов системы $A$:\begin{center}
	$b = \alpha_1a_1 + \ldots + \alpha_na_n$, $\alpha_1,\dots,\alpha_n \in P.$
\end{center}
$\bullet$ \textit{Представление вектора $b$ в виде линейной комбинации векторов базиса $A$ называется \textbf{разложением} вектора $b$ по базису $A$, коэффициенты разложения $\alpha_1,\dots,\alpha_n$ называются \textbf{координатами} вектора $b$ в базисе $A$.}\\\\
Так как $A$ --- упорядоченная система векторов, то координаты вектора $b$ в базисе $A$ являются элементами пространства $P^n$.\\\\
$\bullet$ \textit{Столбец, составленный из координат вектора называется \textbf{координатным столбцом.}}\\
Обозначим $X_b = \begin{pmatrix}
	\alpha_1\\ \vdots \\ \alpha_n
\end{pmatrix}$ координатный столбец вектора $b$ в базисе $A$. Тогда разложение $b = \alpha_1a_1 + \ldots + \alpha_na_n$ может быть записано в виде: $b = AX_b$.
\newtheorem*{th4_1}{Теорема}\begin{th4_1} Координаты вектора в заданном базисе определяются однозначно.
\end{th4_1}
\begin{Proof}
	Пусть вектор $b$ имеет 2 разложения в базисе $A$: $b = \alpha_1a_1 + \ldots + \alpha_na_n = \beta_1a_1 + \ldots + \beta_na_n$. Отнимем от первого разложения второе и получим: 
	$0_v = (\alpha_1 - \beta_1)a_1 + \ldots + (\alpha_n - \beta_n)a_n$. Так как система $A$ является базисом, то $A$ линейно независима. Следовательно, она допускает лишь тривиальную линейную комбинацию: $\alpha_i - \beta_i = 0$ $\Rightarrow$ $\alpha_i = \beta_i$
\end{Proof}
\newtheorem*{th4_2}{Теорема}\begin{th4_2} Координатный столбец сумыы векторов равен сумме координатных стобцов слагаемых. При умножении вектора на скаляр его координатный столбец умножается на этот скаляр: $\forall a, b \in V$, $\forall \alpha \in P$\begin{center}
		$X_{a+b} = X_a + X_b$\\
		$X_{\alpha a} = \alpha X_a$.
	\end{center}
\end{th4_2}
\begin{Proof}
	Пусть $X_a=\begin{pmatrix} \alpha_1 \\ \vdots \\ \alpha_n \end{pmatrix}$, $X_b=\begin{pmatrix} \beta_1 \\ \vdots \\ \beta_n \end{pmatrix}$ --- координатные столбцы векторов $a$ и $b$ в базисе $A$. Тогда $\begin{cases} a = \alpha_1a_1 + \ldots + \alpha_n{a_n} \\ b = \beta_1b_1 + \ldots + \beta_n{b_n} \end{cases} \Rightarrow \begin{cases} a + b = (\alpha_1+\beta_1)a_1 + \ldots + (\alpha_n+\beta_n){a_n} \\ \alpha a = \alpha \alpha_1 a_1 + \ldots + \alpha \alpha_n a_n \end{cases} \Rightarrow\\ X_{a+b} = \begin{pmatrix} \alpha_1+\beta_1 \\ \vdots \\ \alpha_n+\beta_n \end{pmatrix} = X_a + X_b$;\\ $X_{\alpha a} = \begin{pmatrix} \alpha \alpha_1 \\ \vdots \\ \alpha \alpha_n \end{pmatrix} = \alpha X_a$.
\end{Proof} 
\newtheorem*{cor4_1}{Следствие}\begin{cor4_1} Векторы линейно зависимы $\Longleftrightarrow$ линейно зависимы их координатные столбцы.
\end{cor4_1} \begin{Proof}
	Если существует нетривиальная линейная комбинация векторов, равная $0_v$, то линейная комбинация координатных столбцов с теми же коэффициентами является нетривиальной и равной нулевому столбцу.
\end{Proof}\\\\
\underline{\textbf{Изоморфизм}}\\\\
$\bullet$ \textit{Пусть $V, U$ --- векторные пространства над одним полем $P$. Отображение
	$\varphi: V \rightarrow U$ называется изоморфизмом пространства $V$ на пространство $U$, если}\begin{enumerate}
	\item \textit{$\varphi$ --- биекция}
	\item $\varphi(a+b) = \varphi(a) + \varphi(b),\quad \forall\ a,b \in V$\\
	$\varphi(\alpha a) = \alpha \varphi(a),\quad\forall\ a\in V,\ \alpha \in P$.
\end{enumerate}
$\bullet$ \textit{Если существует изоморфизм пространства $V$ на пространство $U$, то $V$ называется \textbf{изоморфным} пространству $U$ (Обозначение: $V \cong U$)}.\\\\
\textbf{\textit{Свойство: }}\textit{Бинарное отношение изоморфности на множестве векторных пространств является отношением эквивалентности.}
\newtheorem*{th4_3}{Лемма}\begin{th4_3} Если $\dim  V = n \geqslant 1$, то $V \cong P^n$.
\end{th4_3}
\begin{Proof}
	Из первой и второй теорем параграфа следует, что отображение, ставящее в соответствие каждому вектору его координатный столбец в некотором базисе, является изоморфизмом.
\end{Proof}
\newtheorem*{th4_4}{Следствие (Критерий изоморфности)}\begin{th4_4} Конечномерные векторные пространства над одним и тем же полем изоморфны $\Longleftrightarrow$ их размерности равны.
\end{th4_4}
\begin{Proof} $\Rightarrow)$ Пусть $V \cong U$. Тогда $\exists$ $\varphi: V \rightarrow U$. Базис $V$ --- линейно независимая система векторов. Изоморфность $\varphi$ его отображает в линейно независимую систему $U$. Следовательно, число векторов в базисе $U$ не меньше, чем в базисе $V$, то есть $\dim V\leqslant \dim  U$. Обратное утверждение следует из того, что отображение $\varphi ^{-1}$ также является изоморфизмом.\\\\
	$\Leftarrow)$ Если $\dim U = \dim V=n$, то по лемме $U \cong P^n$ и $V \cong P^n$ $\Rightarrow$ $U \cong V$ по свойству изоморфизма.
\end{Proof} 












\section{Подпространство.}
Пусть $V$ --- векторное пространство над полем $P$.\\\\
$\bullet$ \textit{Подмножество $U$ пространства $V$ называется \textbf{подпространством} пространства $V$, если оно само является векторным пространством над тем же полем и относительно тех же операций, что и $V$.}
\newtheorem*{th5_1}{Теорема}\begin{th5_1}
	Непустое подмножество $U$ векторного пространства $V$ является подпространством $\Longleftrightarrow$ в $U$ определены те же операции, что и в $V$, то есть:
	\begin{enumerate}
		\item  $a + b \in U$, $\forall$ $a, b \in U$
		\item $\alpha a \in U$, $\forall$ $a \in U$, $\alpha \in P$
\end{enumerate}\end{th5_1}
\begin{Proof}
	$\Rightarrow)$ Если $U$ --- подпространство $V$, то оно само является векторным пространством, а значит в нём операции определены.\\
	$\Leftarrow)$ Пусть в $U$ определенены линейные операции. Тогда $\forall a\in U\ \exists (-a) = (-1)\cdot a\Rightarrow(-a)\in U$, а так как в $U$ определены операции сложения, то $(U,+)$ --- подгруппа $V \Rightarrow (U, +)$ --- абелева группа.\\
	Вторая группа аксиом определения векторного пространства справедлива для векторов из $U$, так как она справедлива для всех векторов из $V$.
\end{Proof}
\newtheorem*{cor5_1}{Следствие}\begin{cor5_1} Для любого подпространства $U$ векторного пространства $V$ $0_v\in U$
\end{cor5_1}
\begin{Proof}
	Так как $U$ --- подпространство, то $U$ непустое множество. Следовательно, $\exists\ a \in U:0_v = 0_p \cdot a \in U$
\end{Proof}
\newtheorem*{cor5_2}{Следствие}\begin{cor5_2}Для любого непустого множества $A\subset V$ линейная оболочка $L(A)$ является подпространством.\end{cor5_2}
\begin{Proof}
	Если $a, b \in L(A)$, то\\ $\begin{cases}
		a = \alpha_1 a_1 + \ldots + \alpha_n a_n\\
		b = \beta_1 a_1 + \ldots + \beta_n a_n
	\end{cases} \Rightarrow a+b = (\alpha_1 + \beta_1) a_1 + \ldots + (\alpha_n + \beta_n) a_n;$\\
	$\alpha a = \alpha \alpha_1 a_1 + \ldots + \alpha \alpha_n a_n$. Следовательно, $a+b,\ \alpha a \in L(A)$
\end{Proof}
\newtheorem*{cor5_3}{Следствие}\begin{cor5_3}Размерность линейной оболочки $L
	(A)$ системы векторов $A$ равна максимальному числу линейнонезависимых векторов в системе $A$\end{cor5_3}
\begin{Proof}
	Пусть $\asys$ имеет $k$ линейно независимых векторов $a_1,\dots,a_k$, а векторы $a_{k+1},\dots,a_n$ линейно заивисимые. Тогда $a_{k+1},\dots,a_n$ линейно выражаются через $a_1,\dots,a_k$. Следовательно, $L(A)$ совпадает с $L(a_1,\dots,a_k)$, а значит все векторы из $L(A)$ линейно выражаются через $a_1,\dots,a_k$, то есть $a_1,\dots,a_k$ --- линейно независимая подсистема, через которую выражаются все векторы из $L(A)$. Из этого можно сделать вывод, что подсистема $a_1,\dots,a_k$ --- базис $L(A)$. Следовательно $\dim L(A) = k$. Если $A$ линейно независима, то она сама является линейно независимой системой, через которую выражаются все векторы из $L(A)$, то есть базисом.
\end{Proof}
\newtheorem*{th5_2}{Теорема о монотонности размерности}\begin{th5_2}Для любого подпространства $U$ $n$-мерного векторного пространства $V$ справедливо неравенство $\dim U\leqslant n$, причем если $\dim U = n$, то $U=V$ \end{th5_2}
\begin{Proof}
	Базис $A(a_1,\dots,a_k)$ подпространства $U$ является линейно независимой системой векторов пространства $V$. Следовательно, $k = \dim  U \leqslant n$, так как $V$ не может быть линейно независимой системой, содержащей векторов больше, чем $n$. Если $\dim  U = n$, то $A$ --- базис $V$, а значит $V = L(A) = U$
\end{Proof}












\section{Ранг системы векторов.}
$\bullet$ \textit{ Пусть $A$ --- некоторая система векторов, $B$ --- её подсистема. Система $B$ называется \textbf{базисом системы $A$}, если}\begin{enumerate}
	\item \textit{система $B$ линейно независима;}
	\item \textit{каждый вектор системы $A$ линейно выражается через подсистему $B$.}
\end{enumerate}
\textit{\textbf{Свойства базиса систем векторов:}}\begin{enumerate}
	\item \textit{Если $B$ --- базис системы векторов $A$, то $B\sim A$.}\begin{Proof}
		Система $B$ линейно выражается через систему $A$, так как $B$ --- подсистема $A$. В свою очередь, $A$ линейно выражается через $B$, так как $B$ --- базис $A$.
	\end{Proof}
	\item \textit{Если $B$  --- базис системы векторов $A$, то $L(A) = L(B)$.}
	\item \textit{Базис системы векторов является базисом линейной оболочки этой системы векторов.}
	\item \textit{Все базисы системы векторов состоят из одного и того же числа векторов.}
\end{enumerate}
$\bullet$ \textit{Число векторов в базисе системы векторов называется \textbf{рангом} системы векторов (Обозначение: $\rank A$). Если система содержит лишь нулевые векторы, то ранг системы будем считать равным нулю.}\\\\
\textbf{\textit{Свойства ранга системы векторов:}}\begin{enumerate}
	\item \textit{Для любой системы векторов $\rank A = \dim  L(A)$.}
	\begin{Proof}
		По третьему свойству базисов системы векторов.
	\end{Proof}
	\item \textit{Если система векторов $B$ линейно выражается через систему векторов $A$, то $\rank B\leqslant \rank A$.}\begin{Proof}
		Если система векторов $B$ линейно выражается через систему $A$, то $L(B)$ содержится в $L(A)$. Тогда, по теореме о монотонности размерности, $\dim L(B)\leqslant \dim L(A)$. Следовательно, $\rank B\leqslant \rank A$
	\end{Proof}
\end{enumerate}
$\bullet$\textit{\textbf{ Элементарными преобразованиями} системы векторов называются:}\begin{enumerate}
	\item \textit{Домножение любого вектора системы на ненул. скаляр.}
	\item \textit{Сложение двух векторов системы, один из которых домножен на скаляр.}
\end{enumerate}
\newtheorem*{th6_1}{Теорема}\begin{th6_1}Элементарные преобразования системы векторов не меняют её ранг.\end{th6_1}
\begin{Proof} Пусть $A$ --- система векторов, и пусть система $B$ получена из $A$ заменой некоторого вектора $a_1$ на $\alpha a_1 \ne 0$, а cистема $C$ --- заменой $a_1$ на $a_1 + \alpha a_2$. Система $A$ линейно выражается через $B$, поскольку $a_1 = \alpha^{-1}(\alpha a_1)$, а $B$ --- через $A$. Следовательно, эти системы эквивалентны и их ранги равны. \\Система $C$ линейно выражается через $A$, а $A$ --- через $C$, поскольку $a_1 = (a_1 + \alpha a_2) + (-\alpha) a_2$. Следовательно, $A$ и $C$ эквивалентны и их ранги равны.
\end{Proof}




















\section{Ранг матрицы.}
Пусть  $A=(\alpha_{ij}) \in P_{m,n}$ --- $m\times n$-матрица над полем $P$, то есть
$$A= \begin{pmatrix} 
	\alpha_{11} & \dots & \alpha_{1n} 
	\\ \vdots & \ddots & \vdots
	\\ \alpha_{m1} & \dots & \alpha_{mn}
\end{pmatrix}$$ Обозначим $A_{i}=\begin{pmatrix} 
	\alpha_{1i}
	\\\vdots
	\\\alpha_{mi}
\end{pmatrix}$ --- $i$-ый столбец матрицы $A$.\\\\
$\bullet$ \textit{\textbf{Рангом} матрицы $A$ называется ранг системы столбцов матрицы $A$, то есть $\rank A = \rank (A_1,\dots,A_n)$}\\\\
$\bullet$ \textit{\textbf{Базисным минором} матрицы $A$ назыввается такой её минор $M$, для которого:}\begin{enumerate}
	\item $M\ne 0;$
	\item \textit{все миноры, порядок которых больше порядка минора $M$, равны нулю.}
\end{enumerate}
\newtheorem*{th7_1}{Теорема о базисном миноре}\begin{th7_1} Столбцы матрицы, в которых расположен базисный минор, образуют базис системы столбцов матриц. \end{th7_1}
\begin{Proof} Пусть $M$ --- базисный минор системы $A$. Так как порядок векторов в системе не влияет на зависимость, переупорядочим столбцы так, чтобы $M$ распологался в первых $k$-столбцах. Возьмём строки $i_1,\dots,i_k$. Таким образом $M$ имеет вид:\\
	$$M=\begin{vmatrix}
		\alpha_{i_11} & \dots & \alpha_{i_1k}
		\\ \vdots & \ddots & \vdots
		\\ \alpha_{i_k1} & \dots & \alpha_{i_kk}
	\end{vmatrix}$$ Покажем, что первые $k$ столбцов $A$ образуют базис системы столбцов: 1) Покажем линейную независимость от противного. Пусть система $(A_{1}$, \dots, $A_{k})$ линейно зависима. Тогда $\exists$  нетривиальная линейная комбинация $\beta_1 A_1 + \ldots + \beta_k A_k = 0_v$. Распишем
	$$\begin{cases}
		\beta_{1}\alpha_{11} + \ldots + \beta_{n}\alpha_{1k} = 0,
		\\ \dotfill
		\\ \beta_{1}\alpha_{m1} + \ldots + \beta_{n}\alpha_{mk} = 0;
	\end{cases}$$
	Выберем из этой системы уравнения с номерами $i_1,\dots,i_k$. Получим новую систему, матрица которой квадратная, а ее определитель не равен нулю. Следовательно, по принципу Крамера, $\beta_i = 0\ \forall i \Rightarrow$ получаем противоречие с нетривиальностью исходной линейной комбинации $\Rightarrow$ столбцы ($A_1,\dots,A_k$) линейно независимы.\\\\
	Покажем, что $A_p = A_{k+1},\dots,A_n,\ p = k+1,\dots, n$ линейно выражается через $A_1,\dots,A_k$. Для этого рассмотрим определители вида
	$$M_{ы} = \begin{vmatrix} \alpha_{i_11} & \dots & \alpha_{i_1k} & \alpha_{i_1p}
		\\ \vdots & \ddots & \vdots & \vdots \\ \alpha_{i_k1} & \dots & \alpha_{i_kk} & \alpha_{i_kp}
		\\ \alpha_{s1} & \dots & \alpha_{sk} & \alpha_{sp}
	\end{vmatrix} = 0,$$ которые получаются в ходе окаймления минора $M$ элементами $s$-ой строки и $p$-ого столбца. Если $s\in \{i_1,\dots,i_k\}$, то $detM_s = 0$, так как содержит две одинаковые строки.\\
	Если $s\not\in \{i_1,\dots,i_k\}$, то $M_s$ --- минор порядка $k+1$, то есть порядка большего чем порядок базисного минора. Следовательно, $M_s = 0\ \forall s$.\\\\
	Разложим определители $M_s$ по элементам последней строки. Заметим, что алгебраические дополнения элементов последней строки не зависят от выбора $s$ в определителе $M_s$. Обозначим их через $D_1,\dots, D_k,D_{k+1},$ причем $D_{k+1} = M \ne 0$. Тогда
	$\alpha_{s1}D_{1} + \ldots +\alpha_{sk}D_{k} + \alpha_{sp}M = 0 = M_{s}$. Следовательно
	$A_{1}D_{1} + \ldots + A_{k}D_{k} + A_{p}M = 0\Rightarrow A_p = -\frac{1}{M}(A_{1}D_{1} + \ldots + A_{k}D_{k})$. То есть столбцы $A_p$ линейно выражаются через столбцы $A_1,\dots, A_k$. И, следовательно, $(A_1,\dots, A_k)$ --- базис системы столбцов матрицы $A$.
\end{Proof}
\newtheorem*{cor10_7_1}{Следствие}\begin{cor10_7_1}Ранг матрицы равен порядку ее базисного минора.
\end{cor10_7_1}\begin{Proof}
	Количество векторов в базисе системы столбцов совпадает с порядком базисного минора.
\end{Proof}\\\\
Из доказательства теоремы следует, что ранг матрицы можно вычислить с помощью метода окаймляющих миноров.
\newtheorem*{cor10_7_2}{Следствие}\begin{cor10_7_2}Ранг матрицы не меняется при транспонировании и элементарных преобразованиях строк и столбцов.
\end{cor10_7_2}\begin{Proof}
	При транспонировании и при элементарных преобразованиях определитель матрицы не изменяется.
\end{Proof}
\newtheorem*{cor10_7_3}{Следствие (Критерий равенства определителя нулю)}\begin{cor10_7_3}Определитель квадратной матрицы равен нулю $\Longleftrightarrow$ ее столбцы линейно зависимы.
\end{cor10_7_3}\begin{Proof}
	$\Rightarrow)$ Пусть $detA = 0$. Тогда этот определитель не является базисным минором матрицы $A\Rightarrow $ порядок базисного минора меньше порядка матрицы, следовательно, количество векторов в базисе системы столбцов меньше, чем количество столбцов в матрице.\\\\
	$\Leftarrow)$ Если ранг матрицы $A$ меньше ее порядка, то определитель матрицы $A$ не является базисным минором, следовательно, он равен нулю.
\end{Proof}
\newtheorem*{cor10_7_4}{Следствие}\begin{cor10_7_4}Элементарные преобразования строк и столбцов матрицы не меняют ее ранга.
\end{cor10_7_4}
\newtheorem*{th10_7}{Теорема}\begin{th10_7}Ранг произведения матриц не превосходит ранга каждого из сомножителей. Если один из сомножителей --- невырожденная матрица, то ранг произведения матриц равен рангу второго сомножителя.
\end{th10_7}\begin{Proof}
	Пусть матрица $A = (\alpha_{ij}) \in P_{m,n}$, $B = (\beta{ij}) \in P_{n,k}$ и $AB = (\gamma_{ij}) \in P_{m,k};$ Тогда $s$-ый столбец матрицы $AB$ имеет вид\\\\
	$\begin{pmatrix} \gamma_{1s}
		\\ \vdots
		\\ \gamma_{ms}
	\end{pmatrix} = \begin{pmatrix} 
		\alpha_{11}\beta_{1s} + \ldots + \alpha_{1n}\beta_{ns}
		\\ \dotfill
		\\ \alpha_{m1}\beta_{1s} + \ldots + \alpha_{mn}\beta_{ns}
	\end{pmatrix} = \beta_{1s}\underbrace{\begin{pmatrix} \alpha_{11}
			\\ \vdots
			\\ \alpha_{m1}
	\end{pmatrix}}_{A_1} + \ldots + \beta_{ns}\underbrace{\begin{pmatrix} \alpha_{1n}
			\\ \vdots
			\\ \alpha_{mn}
	\end{pmatrix}}_{A_n}$ --- линейная комбинация столбцов матрицы $A$. Следовательно все столбцы матрицы $AB$ линейно выражаются через столбцы матрицы $A$. Следовательно, $\rank (AB) \leqslant \rank A$\\
	С другой стороны, $\rank  (AB) = \rank  (AB)^T = \rank  (B^T A^T) \leqslant \rank  B^T = \rank  B$.\\\\
	Если матрица $A$ невырожденная, то существует обратная матрица $A^{-1}: B = A^{-1} (AB)\Rightarrow \rank  B \leqslant \rank (AB) \Rightarrow \rank (AB) = \rank  B $.
\end{Proof}


















\section{Матрица перехода от базиса к системе векторов. Преобразование координат векторов.}
Пусть $V$ --- $n$-мерное векторное пространство над полем $P$, $A=(a_{1}, \dots , a_{n})$ --- базис пространства $V$,  $B=(b_{1}, \dots ,b_{n})$ --- некоторая система векторов. Разложим вектор $b_i$ системы $B$ по базису $A: b_{i} = \alpha_{1i}a_{1} + \ldots + \alpha_{ni}a_{n}$. Обозначим через
$B_{i} = \begin{pmatrix} \alpha_{1i}
	\\ \vdots
	\\ \alpha_{ni}
\end{pmatrix}$ --- координатные столбцы векторов $b_i$ в базисе $A$.\\\\
$\bullet$\textit{ Матрица $S_{A\rightarrow B} = [B_{1}, \dots, B_{k}]$, составленная из координатных столбцов векторов системы $B$ в базисе $A$, называется \textbf{матрицей перехода} от базиса $A$ к системе векторов $B$.}\\\\
Если системы векторов $A$ и $B$ формально записать в виде однострочных матриц $\overline{A} = (a_1,\dots,a_n), \overline{B} = (b_1,\dots,b_n)$, то $\overline{B} = \overline{A}S_{A\rightarrow B}$\\\\
\textit{\textbf{Свойства матриц перехода:}}
\begin{enumerate}
	\item $\rank S_{A\rightarrow B} = \rank B$.
	\begin{Proof}
		Система векторов линейно независима тогда и только тогда, когда линейно независима система координатных столбцов этих векторов. Следовательно, координатные столбцы векторов базисы системы $B$ образуют базис системы координатных столбцов векторов системы $B$. Но матрица $S_{A\rightarrow B}$ и состоит из этих координатных столбцов. Значит, $\rank S_{A\rightarrow B} = \rank B$.
	\end{Proof}
	\item \textit{Система векторов $B$ является базисом пространства $V$ $\longleftrightarrow$ матрица перехода от некоторого базиса $A$ пространства $V$ к системе $B$ квадратная и невырожденная.}\begin{Proof}
		$\Rightarrow)$ Если система $B$ --- базис пространства $V$, то количество векторов в нем совпадает с количеством векторов в базисе $A$. Следовательно, $S_{A\rightarrow B}$ является квадратной. Так как $B$ --- линейно независимая система векторов, то координатные столбцы векторов системы $B$ также линейно независимы. Следовательно, $\rank S_{A\rightarrow B} = n\Rightarrow detS_{A\rightarrow B}\ne 0$ по следствию из теоремы о базисном миноре.\\\\
		$\Leftarrow)$ Пусть $S_{A\rightarrow B}$ невырожденная и квадратная. Она квадратная, значит, количество векторов в $A$ и $B$ одинаково. А так как она невырожденная, то $\rank S_{A\rightarrow B} = n\Rightarrow$ столбцы $S_{A\rightarrow B}$ линейно независимы. Следовательно, $B$ линейно независима и является базисом пространства.
	\end{Proof}
	\item \textit{Пусть $A,B,C$ --- базисы пространства $V$, $S_{A\rightarrow B}$ $S_{A\rightarrow C}$ $S_{B\rightarrow C}$ --- матрицы перехода. Тогда}\begin{enumerate}
		\item $S_{B\rightarrow A} = (S_{A\rightarrow B})^{-1}$
		\item $S_{A\rightarrow C} = S_{A\rightarrow B}S_{B\rightarrow C}$
	\end{enumerate}
	\begin{Proof}
		\begin{enumerate}
			\item Из определения матрицы перехода следует, что $\overline{B} = \overline{A}S_{A\rightarrow B}$. Так как $S_{A\rightarrow B}$ --- матрица перехода от базиса $A$ к базису $B$, то она невырожденная. Следовательно, $\exists (S_{A\rightarrow B})^{-1}:\overline{B}(S_{A\rightarrow B})^{-1} = \overline{A}$. Столбцы $(S_{A\rightarrow B})^{-1}$ --- координатные столбцы векторов системы $A$ в базисе $B$. Тогда матрицы $(S_{A\rightarrow B})^{-1}$ является матрицей перехода от базиса $B$ к базису $A$.
			\item Из определения матрицы перехода следует, что $\overline{C} = \overline{B}S_{B\rightarrow C}, \overline{B} = \overline{A}S_{A\rightarrow B}\Rightarrow\overline{C} = \overline{A}(S_{A\rightarrow B}\cdot S_{B\rightarrow C})\Rightarrow S_{A\rightarrow B}\cdot S_{B\rightarrow C}$ является матрицей перехода от базиса $A$ к базису $C$.
		\end{enumerate}
	\end{Proof}
\end{enumerate}
\newtheorem*{th8_1}{Теорема}\begin{th8_1} Пусть $x$ --- некоторый вектор пространства $V$, $X_A$ и $X_B$ --- его координатные столбцы в базисах $A$ и $B$ соответственно, $S_{A\rightarrow B}$ --- матрица перехода от базиса $A$ к базису $B$. Тогда $X_{A} = S_{A \rightarrow B}X_{B}$\end{th8_1}\begin{Proof}
	Запишем в матричном виде разложение вектора $x$: $x = \overline{A}X_A = \overline{B}X_B$, но $\overline{B} = \overline{A}S_{A\rightarrow B}$. Следовательно, $\overline{A}X_A = \overline{A}S_{A\rightarrow B}X_B\Rightarrow \overline{A}(X_A - S_{A\rightarrow B} X_B) = 0_v\Rightarrow X_A - S_{A\rightarrow B} X_B$ является координатным столбцом нулевого вектора в базисе $A$. А так как нулевой вектор в любом базисе имеет нулевой координатный столбец, то $X_A - S_{A\rightarrow B}X_B = 0$
\end{Proof}














\section{Сумма и пересечение подпространств.}
Пусть $V$ --- векторное пространство над полем $P$. И пусть $U_1$ и $U_2$ --- подпространства $V$.\\\\
$\bullet$ \textit{\textbf{Пересечением} подпространств $U_1$ и $U_2$ называется множество всех векторов, принадлежащих каждому из этих множеств (Обозначение: $U_1 \cap U_2$).}\\\\
$\bullet$ \textit{\textbf{Суммой} подпространств называется множество векторов $a\in V$, представимых в виде: $a_1+a_2$, где $a_1 \in U_1,\ a_2\in U_2$ (Обозначение: $U_1 + U_2$). Если каждый вектор $a\in U_1 + U_2$ представим в виде $a_1 + a_2,\ a_1 \in U_1, a_2 \in U_2$ лишь единственным образом, то сумма $U_1 + U_2$ называется \textbf{прямой} (Обозначение: $U_1\oplus U_2$).}\\\\
\textbf{\textit{Свойства суммы и пересечения подпространств:}}
\begin{enumerate}
	\item \textit{Пересечение и сумма подпространств являются подпространствами.}\begin{Proof}
		\begin{enumerate}
			\item Пересечение: Пусть $a,b \in U_1 \cap U_2\Rightarrow a,b \in U_1,\ a,b\in U_2\Rightarrow a+b, \alpha a \in U_1,\ a+b, \alpha a \in U_2\Rightarrow a+b, \alpha a\in U_1 \cap U_2$. Значит в пересечении операции определены. Следовательно, $U_1 \cap U_2$ --- подпространство по критерию подпространств.
			\item Сумма: Пусть $a,b \in U_1 + U_2$.\\
			$\begin{cases}
				a = a_1 + a_2,\\
				b = b_1 + b_2;
			\end{cases}\Rightarrow a_1,b_1 \in U_1,\ a_2,b_2 \in U_2\Rightarrow\\ a + b = \underset{\in U_1}{(a_1 + b_1)} + \underset{\in U_2}{(a_2 + b_2)} \Rightarrow a + b \in U_1 + U_2$\\
			$\alpha a = \underset{\in U_2}{\alpha a_1} + \underset{\in U_2}{\alpha a_2} \Rightarrow \alpha a\in U_1 + U_2$\\
			Так как $U_1, U_2$ --- подпространства, то и $U_1 + U_2$ --- подпространство.
		\end{enumerate}
	\end{Proof}
	\item $U_1 + U_2 = L(U_1 \cup U_2)$
	
	\begin{Proof}
		Пусть $x\in U_1+ U_2\Rightarrow x=\underset{\in U_1}{x_1} + \underset{\in U_2}{x_2}:x_1 \in U_1,\ x_2\in U_2;\ x_1, x_2 \in U_1 \cup U_2\Rightarrow\\ x \in L(U_1 \cup U_2)\Rightarrow x_1 + x_2 \in L(U_1\cup U_2)\Rightarrow U_1+ U_2\subseteq L(U_1 \cup U_2)$\\\\
		Пусть $x\in L(U_1\ \cup\  U_2)\Rightarrow$ он представим в виде: $x = \underbrace{\alpha_1 a_1 + \ldots + \alpha_n a_n}_{a_i\in U_1} + \underbrace{\beta_1 b_1 + \ldots + \beta_k b_k}_{b_i\in U_2}\Rightarrow$ так как $U_1$ и $U_2$ --- подпространства $\Rightarrow x \in U_1 + U_2 \Rightarrow L(U_1\cup U_2) \subseteq U_1 + U_2$
	\end{Proof}
	\item \textit{Сумма $U_1 + U_2$ --- прямая $\Longleftrightarrow U_1\cap U_2 = \{0_v\}$}
	\begin{Proof}
		$\Rightarrow )$ Пусть сумма $ U_1 + U_2,\ $ --- прямая. Предположим, что $U_1\cap U_2 \ne \{0_v\}$. То есть, $\exists\  a\in U_1\cap U_2,\ a\ne 0_v$. Так как $U_1 \cap U_2$ --- подпространство, то $-a = (-1) a\in U_1 \cap U_2\Rightarrow$ нулевой вектор, принадлежащий $U_1 + U_2$, представим в виде:\\
		$0_v = \underset{\in U_1}{0_v} + \underset{\in U_2}{0_v}, \\ 0_v = \underset{\in U_1}{a} + \underset{\in U_2}{(-a)}.$\\
		Мы получили два различных разложения нулевого вектора --- противоречие с тем, что сумма прямая.\\\\
		$\Leftarrow )$ Предположим, что $\exists \ x\in U_1 + U_2: x=a_1 + a_2, \ x = b_1 + b_2,$ где $\ a_1,b_1 \in U_1, \ a_2,b_2 \in U_2$\\
		Тогда $\underset{\in U_1}{(a_1 - b_1)} + \underset{\in U_2}{(a_2 - b_2)} = 0_v$.\\
		Так как $U_1$ --- подпространство, то $(a_1 - b_1)\in U_1$. С другой стороны, $(b_1 - a_1) = (a_2 - b_2)\Rightarrow (a_1 - b_1)\in U_1\Rightarrow (a_1 - b_1) \in U_1 \cap U_2 \Rightarrow a_1 - b_1 = 0_v\Rightarrow b_2 - a_2 = 0_v\Rightarrow a_1 = b_1,\ a_2 = b_2$.
	\end{Proof}
\end{enumerate}
\newtheorem*{th10_9_1}{Теорема (Формула Грассмана)}\begin{th10_9_1}
	Если $U_1, U_2$ --- подпространства векторного пространства $V$ над полем $P$, то $$\dim (U_1 + U_2) = \dim  U_1\  +\  \dim  U_2\  -\  \dim  (U_1 \cap U_2).$$
\end{th10_9_1}\begin{Proof}
	Пусть $C (c_1,\dots,c_m)$ --- базис пространства $U_1\cap U_2$, тогда $C$ --- линейно независимая система векторов пространства $U_1\cap U_2$, значит её можно дополнить до базисов каждого из подпространств $U_1$ и $U_2$.\\
	Пусть $A (c_1,\dots,c_m,a_1,\dots, a_k)$ --- базис $U_1$,
	a $B (c_1,\dots,c_m,b_1,\dots, b_s)$ --- базис $U_2$.\\\\
	Покажем, что $D (c,\dots,c_m,a_1,\dots,a_k,b_1,\dots,b_s)$ является базисом пространства $U_1 + U_2$.\\\\
	Докажем выражаемость:\\
	Любой вектор $x\in U_1 + U_2$ представим в виде: $x=\underset{\in U_1}{x_1} + \underset{\in U_2}{x_2}$. Так как $x_1 \in U_1$, то он линейно выражается через систему $A$, соответственно $x_2\in U_2$ линейно выражается через систему $B$. Следовательно $x$ --- линейная комбинация векторов системы $D$.\\\\
	Докажем линейную независимость:\\
	Пусть $D$ --- линейно зависимая система векторов. Тогда существует нетривиальная линейная комбинация, равная нулевому вектору:\\
	$\gamma_1 c_1 + \ldots +\gamma_m c_m +\alpha_1 a_1 + \ldots + \alpha_k a_k + \beta_1 b_1 + \ldots+\beta_s b_s = 0_v$\\
	$\gamma_1 c_1 + \ldots +\gamma_m c_m +\alpha_1 a_1 + \ldots + \alpha_k a_k = d$, где $d = -\beta_1 b_1 - \ldots-\beta_s b_s$.\\ Следовательно, $d$ линейно выражается через систему векторов $A$ и через систему векторов $B\Rightarrow d \in U_1 \cap U_2\Rightarrow d$ линейно выражается через базис пересечения, то есть через систему $C : d = \delta_1 c_1 + .. + \delta_m c_m + 0\cdot b_1 + \ldots + 0 \cdot b_s$. С другой стороны, справедливо разложение $d = 0\cdot c_1 + \ldots + 0\cdot c_n - \beta_1 b_1 - \ldots - \beta_s b_s$.\\
	То есть $d$ имеет два разложения по базису $B$ $\Rightarrow \delta_i = \beta_i = 0\ \forall i \Rightarrow d = 0_v$ --- это нетривиальная линейная комбинация векторов системы $A$ равная $0_v\Rightarrow$ противоречие с тем, что система векторов $A$ --- базис $\Rightarrow$ система $D$ --- базис пространства $U_1+U_2\Rightarrow \dim (U_1 + U_2) = m + k + s = (m+k)+(m+s)-m = \dim (U_1 + U_2) = \dim  U_1 + \dim  U_2 - \dim (U_1\cap U_2)$
\end{Proof}
\newtheorem*{cor10_9_1}{Следствие}\begin{cor10_9_1} Если $W = U_1 \oplus U_2$, $A = (a_1,\dots, a_k)$ --- базис $U_1$, $B(b_1,\dots, b_s)$ --- базис $U_2$, то $(a_1,\dots,a_k,b_1,\dots,b_s)$ --- базис $U_1\oplus U_2$.
\end{cor10_9_1}







\section{Пространство решений линейной однородной системы.}
Рассмотрим линейную однородную систему уравнений над полем $P$
$$\begin{cases}
	\alpha_{11}x_1+ \ldots +\alpha_{1n}x_n=0, \\
	\dotfill\\\
	\alpha_{m1}x_1+ \ldots +\alpha_{mn}x_n=0.
\end{cases}\eqno(10.10.1)$$ Обозначим через $A$ матрицу системы (10.10.1), $A_1,\dots,A_n$ --- столбцы матрицы $A$, $X = \begin{pmatrix} x_1 \\ \vdots\\ x_n \end{pmatrix}$ --- столбец неизвестных. Тогда система (10.10.1) в матричном виде может быть записана следующим образом:\begin{center}
	$AX = 0$ или $A_1 x_1 + \ldots + A_n x_n = 0$.
\end{center}
Так как однородная система линейных уравнений всегда имеет нулевое решение, то множество решений системы (10.10.1) непусто, а так как каждое решение является упорядоченной последовательностью элементов поля $P$, то есть элементом из $P^n$, то множество решений системы --- непустое подмножество пространства $P^n$.
\newtheorem*{th10_10_1}{Теорема}\begin{th10_10_1}Множество решений линейной однородной системы уравнений над полем $P$ с $n$ неизвестными является подпространством пространства $P^n$ размерности $n-\rank A$, где $A$ --- матрица системы.
\end{th10_10_1}\begin{Proof}
	Пусть $L$ --- множество решений линейной однородной системы уравнений (10.10.1). Тогда $L\subseteq P^n, L \ne \varnothing$.\\ Покажем, что $L$ --- подпространство пространства $P^n$. Пусть $b (\beta_1,\dots,\beta_n) \in P^n,\ c(\gamma_1,\dots,\gamma_n)\in P^n$ --- решения системы (10.10.1). Тогда подставим решения в систему: $A_1\beta_1 + \ldots + A_n\beta_n = 0$, $A_1\gamma_1 + \ldots + A_n\gamma_n = 0$. Следовательно,
	$$A_1(\beta_1 + \gamma_1)+ \ldots +A_n(\beta_n + \gamma_n) = 0,$$
	$$A_1(\alpha \beta_1) + \ldots + A_n(\alpha \beta_n) = 0.$$
	Тогда $b+c = (\beta_1 + \gamma_1,\ldots, \beta_n + \gamma_n)$ и $\alpha b = (\alpha \beta_1,\ldots, \alpha \beta_n)$ --- решения системы (10.10.1), то есть $b+c, \alpha b \in L$. Следовательно, $L$ --- подпространство $P^n$.\\
	Покажем, что $\dim  L = n - \rank  A$.\begin{enumerate}
		\item Если $\rank A = 0 $, то матрица $A$ ненулевая. Следовательно, любой элемент пространства $P^n$ является решением системы (10.10.1) и $L = P^n$. Тогда $\dim L = \dim  P^n = n =n-0= n - \rank A$.
		\item Пусть $\rank A = n$. Тогда все столбцы матрицы $A$ линейно независимы. Следовательно, $A_1 x_1 + \ldots + A_n x_n = 0$ справедливо лишь в случае, когда $x_1 = \ldots = x_n = 0\Rightarrow  L=\{0\} \Rightarrow \dim L = 0 = n - n = n-\rank A$.
		\item Пусть $\rank A = k$, $0<k<n$. Тогда базис системы столбцов матрицы $A$ состоит из $k$ столбцов. Без ограничения общности будем считать, что базис базис расположен в первых $k$ столбцах (в противном случае перенумеруем неизвестные). Тогда остальные $n-k$ столбцов линейно выражаются через первые $k$ столбцов. То есть существуют $\beta_{i,j}$ такие, что
		$\begin{cases}
			\beta_{11}A_1+ \ldots +\beta_{1k}A_k + A_{k+1}=0, \\
			\dotfill\\
			\beta_{(n-k)1}A_1+ \ldots +\beta_{(n-k)k}A_k + A_n=0.
		\end{cases}$ Тогда последовательности $\begin{cases}
			b_1(\beta_{11},\dots,\beta_{1k},1,0,\dots0), \\
			\dotfill\\
			b_{n-k}(\beta_{(n-k)1},\dots,\beta_{(n-k)k},0,0,\dots1);
		\end{cases}  $ являются решениями системы (10.10.1).\\\\
		Покажем, что последовательности $b_1,b_2,\dots,b_{n-k}$ являются базисом пространства решений $L$. Запишем эти последовательности в виде матрицу по строкам:
		\begin{center}
			$\begin{pmatrix} \beta_{11} & \dots & \beta_{1n} & 1 & 0 & \dots & 0 \\ \beta_{21} & \dots & \beta_{2n} & 0 & 1 & \dots & 0 \\\vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\ \beta_{k1} & \dots & \beta_{kn} & 0 & 0 & \dots & 1 \end{pmatrix}$.
		\end{center} Последние $n-k$ столбцов образуют единичную матрицу. А так как ее определитель ненулевой, то он является базисным минором этой матрицы. Тогда строки, элементы которых образуют этот базисный минор, то есть  $b_1,b_2,\dots,b_{n-k}$, являются линейно независимыми.\\
		Покажем, что любое решение системы (10.10.1) линейно выражается через эти последовательности. Пусть $c = (\gamma_1,\dots,\gamma_n)\in L$ --- произвольное решение системы (10.10.1). Построим последовательность $d = c - \gamma_{k+1}b_1 - \gamma_{k+2}b_2 - \ldots - \gamma_nb_{n-k}$, у которой все компоненты начиная с $k+1$ нулевые, то есть $d = (\delta_1,\dots,\delta_k,0,0,\dots,0)$. А так как последовательность $d$ является линейной комбинацией решений системы (10.10.1), то она также является решением системы (10.10.1). Следовательно, $A_1\delta_1 + \ldots + A_k\delta_k = 0$. Поскольку столбцы $A_1,\dots,A_k$ линейно независимы, то $\delta_1=\dots=\delta_k=0$, то есть $d$ --- нулевая последовательность (нулевой элемент поля $P^n$). Тогда $c=\gamma_{k+1}b_1 - \gamma_{k+2}b_2 - \ldots - \gamma_nb_{n-k}$ и последовательности $b_1,\dots,b_{n-k}$ являются базисом пространства $L$. Получается, $\dim L = n-k = n - \rank A$.
	\end{enumerate}
\end{Proof}
\newtheorem*{cor10_10}{Следствие}\begin{cor10_10}Однородная система линейных уравнений имеет единственное решение $\Longleftrightarrow$ ранг матрицы системы равен числу неизвестных.
\end{cor10_10}\begin{Proof}
	Система имеет ненулевое решение $\Rightarrow \dim  L > 0 \Rightarrow n - \rank A > 0 \Rightarrow n > \rank A$
\end{Proof}\\\\
$\bullet$\textit{ Базис пространства решений линейной однородной системы уравнений называется \textbf{фундаментальной системой решений}.}
\newtheorem*{th10_10_2}{Теорема}\begin{th10_10_2}Для любого подпространства $L$ пространства $P^n$ существует линейная однородная система уравнений над полем $P$ с $n$ неизвестными, множество решений которой совпадает с $L$.
\end{th10_10_2}\begin{Proof} Для начала рассмотрим тривиальные случаи.\begin{enumerate}
		\item $L = P^n \Rightarrow $ система, все коэффициенты которой равны нулю, имеет множество решений, совпадающее с $L$;
		\item $L = \{0_{P^n}\} \Rightarrow$ из принципа Крамера любая невырожденная система имеет множество решений, совпадающее с $L$;
	\end{enumerate}
	Пусть $\dim L = k, 0< k< n$. И пусть
	$$\begin{cases}a_1(\alpha_{11},\dots,\alpha_{1n}),\\
		\dotfill\\
		a_k(\alpha_{k1},\dots,\alpha_{kn});\end{cases}\text{--- базис \textit{L}.}$$
	Составим систему
	$$\begin{cases}
		\alpha_{11}x_1+ \ldots +\alpha_{1n}x_n=0, \\
		\dotfill\\
		\alpha_{k1}x_1+ \ldots +\alpha_{kn}x_n=0.
	\end{cases}\eqno(10.10.2)$$ Так как векторы $a_1,\ldots,a_k$ линейно независимы, то ранг матрицы системы (10.10.2) равен $k$, а пространство решений системы (10.10.2) имеет размерность $n-k$. Пусть\\
	$$\begin{cases}b_1(\beta_{11},\dots,\beta_{1n}),\\
		\dotfill\\
		b_{n-k}(\beta_{(n-k)1},\dots,\beta_{(n-k)n});\end{cases}\text{--- фундаментальная системы решений системы (10.10.2).}$$ Построим систему
	$$\begin{cases}
		\beta_{11}y_1+ \ldots +\beta_{1n}y_n=0, \\
		\dotfill\\
		\beta_{(n-k)1}y_1+ \ldots +\beta_{(n-k)n}y_n=0.
	\end{cases}\eqno(10.10.3)$$ Покажем, что пространство решений системы (10.10.3) совпадает с пространством $L$. Последовательности $a_1,\ldots,a_k$ являются решениями системы (10.10.3), следовательно, любая линейная комбинация этих последовательностей также является решением системы (10.10.3), и любой элемент подпространства $L$ является пространством решений (10.10.3). Тогда $L$ --- подпространство пространства решений системы (10.10.3).\\
	Так как система векторов $(b_1,\dots,b_{n-k})$ линейно независимая, то ранг матрицы системы (10.10.3) равен $n-k \Rightarrow$ пространство решений системы (10.10.3) имеет размерность $ n- (n-k) = k\Rightarrow$ по теореме о монотонности размерности, $L$ --- пространство решений системы (10.10.3).
\end{Proof}











\section{Критерий совместности линейных неоднородных систем. Структура множества решений линейной неоднородной системы уравнений.}
Рассмотрим линейную неоднородную систему уравнений над полем $P$
$$\begin{cases}
	\alpha_{11}x_1+ \ldots +\alpha_{1n}x_n=b_1 \\
	\dotfill\\
	\alpha_{m1}x_1+ \ldots +\alpha_{mn}x_n=b_m.
\end{cases}\eqno(10.11.1)$$ Обозначим через $A$ матрицу системы (10.11.1), $\widetilde{A}$ --- расширенную матрицу системы (10.11.1), $A_1,\dots,A_n$ --- столбцы матрицы $A$, $X = \begin{pmatrix} x_1 \\ \vdots\\ x_n \end{pmatrix}$ --- столбец неизвестных, $B = \begin{pmatrix} b_1 \\ \vdots\\ b_m \end{pmatrix}$ --- столбец свободных членов. Тогда система (10.11.1) в матричном виде может быть записана следующим образом:\begin{center}
	$AX = B$ или $A_1 x_1 + \ldots + A_n x_n = B$.
\end{center}
\newtheorem*{th10_11_1}{Теорема Кронекера-Капелли (Критерий совместности линейных неоднородных систем)}\begin{th10_11_1}
	Система линейных уравнений совместна $\Longleftrightarrow$ ранг матрицы системы равен рангу расширенной матрицы, то есть $\rank  A = \rank \widetilde{A}$.
\end{th10_11_1}\begin{Proof}
	$\Rightarrow)$ Если система (10.11.1) совместна, существуют скаляры $(\gamma_1,\dots,\gamma_n)\in P$ такие, что $\gamma_1 A_1 + \ldots + \gamma_n A_n = B$. Следовательно, столбец $B$ линейно выражается через столбцы $A_1,\dots,A_n \Rightarrow L(A_1,\dots,A_n,B) =L(A_1,\dots,A_n) \Rightarrow \dim L(A_1,\dots,A_n,B) = \dim L(A_1,\dots,A_n)\Rightarrow \rank (A_1,\dots,A_n,B) = \rank (A_1,\dots,A_n) \Rightarrow \rank \widetilde{A} =\rank  A$.\\\\
	$\Leftarrow)$ Пусть $\rank \widetilde{A} =\rank  A$. Тогда $ \rank (A_1,\dots,A_n,B) = \rank (A_1,\dots,A_n) \Rightarrow \dim L(A_1,\dots,A_n,B) = \dim L(A_1,\dots,A_n).$ Заметим, что $L(A_1,\dots,A_n) \subseteq L(A_1, \dots, A_n, B)$, по теореме о монотонности размерности $L(A_1,\dots,A_n)=L(A_1, \dots, A_n, B) \Rightarrow B \in L(A_1,\dots,A_n)$, то есть существуют скаляры $\gamma_i$ такие, что $B =  \gamma_1 A_1 + \ldots + \gamma_n A_n \Rightarrow (\gamma_1,\dots,\gamma_n)$ --- решения системы (10.11.1).
\end{Proof}\\\\
$\bullet$ \textit{Однородная система линейных уравнений $AX = 0$ с той же матрицей, что и система (10.11.1), называется \textbf{приведенной} для неоднородной системы (10.11.1).}
\newtheorem*{th10_11_2}{Теорема}\begin{th10_11_2}Если $x = (x_1,\dots,x_n)\in P^n$ --- решение системы  (10.11.1), а $L_o$ --- пространство решений приведенной системы для системы (10.11.1), то множество решений системы (10.11.1) совпадает с множеством $L_H = \{x\} + L_o = \{ x + y \ | \ y\in L_o \}$.
\end{th10_11_2}\begin{Proof}
	Пусть $z = (z_1,\dots, z_n)\in L_H$. Тогда построим последовательность $y = (y_1,\dots, y_n) \in L_o$ такую, что $y = z - x$. При этом $A_1y_1 + \ldots + A_n y_n = A_1(z_1 - x_1) + \ldots + A_n(z_n - x_n) = \underbrace{ (A_1z_1 + \ldots + A_nz_n) }_{B} - \underbrace{ (A_1x_1 + \ldots + A_nx_n) }_{B} = 0 \Rightarrow y\in L_o \Rightarrow z = x + y\Rightarrow L_H \subseteq \{x\} + L_o.$\\\\
	Пусть $z=(z_1,\dots,z_n)\in \{x\} +L_o$. Построим последовательность $z = x+y$. Тогда $A_1z_1 +\dots +A_nz_n = A_1(x_1 + y_1) + \ldots + A_n(x_n + y_n)=\underbrace{(A_1 x_1 + \ldots + A_n x_n)}_{B} + \underbrace{(A_1 y_1 + \ldots + A_n y_n)}_{0} = B\Rightarrow z$ --- решение системы (10.11.1), то есть $z \in L_H\Rightarrow \{x\} + L_o \subseteq L_H$.
\end{Proof}
\newtheorem*{cor10_11}{Следствие}\begin{cor10_11}Система линейных уравнений имеет единственное решение $\Longleftrightarrow$ ранги матрицы системы и расширенной матрицы системы равны числу неизвестных.
\end{cor10_11}\begin{Proof}
	$\Rightarrow)$ Если система (10.11.1) имеет единственное решение, то множество $L_o$ состоит из одного элемента. А так как $L_o$ --- подпространство пространства $P^n$, то $L_o = \{0_{P^n}\} \Rightarrow \dim L_o = n-\rank A = 0\Rightarrow n= \rank A = \rank  \widetilde{A}$ ($\rank A = \rank \widetilde{A}$ по теореме Кронекера-Капелли).\\\\
	$\Leftarrow )$ От противного. Пусть $\rank A = \rank \widetilde{A} = n$ и пусть система (10.11.1) имеет два решения $x,y \in L_H$ $\Rightarrow L_H = \{ x\} + L_o = \{ y\} + L_o$.
	Так как $\rank A = n$, то $\dim  L_o = n-\rank A = 0\Rightarrow L_o =\{ 0_v \}\Rightarrow \{x\} + L_o = \{x\} + \{0\} = \{x\}$ и $\{y\} + L_o = \{y\} + \{0\} = \{y\}\Rightarrow \{x\} = \{y\} \Rightarrow x = y$.
\end{Proof}	





\chapter{Линейные операторы}
\section{Определение и простейшие свойства линейных операторов.}
$\bullet$ \textit{Пусть $V$, $U$ --- векторные пространства над полем $P$. Отображение $f : V \rightarrow U$ называется \textbf{линейным оператором} (линейным отображением) пространства $V$ в пространство $U$, если}
\begin{enumerate}
	\item $f(a+b) = f(a) + f(b),\quad \forall a,b \in V$
	\item $f(\alpha a) = \alpha f(a), \quad \forall a\in V,\  \forall \alpha \in P$
\end{enumerate}
$\bullet$ \textit{Линейный оператор пространства $V$ в себя называется \textbf{линейным оператором} или \textbf{линейным преобразованием} пространства $V$. Множество линейных операторов пространства $V$ (Обозначается: $End(V)$, эндоморфизм).}\\\\
\textit{\textbf{Простейшие свойства линейных операторов:}}
\begin{enumerate}
	\item $f(0_v) = 0_v$
	
	$f(-a) = -f(a), \quad \forall a\in V$
	
	\begin{Proof}
		Следует из аксиомы 2 при $\alpha = 0$ и $\alpha = -1$.
	\end{Proof}
	
	\item \textit{Линейный оператор линейно зависимую систему отображает в линейно зависимую}.
	\begin{Proof}
		Пусть система $\asys$ линейно зависимая. Тогда существует нетривиальная $\alpha_1 a_1 + \ldots + \alpha_n a_n = 0_v$, тогда $f(A) = (f(a_1),\dots f(a_n))$ с теми же коэффициентами тоже нетривиальная и, при этом, $\alpha_1 f(a_1) + \ldots + \alpha_n f(a_n) =f(\alpha_1 a_1) + \ldots + f(\alpha_n a_n) =f(\alpha_1 a_1 + \ldots + \alpha_n a_n) = f(0_v) = 0_v \Rightarrow f(A) = (f(a_1),\dots,f(a_n))$ линейно зависима.
	\end{Proof}
\end{enumerate}
\newtheorem*{cor2_1}{Следствие}\begin{cor2_1} Для любой системы векторов $A$ и линейного оператора $f$ $\rank  f(A) \leqslant \rank  A$.\end{cor2_1}\begin{Proof}
	Максимальное количество линейно независимых векторов системы $f(A)$ не может быть больше, чем максимальное количество линейно независимых векторов из $A$.
\end{Proof}
\begin{enumerate}
	\item[3.] \textit{Если $f$ --- линейный оператор, $\asys$ --- система векторов, то $f(L(A)) = L(f(A))$} \begin{Proof} Следует из равенства
		$f(\alpha_1 a_1 + \ldots + \alpha_n a_n) = \alpha_1 f(a_1) + \ldots + \alpha_n f(a_n)\quad \forall\alpha_1 \in P$.
	\end{Proof}
	\item[4.] \textit{Если $U$ --- подпространство пространства $V$, то $f(U)$ --- также подпространство пространства $V$, причем} $\dim f(U) \leqslant \dim U.$
	\begin{Proof}
		\begin{enumerate}
			\item Покажем, что $f(U)$ --- подпространство.\\
			Если $U$ --- подпространство пространства $V$, то в нём существует базис $A$ и $U = L(A)$. Тогда $f(U) = f(L(A)) = L(f(A))$, следовательно, $f(U)$ --- подпространство.
			\item Докажем неравенство размерности.\\
			$\dim  f(U) = \dim  L(f(A)) = \rank  f(A) \leqslant \rank  A = \dim  L(A) = \dim  U$.
		\end{enumerate}
	\end{Proof}
\end{enumerate}









\section{Матрица линейного оператора.}
\newtheorem*{th11_2_1}{Теорема}\begin{th11_2_1} Пусть $V$ --- векторное пространство над полем $P$, $\asys$ --- базис пространства $V$, $B(b_1,\dots,b_n)$ --- некоторая система векторов из $V$. Тогда существует единственный линейный оператор $f:V\rightarrow V$, для которого $f(a_i) = b_i$.
\end{th11_2_1}
\begin{Proof}\begin{enumerate}
		\item Существование:\\\\
		Так как система векторов $A$ --- базис пространства $V$, то любой вектор $c\in V$ можно представить в виде $c = \gamma_1a_1 + \ldots + \gamma_n a_n$, где $\gamma_i$ --- координаты вектора $c$ в базисе $A$.\\\\ Покажем, что отображение $f(c) = \gamma_1 b_1 + \ldots + \gamma_n b_n$ является единственным линейным оператором пространства $V$, для которого выполняется $f(a_i) = b_i\quad \forall i$.\\ Вектор $a_i$ в базисе $A$ имеет разложение $a_i = 0\cdot a_1 + \ldots + 0\cdot a_{i-1} + 1\cdot a_i + 0\cdot a_{i+1} + \ldots + 0\cdot a_n$. Тогда $f(a_i) = 0\cdot b_1 + \ldots + 0\cdot b_{i-1} + 1\cdot b_i + 0\cdot b_{i+1} + \ldots + 0\cdot b_n = b_i$.\\\\
		Покажем, что отображение $f$ --- линейное. Пусть произвольные векторы векторы $c, d$ пространства $V$ в базисе $A$ имеют координаты $(\gamma_1,\dots,\gamma_n)$ и $(\delta_1,\dots,\delta_n)$ соответственно. Тогда $f(c+d) =f((\gamma_1 a_1 + \ldots + \gamma_n a_n) + (\delta_1 a_1 + \ldots + \delta_n a_n))= (\gamma_1 + \delta_1) b_1 + \ldots + (\gamma_n + \delta_n) b_n = (\gamma_1 b_1 + \ldots + \gamma_n b_n) + (\delta_1 b_1 + \ldots + \delta_n b_n) = f(c) + f(d)$. Случай $f(\alpha c) = \alpha f(c)$ доказывается аналогично.
		\item Единственность:\\\\
		Пусть $g:V\rightarrow V$ --- линейный оператор пространства $V$, для которого выполняется $g(a_i) = b_i\quad \forall i$. Тогда $\forall c \in V: c = \gamma_1 a_1 + \ldots + \gamma_n a_n$ будет справедливо $g(c) = g(\gamma_1 a_1 + \ldots + \gamma_n a_n) = \gamma_1 g(a_1) + \ldots + \gamma_n g(a_n) = \gamma_1 b_1 + \ldots + \gamma_n b_n = f(c)$. Следовательно, $f$ и $g$ --- одно и то же отображение.
	\end{enumerate} 
\end{Proof}\\
Из теоремы следует, что действие линейного оператора пространства $V$ полностью определяется действием этого оператора на базис пространства $V$.\\\\
$\bullet$ \textit{Пусть $\asys$ --- базис пространства $V$, $f$ --- линейный оператор пространства $V$. Матрица, составленная из координатных столбцов векторов $f(a_i)$ в базисе $A$, называется \textbf{матрицей линейного оператора $f$ в базисе $A$} (обозначение: $M_f, M_f^A$). Таким образом, если обозначить через $A_i$ координатные столбцы векторов $f(a_i)$ в базисе $A$, то $M_f^A = [A_1,\dots,A_n]\in P_{n,n}$}\\\\
Из определения следует, что матрица линейного оператора $f$ в базисе $A$ является матрицей перехода от базиса $A$ к системе векторов $f(A)$, то есть $M_f^A = S_{A\rightarrow f(A)}$. 
\newtheorem*{th11_2_2}{Теорема}\begin{th11_2_2}Пусть $M_f$ --- матрица линейного оператора $f:V\rightarrow V$ в базисе $A$, $X_a$ и $X_{f(a)}$ --- координатные столбцы векторов $a$ и $f(a)$ в базисе $A$. Тогда $X_{f(a)} = M_f^A X_a$.
\end{th11_2_2}\begin{Proof}
	Если базис $A$ пространства $V$ записать в виде $\overline{A}(a_1,\dots, a_n)$, то произвольный вектор $a$ имеет разложение $a = \alpha_1 a_1 +\ldots + \alpha_n a_n = \overline{A}X_a$ по базису $A$. Тогда
	$f(a)= \overline{A}X_{f(a)}$.\\\\ А из опеределения матрицы перехода следует, что $\overline{f(A)} = \overline{A}S_{A\rightarrow f(A)} = \overline{A}M^A_f$. Так как $f$ --- линейный оператор, то $f(a) = f(\alpha_1 a_1 + \ldots + \alpha_n a_n) = \alpha_1 f(a_1) + \ldots + \alpha_n f(a_n) = \overline{f(A)}X_A$.\\\\ Следовательно, из уравнения полученного ранее и этого уравнения имеем
	$\overline{A}X_{f(a)} = \overline{f(A)}X_A\Rightarrow (\overline{A}\cdot M^A_f) X_A = \overline{A}(M^A_f X_A)$. А так как разложение по базису единственно, то $X_{f(a)} = M_f X_a$, что и требовалось доказать.\end{Proof}














\section{Пространство линейных операторов.}
Пусть $V$ --- $n$-мерное векторное пространство над полем $P$, а $f$ и $g$ --- линейные операторы пространства $V$.\\\\
$\bullet$\textit{\textbf{ Суммой} линейных операторов $f$ и $g$ называется отображение $(f + g) : V\rightarrow V$, которое каждому вектору $a\in V$ ставит в соответствие вектор $f(a) + g(a)\in V$, то есть }
\begin{center}
	$(f+g)(a) = f(a) + g(a),\quad \forall\ a \in V.$
\end{center}
$\bullet$ \textit{\textbf{Произведением оператора $f$ на скаляр $\alpha$} называется отображение $(\alpha f) : V\rightarrow V$, которое каждому вектору $a\in V$ ставит в соответствие вектор $\alpha f(a)\in V$, то есть}
\begin{center}
	$(\alpha f)(a) = \alpha f(a),\quad \forall\ a \in V.$
\end{center}
$\bullet$ \textit{\textbf{Композицией операторов} $f$ и $g$ называется отображение $(g \circ f) : V\rightarrow V$, которое каждому вектору $a\in V$ ставит в соответствие вектор $g(f(a))\in V$, то есть}
\begin{center}
	$(g \circ f)(a) =g(f(a)),\quad \forall\ a \in V.$
\end{center}
\newtheorem*{th11_3_1}{Теорема}\begin{th11_3_1}
	Если $f$ и $g$ --- линейные оператор пространства $V$, то отображения  $(f+g), (\alpha f), (g\circ f)$ также являются линейными операторами пространства $V$, причем для их матриц в некотором фиксированном базисе справедливы равенства:\begin{center}
		$M_{f+g} = M_f + M_g,\quad M_{\alpha f} = \alpha M_f,\quad M_{g \circ f} = M_g M_f.$
	\end{center}
\end{th11_3_1}\begin{Proof}
	Доказательство проведем для случая $g\circ f$. Для остальных случаев доказательство аналогично.\begin{enumerate}
		\item Линейность. $\forall a,b \in V,\ \forall\alpha \in P$\\
		$(g\circ f)(a+b) =g(f(a+b)) = g(f(a) + f(b)) = g(f(a)) + g(f(b)) = (g\circ f)(a) + (g\circ f)(b)$.\\
		$(g\circ f)(\alpha a) = g(f(\alpha a)) = g(\alpha f(a)) = \alpha (g(f(a))) = \alpha (g\circ f)(a).$
		\item Покажем, что $M_{g \circ f} = M_g M_f$.\\
		Пусть $M_f = (\alpha_{ij})$ и $M_g = (\beta_{ij})$  --- матрицы операторов $f$ и $g$ в базисе $\asys$. Тогда $f(a_j) = \alpha_{1j} a_1 + \ldots + \alpha_{nj} a_n,\ g(a_j) = \beta_{1j} a_1 + \ldots + \beta_{nj} a_n$.\\
		$(g\circ f)(a_j) = g(f(a_j)) = g(\alpha_{1j} a_1 + \ldots + \alpha_{nj} a_n) = \alpha_{1j} g(a_1) + \ldots + \alpha_{nj} g(a_n) = \alpha_{1j} (\beta_{11} a_1 + \ldots + \beta_{n1} a_n) + \ldots + \alpha_{nj} (\beta_{1n} a_1 + \ldots + \beta_{nn} a_n) = (\alpha_{1j}\beta_{11} + \ldots + \alpha_{nj}\beta_{1n}) a_1 + \ldots + (\alpha_{1j} \beta_{n1} + \ldots + \alpha_{nj} \beta_{nn}) a_n \Rightarrow j$-ые столбцы матриц $M_{g\circ f}$ и $M_g M_f$ одни и те же.
	\end{enumerate}
\end{Proof}
\newtheorem*{cor11_3_1}{Следствие}\begin{cor11_3_1}
	Множество $End (V)$ всех линейных операторов пространства $V$ является векторным пространством над полем $P$ относительно операций сложения линейных операторов и умножения линейных операторов на скаляр.
\end{cor11_3_1}\begin{Proof}
	Из теоремы следует, что операции сложения и умножения на скаляр определены в $End(V)$, остальные же аксиомы векторного пространства проверяются аналогично.
\end{Proof}
\newtheorem*{th11_3_2}{Теорема}\begin{th11_3_2}
	Отображение $\varphi:End(V) \rightarrow P_{n,n}$, ставящее в соответствие каждому линейному оператору $f \in End(V)$ его матрицу в некотором фиксированном базисе $A$, то есть $\varphi(f) = M_f^A$, является изоморфизмом.
\end{th11_3_2}\begin{Proof}\begin{enumerate}
		\item Докажем биективность:
		\begin{enumerate}
			\item Докажем инъективность:\\
			От противного. Пусть $f \ne g$ и предположим, что $\varphi(f) = \varphi(g)$, то есть $M_f^A = M_g^A$. Тогда любой вектор $a \in V$ представим в виде $a = \alpha_1 a_1 + \ldots + \alpha_na_n\Rightarrow f(a) = f(\alpha_1 a_1 + \ldots + \alpha_na_n) = f(\alpha_1 a_1) + \ldots + f(\alpha_n a_n) = \alpha_1f(a_1) + \ldots + \alpha_n f(a_n) = [M_f^A = M_g^A \Rightarrow f(a_j) = g(a_j)] = \alpha_1g(a_1) + \ldots + \alpha_ng(a_n) = g(\alpha_1 a_1 + \ldots + \alpha_na_n) = g(a)$, что противоречит тому, что $f \ne g$. Значит $f = g$ и $\varphi$ --- инъекция.
			\item Докажем сюръективность:\\
			Покажем, что для любой матрицы $C = (\gamma_{ij})\in P_{n,n}$ существует единственный линейный оператор $f\in End(V)$ такой, что $M^A_f = \varphi(f) = C$. Пусть $\asys$ --- базис пространства $V$. Так как для произвольной системы векторов $\bsys$ существует единственный линейный оператор, который векторы $a_i$ базиса $A$ отображает в векторы $b_i = \gamma_{1i}a_1 + \ldots + \gamma_{ni}a_n\quad \forall i$, то матрица этого линейного оператора в базисе $A$ совпадает с матрицей $C$. Следовательно, $\varphi$ --- сюръекция. А значит и биекция.
		\end{enumerate}
		\item Докажем линейность: $\forall f,g\in End(V),\ \forall \alpha \in P$\\
		$\varphi(f+g) = M_f + M_g =\varphi(f) + \varphi(g),$\\
		$\varphi(\alpha f) = M_{\alpha f} = \alpha \varphi_f \Rightarrow$ отображение линейно.
	\end{enumerate}
\end{Proof} 












\section{Ранг и дефект линейного оператора.}
Пусть $f:V\rightarrow V$ --- линейный оператор пространства $V$ над полем $P$.\\\\
$\bullet$ \textit{Множество $\{f(a)\ |\ a\in V\}$ называется \textbf{образом} линейного оператора $f$. Обозначение: $Im\ f,\ f(V)$.}\\\\
$\bullet$ \textit{Множество $\{a\in V\ |\ f(a) = 0_v\}$ называется \textbf{ядром} линейного оператора $f$. Обозначение: $Ker\ f$.}
\newtheorem*{lem11_4_1}{Лемма}\begin{lem11_4_1}Образ и ядро линейного оператора $V$ являются подпространствами пространства $V$.
\end{lem11_4_1}
\begin{Proof}\begin{enumerate}
		\item Для любого подпространства $U$ пространства $V$ множество $f(U)$ также является подпространством пространства $V$ (по свойству 4). Так как $V$ является подпространством самого себя, то $f(V)$ --- подпространство пространства $V$.
		\item Так как линейный оператор нулевой вектор отображает в нулевой, то $0_v \in Ker\ f$, следовательно, $Ker\ f \ne \varnothing$.\\
		Покажем, что множество $Ker\ f$ замкнуто относительно линейных операций. Пусть произвольные векторы $a, b \in Ker\ f,\ \alpha \in P.$ Тогда\\
		$f(a+b) = f(a) + f(b) = 0_v + 0_v = 0_v$,\\
		$f(\alpha a) = \alpha  f(a) = \alpha \cdot 0_v = 0_v$.\\ Следовательно, во множестве $Ker\ f$ определены операции сложения векторов и умножения вектора на скаляр, и $Ker\ f$ --- подпространство пространства $V$.
	\end{enumerate}
\end{Proof}\\
$\bullet$ \textit{Размерность образа линейного оператора называется \textbf{рангом} оператора. Обозначение: $\rank  f$.}\\\\
$\bullet$ \textit{Размерность ядра линейного оператора называется \textbf{дефектом} оператора. Обозначение: $def f$.}
\newtheorem*{th11_4_1}{Теорема}\begin{th11_4_1}Линейный оператор инъективен $\Longleftrightarrow$ его ядро --- нулевое подпространство.
\end{th11_4_1}\begin{Proof}
	$\Rightarrow)$ Пусть линейный оператор $f:V\rightarrow V$ инъективен. Так как любой линейный оператор нулевой вектор отображает в нулевой, то других векторов, отображенных инъективно в нулевой, нет. Следовательно, $Kerf = \{ 0_v \}$.\\\\
	$\Leftarrow )$ Пусть $Kerf = \{ 0_v \}$. Возьмем произвольные векторы $a,\ b\in V$ такие, что $a\ne b,\ f(a) = f(b).$ Тогда $f(a) - f(b) = 0_v \Rightarrow f(a-b) = 0_v \Rightarrow$ [по определению ядра] $\Rightarrow \underset{\ne 0_v}{a-b} \in Kerf \Rightarrow$ получаем противоречие с тем, что $a\ne b$. Значит $a = b$ и $f$ --- инъекция.
\end{Proof}\\\\
Пусть $f:V\rightarrow V$ --- линейный оператор, $M_f$ --- матрица этого оператора в базисе $A$.
\newtheorem*{th11_4_2}{Теорема}\begin{th11_4_2}Ранг линейного оператора равен рангу матрицы оператора: $\rank  f = \rank  M_f$. Дефект линейного оператора пространства $V$ равен разности размерности пространства $V$ и линейного оператора: $def f = \dim  V - \rank  f$.
\end{th11_4_2}\begin{Proof}\begin{enumerate}
		\item $\rank  f=$ [ по определению ] $=\dim f(V) =$ [ так как $A$ --- базис ] $= \dim f(L(A)) = \dim  L(f(A)) = \rank  f(A)$. Так как система векторов линейно независима $\Longleftrightarrow$ линейно независима система координатных столбцов этих векторов, то максимальное число линейно независимых векторов в системе $f(A)$ совпадает с максимальным числом линейно независимых координатных столбцов векторов системы $f(A)$, то есть столбцов матрицы $M_f$, а значит $\rank  f(A) = \rank  M_f$.
		\item Пусть $a \in V$ --- некоторый вектор, $X_a$ --- его координатный столбец в базисе $A$, тогда $f(a)$ имеет координатный столбец равный $M_f X_a$. \\Вектор $a\in Ker\ f \Longleftrightarrow f(a) = 0_v \Longleftrightarrow$ координатный столбец $X_a$ является решением матричного уравнения $M_f X = 0 \Longleftrightarrow M_fX_a = 0$. Значит ядро состоит из векторов, координатные столбцы которых совпадают с пространством решений системы $M_f X = 0$, которое имеет размерность $\dim (Ker\ f) = def= f = \dim V-\rank M_f = \dim V - \rank f.$
	\end{enumerate}
\end{Proof}
\newtheorem*{cor11_4_1}{Следствие}\begin{cor11_4_1}Линейный оператор пространства $V$ инъективен $\Longleftrightarrow$ он сюръективен.
\end{cor11_4_1}\begin{Proof}
	$\Rightarrow)$ Пусть линейный оператор $f$ инъективен. Тогда $Kef\ f = \{0_v\}\Rightarrow def\ f = 0\Rightarrow \rank f = \dim V - def\ f = \dim  V$. Тогда, по теореме о монотонности размерности, подпространство $f(V)$ совпадает с пространством $V$. Следовательно, $f(V) = V$ и $\forall a \in V \exists b : f(a) = b$, значит оператор сюръективен.\\\\
	$\Leftarrow)$ Пусть линейный оператор $f$ сюръективен. Тогда $f(V) = V\Rightarrow \rank  f = \dim  V \Rightarrow def\ f = 0\Rightarrow Ker\ f =\{0_v\} \Rightarrow$ оператор $f$ инъективен.
\end{Proof}











\section{Изменение матрицы линейного оператора при замене базиса. Подобные матрицы.}
$\bullet$ \textit{Пусть $A$ и $B$ --- квадратные матрицы одного порядка над полем $P$. Матрица $A$ называется \textbf{подобной} матрице $B$ над полем $P$, если существует невыоржденная матрица $S$ над полем $P$ такая, что $A = S^{-1}BS$. При этом матрица $S$ называется матрицей, \textbf{трансформирующей} $B$ в $A$.}\\\\
\textit{ \textbf{Свойства подобных матриц:}}
\begin{enumerate}
	\item \textit{Бинарное отношение подобия матриц является отношением эквивалентности.}\begin{Proof}\begin{enumerate}
			\item рефлексивность: $A$ подобна $A$, так как $A = E^{-1}AE$.
			\item симметричность: пусть $A$ подобна $B$. Тогда существует невырожденная матрица $S$ такая, что под. $A = S^{-1}BS$. Обозначим $S' = S^{-1}$. Так как $S$ невырожденная, то $S'$ также невырожденная. Следовательно, $B = S A S^{-1} = (S')^{-1} A S' \Rightarrow B$ подобна $A$.
			\item транзитивность: пусть $B$ подобна $A$, $C$ подобна $B$. Тогда существуют невырожденные матрицы $S_1, S_2$ такие, что $B = S_1^{-1}AS_1, C = S_2^{-1}BS_2\Rightarrow C = S_2^{-1}(S^{-1}AS)S_2= (S_2^{-1}S_1^{-1})A(S_1S_2) = (S_1S_2)^{-1}A(S_1S_2).$ $detS_1S_2 = detS_1\cdot detS_2 \ne 0\Rightarrow C$ подобна $A$.
		\end{enumerate}
	\end{Proof}
	\item \textit{Ранги подобных матриц равны.}\begin{Proof}
		Пусть $A$ подобна $B$ $\Rightarrow$ существует невырожденная матрица $S$ такая, что $A = S^{-1} B S \Rightarrow \rank  A = \rank   (S^{-1} B S ) =$ [так как $S$ невырожденная] $ = \rank  (S^{-1} B) =$ [так как $S^{-1}$ невырожденная] $= \rank  B$.
	\end{Proof}
	\item \textit {Определители подобных матриц равны.}\begin{Proof}
		Пусть $A$ и $B$ подобны. Тогда существует невырожденная матрица $S$ такая, что $A = S^{-1}BS\Rightarrow det A = detS^{-1}\cdot det B\cdot detS = \dfrac{1}{detS}\cdot detB\cdot detS = det B$.
\end{Proof}\end{enumerate}
Пусть $V$ векторное пространство над полем $P$, $A$ и $B$ --- базисы пространства $V$, $f$ --- линейный оператор пространства $V$.
\newtheorem*{th11_5_1}{Теорема}\begin{th11_5_1}Матрицы $M^A$ и $M^B$ линейного оператора $f$ в базисах $A$ и $B$ подобны, причем матрица $S$, трансформирующая $M^A$ и $M^B$, является матрицей перехода от базиса $A$ к базису $B$: $$M^B = (S_{A\rightarrow B})^{-1}M^AS_{A\rightarrow B} =S_{B\rightarrow A}M^AS_{A\rightarrow B} .$$
\end{th11_5_1}\begin{Proof}
	Пусть $x \in V$ --- произвольный вектор и $X_A$ и $X_B$ --- его координатные столбцы в базисах A и B соответственно. Тогда $X_A = S_{A\rightarrow B} X_B = SX_B.$\\\\
	Аналогичное равенство справедливо для $f(x) \in V$, который в базисах $A$ и $B$ имеет столбцы $M^AfX_A$ и $M_f^BX_B \Rightarrow M^A X_A = S M^B X_B \Rightarrow M^A SX_B = SM^BX_B$. Так как полученное соотношение верно для всех векторов из пространства $V$, то оно, в частности, верно для $b_1$ (первого вектора из системы $B$), который в базисе $B$ имеет координатный столбец $\begin{pmatrix}
		1\\ \vdots \\0
	\end{pmatrix}$ $\Rightarrow$ первые столбцы $M^A S$ и $SM^B$ равны. Продолжая рассуждения аналогичным образом и используя остальные векторы из базиса $B$, получим равенства остальных столбцов, а значит $M^A S = SM^B$. Так как $S$ --- матрица перехода от базиса к базису, то она невырожденная $\Rightarrow  M^B = S^{-1}M^A S$, где $S = S_{A\rightarrow B}.$
\end{Proof}












\section{Инвариантное подпространство.}
$\bullet$ \textit{Пусть $f$ --- линейный оператор векторного пространства $V$ над полем $P$. Подпространство $U\subseteq V$ называется \textbf{инвариантным} относительно $f$, если $f(U)\subseteq U$.}\\\\
\textit{\textbf{Свойства инвариантных подпространств:}}
\begin{enumerate}
	\item \textit{Сумма и пересечение подпространств, инвариантных относительно линейного оператора $f$, также инвариантны относительно $f$.}\begin{Proof} Пусть $U_1, U_2$ инвариантны относительно оператора $f$ $\Rightarrow f(U_1) \subseteq U_1, f(U_2) \subseteq U_2.$\begin{enumerate}
			\item $\forall a \in U_1 + U_2\quad a = \underset{\in U_1}{a_1} + \underset{\in U_2}{a_2}\Rightarrow f(a) = f(a_1 + a_2) = \underset{\in U_1}{f(a_1)} + \underset{\in U_2}{f(a_2)} \Rightarrow$ так как $U_1$ и $U_2$ инвариантны относительно оператора $f$, то $f(a) \in U_1 + U_2$, то есть $U_1 + U_2$ инвариантно относительно $f$.
			\item $\forall a \in U_1 \cap U_2 \Rightarrow a\in U_1,\ a \in U_2 \Rightarrow$ $[$так как $U_1$ и $U_2$ инвариантны относительно оператора $f]$ $\Rightarrow f(a) \in U_1, \ f(a) \in U_2 \ \Rightarrow f(a) \in U_1\cap U_2 \Rightarrow$ $U_1\cap U_2$ инвариантно относительно $f$.
		\end{enumerate}
	\end{Proof}
	\item \textit{Если подпространство $U$ инвариантны относительно линейных операторов $f$ и $g$, то $U$ инвариантно относительно операторов $f + g, \alpha f, f \circ g$.}\begin{Proof} Если
		$a \in U$, то $f(a) \in U, \ g(a) \in U$, так как $U$ инвариантно относительно $f$ и $g$ $\Rightarrow$\\
		$(f + g)(a) = f(a) + g(a) \in U,$\\
		$(\alpha f)(a) = \alpha f(a)\in U,$\\
		$(f \circ g)(a) = f(g(a)) \in U.$
	\end{Proof}
\end{enumerate}
$\bullet$ \textit{Пусть $U$ --- инвариантное относительно линейного оператора $f$ подпространство пространства $V$. Тогда отображение $f|_U : U\rightarrow U$ такое, что $f|_U(a) = f(a), \forall a \in U$, является линейным оператором пространства $U$ и называется \textbf{ограничением} оператора $f$ на подпространство $U$.}
\newtheorem*{th11_6_1}{Теорема}\begin{th11_6_1}Если векторное пространство $V$ имеет нетривиальное инвариантное относительно линейного оператора $f$ подпространство, то существует базис пространства $V$, в котором матрица оператора $f$ имеет блочнотреугольный вид.
\end{th11_6_1}\begin{Proof} Пусть $V$ --- $n$-мерное векторное пространство, $U$ --- нетривиальное относительно линейного оператора $f$ подпространство и $\dim  U = k, \ 0<k<n.$ Построим систему векторов $A_1(a_1, \dots, a_k)$, являющуюся базисом пространства $U$. Дополним систему $A_1$ до базиса пространства $V: A = (\underbrace{a_1, \dots,  a_k}_{U}, a_{k+1}, \dots , a_n)$ и построим матрицу $M_f$ следующим образом: разложим вектор $f(a_i)$ по базису $A$, так как $(a_1,\dots,a_k)$ принадлежит инвариантному относительно $f$ подпространству $U$, то $f(a_1),\dots,f(a_k)\in U\Rightarrow$ разложения этих векторов по базису $A$  содержат лишь векторы из системы $A_1$:\\
	$$\begin{cases}
		f(a_1) = \alpha_{11}a_1 + \ldots + \alpha_{k1}a_k,\\
		\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\\
		f(a_k) = \alpha_{1k}a_1 + \ldots + \alpha_{kk}a_k,\\
		f(a_{k+1}) = \alpha_{1k+1}a_1 + \ldots + \alpha_{kk+1}a_k + \ldots + \alpha_{nk+1}a_n,\\
		\dotfill\\
		f(a_{n}) = \alpha_{1n}a_1 + \ldots + \alpha_{kn}a_k + \ldots + \alpha_{nn}a_n.
	\end{cases}$$
	$$M^A_f = \left(  \begin{tabular}{c|c}
		$\begin{tabular}{cc} $\begin{matrix} \alpha_{11} & \dots & \alpha_{1k} \\ \vdots & \ddots & \vdots \\ \alpha_{k1} & \dots & \alpha_{kk} \end{matrix}$ \end{tabular}$ & $\begin{matrix} \alpha_{1,k+1} & \dots & \alpha_{1n} \\ \vdots & \ddots & \vdots \\ \alpha_{k,k+1} & \dots & \alpha_{kn} \end{matrix}$ \\ \hline $\begin{matrix} 0 \ \ & \dots & 0 \\ \vdots \ \ & \ddots  & \vdots \\ 0\ \ & \dots & 0 \end{matrix}$ & $\begin{matrix} \alpha_{k,k+1} & \dots & \alpha_{kn} \\ \vdots & \ddots & \vdots \\ \alpha_{n,k+1} & \dots & \alpha_{nn} \end{matrix}$ \end{tabular} \right).$$
\end{Proof}\\
\textbf{Замечание:} Блок 		$\begin{pmatrix} \alpha_{11} & \dots & \alpha_{1k}  \\ \vdots & \ddots & \vdots \\ \alpha_{k1} & \dots & \alpha_{kk} \end{pmatrix}$ является матрицей линейного оператора $f|_U$ в базисе $(a_1, \dots, a_k).$ 
\newtheorem*{cor11_6_1}{Следствие}\begin{cor11_6_1}Если векторное пространство $V$ является прямой суммой инвариантных относительно линейного оператора $f$ подпространств, то существует базис пространства $V$, в котором матрица оператора $f$ имеет блочнодиагональный вид.
\end{cor11_6_1}\begin{Proof} Пусть $V = U_1 \oplus U_2$, $U_1$ и $U_2$ --- инвариантные относительно оператора $f$ подпространства пространства $V$.\\
	Если $ A_1,\ A_2$ --- базисы $U_1,\ U_2$ соответственно, то, по следствию из теоремы Грассмана, система векторов $(A_1, A_2)$ является базисом пространства $V$. При этом матрица линейного оператора $f$ блочнодиагональная, так как $f(A_1)$ раскладывается по базису $A_1$, а $f(A_2)$ --- по базису $A_2.$
\end{Proof}
\newtheorem*{cor11_6_2}{Следствие}\begin{cor11_6_2}Если векторное пространство $V$ является прямой суммой одномерных инвариантных относительно линейного оператора $f$ подпространств, то существует базис пространства $V$, в котором матрица оператора $f$ является диагональной.
\end{cor11_6_2}\begin{Proof}
	Доказательство проводится аналогично предыдущему следствию.
\end{Proof}\\\\
$\bullet$ \textit{Линейный оператор $f$ векторного пространства $V$ называется  \textbf{оператором простой структуры}, если существует базис пространства $V$, в котором матрица оператора $f$ имеет диагональный вид.}\\\\
$\bullet$ \textit{Матрица называется \textbf{диагонализируемой}, если она имеет подобную диагональную матрицу.}














\section{Одномерное инвариантное подпространство. Собственные векторы и собственные значения линейного оператора.}
Пусть $f$ --- линейный оператор векторного пространства $V$ над полем $P$.\\\\
$\bullet$ \textit{Если для некоторого ненулевого вектора $a\in V$ справедливо равенство $f(a) = \lambda_0 a$, где $\lambda_0 \in P$,  то скаляр $\lambda_0$ называется \textbf{собственным значением} оператора $f$, а вектор $a$ --- собственным вектором оператора $f$, соответствующим собственному значению $\lambda_0$.}
\newtheorem*{lem11_7_1}{Лемма}\begin{lem11_7_1}Собственные векторы, соответствующие попарно различным собственным
	значениям, линейно независимы.
\end{lem11_7_1}\begin{Proof}
	От противного. Пусть ($a_1, \dots, a_k$) --- cистема векторов, состоящая из собственных векторов оператора $f$, соотвестветствующих попарно различным собственным значениям $\lambda_1, \dots, \lambda_k$, и пусть она линейно зависима. Тогда базис этой системы векторов состит менее чем из $k$ векторов, и, без ограничения общности, система векторов $A(a_1, \dots, a_s)$ будет являться базисом пространства при условии, что $s < k$. Тогда вектор $a_{s+1}$ линейно выражается через систему $A$, то есть представим в виде\\
	$a_{s+1}$ = $\alpha_1a_1 + \ldots + \alpha_sa_s$. Домножим на $\lambda_{s+1}$ и получим $$\lambda_{s+1}a_{s+1} =  \alpha_1 \lambda_{s+1} a_1 + \ldots + \alpha_s \lambda_{s+1}a_s.\eqno (11.7.1)$$
	С другой стороны, $\lambda_{s+1}a_{s+1} = f(a_{s+1}) = f(\alpha_1a_1 + \ldots + \alpha_sa_s) =  \alpha_1 f(a_1) + \ldots + \alpha_sf(a_s)\Rightarrow$ $$\lambda_{s+1}a_{s+1} = \alpha_1\lambda_1a_1 + \ldots + \alpha_s\lambda_sa_s.\eqno (11.7.2)$$
	Вычтем из уравнения (11.7.1) уравнение (11.7.2) и получим $0_v = \alpha_1(\lambda_{s+1}  - \lambda_1)a_1 + \ldots + \alpha_s(\lambda_{s+1} - \lambda_s)a_s$. Так как система $A$ линейно независима, то коэффициенты линейной комбинации нулевые, то есть $\alpha_i(\lambda_{s+1}  - \lambda_1) = 0\quad\forall i$. А
	так как все собственные значения $\lambda_i$ попарно различны, то $(\lambda_{s+1} - \lambda_i)$ $\not=$ 0 $\Rightarrow \alpha_i = 0 \quad\forall i=\overline{1,s} \Rightarrow a_{s+1} = 0_v$, что противоречит тому, что вектор $a_{s+1}$ собственный.
\end{Proof}
\newtheorem*{cor11_7_1}{Следствие}\begin{cor11_7_1}Линейный оператор $n$-мерного векторного пространства не может иметь больше чем $n$ различных собственных значений.
\end{cor11_7_1}
\newtheorem*{th11_7_1}{Теорема}\begin{th11_7_1}Одномерное подпространство векторного пространства $V$ является инвариантным относительно линейного оператора $f$ $\Longleftrightarrow$ все его ненулевые векторы являются собственными векторами оператора $f$.
\end{th11_7_1}\begin{Proof}
	$\Rightarrow)$ Пусть одномерное векторное пространство $U$ инвариантно относительно линейного оператора $f$. Тогда $\forall$ $a$ $\in$ $U$, $a$ $\not=$ 0\quad $f(a)$ $\in$ $U$. Но так как подпространство $U$ одномерное, то ненулевой вектор $a$ является базисом этого подпространства. Следовательно любой вектор из $U$, в том числе и сам вектор $a$, представим в виде линейной комбинации $f(a) = \lambda a$, то есть вектор $a$ является собственным векторов оператора $f$.\\\\ $\Leftarrow$) Пусть прозвольный вектор $a$ $\not=$ 0 и $a$ $\in$ $U$, $f(a) = \lambda a$. Так как $U$ --- подпространство, то $\lambda a$ $\in$ $U$ $\Rightarrow f(a) \in U$. Заметим, что $f(0_v) = 0_v,\ f(0_v) \in U \Rightarrow \forall a \in U\quad f(a) \in U\Rightarrow f(U)$ является подпространством пространства $U$, а значит пространство $U$ инвариантно относительно оператора $f$.
\end{Proof}
\newtheorem*{th11_7_2}{Теорема}\begin{th11_7_2}Линейный оператор векторного пространства $V$ является оператором простой структуры $\Longleftrightarrow$ существует базис пространства $V$, состоящий из собственных векторов оператора.
\end{th11_7_2}\begin{Proof}
	$\Rightarrow$) Пусть $f$ --- оператор простой структуры. Тогда существует базис $\asys$ пространства $V$, в котором матрица линейного оператора диагональна и имеет вид $$M^A_f = \begin{pmatrix} \lambda_1 & 0 & \dots & 0 \\ 0 & \lambda_2 & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & \lambda_n \end{pmatrix}.$$ Тогда \\$f(a_1) = \lambda_1 a_1 + 0\cdot a_2 + \ldots + 0\cdot a_n = \lambda_1 a_1$,\\$f(a_2) = 0\cdot a_1 + \lambda a_2 + \ldots + 0\cdot a_n = \lambda_2 a_2$,\\ $\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots$\\ $f(a_n) =0\cdot a_1 + 0\cdot a_2+\ldots+ \lambda_n a_n = \lambda_n a_n$.\\ Следовательно, $f(a_i) = \lambda_i a_i$, причем, так как векторы $a_i$ ненулевые, поскольку система $A$ является базисом, они являются собственными векторами оператора $f$. \\\\
	$\Leftarrow$) Пусть система векторов $A(a_1, \dots, a_n)$ является базисом пространства $V$, и пусть она состоит из собственных векторов линейного оператора $f$, соответствующих попарно различным собственным значениям $\lambda_i$. Тогда $f(a_i) = \lambda_i a_i$ $\Rightarrow$ $M^A_f = diag(\lambda_1,\dots,\lambda_n)$ $\Rightarrow$ $f$ --- оператор простой структуры.
\end{Proof}
\newtheorem*{cor11_7_2}{Следствие}\begin{cor11_7_2}Линейный оператор $n$-мерного векторного пространства, имеющий $n$ различных собственных значений, является оператором простой структуры.
\end{cor11_7_2}\begin{Proof}
	Если линейный оператор имеет $n$ различных собственных значений, то он имеет $n$ собственных векторов, соответствующих этим собственным значениям. Тогда по лемме эти векторы линейно независимы. Значит они образуют базис этого пространства. Следовательно, $f$ --- оператор простой структуры.
\end{Proof}











\section{Алгебраическая кратность собственного значения.}
Пусть $f$ --- линейный оператор $n$-мерного векторного пространства $V$ над полем $P$.
\newtheorem*{th11_8_1}{Теорема}\begin{th11_8_1}\end{th11_8_1}\begin{enumerate}
	\item \textit{Вектор $x$ $\in$ $V$ является собственным вектором линейного оператора $f$, соответствующим собственному значению $\lambda_0$ $\Longleftrightarrow$ его координатный столбец $X$ в некотором базисе является ненулевым решением уравнения $(M_f - \lambda_0 E)X = 0$, где $M_f$ --- матрица оператора $f$ в том же базисе.}
	\item \textit{Скаляр $\lambda_0$ $\in P$ является собственным значением линейного оператора $f$ $\Longleftrightarrow$ $det(M_f - \lambda_0 E) = 0$.}
\end{enumerate}
\begin{Proof}\begin{enumerate}
		\item Ненулевой вектор $x$ является собственным вектором линейного оператора $f$, соответствующим собственному значению $\lambda_0$, если $f(x) = \lambda_0x$. Два вектора равны $\Leftrightarrow$ равны их координатные столбцы. Если $X$ --- это координатный столбец вектора $x$, то $\lambda_0 x$ имеет координатный столбец $\lambda_0 X$, а $f(x)$ --- $M_f X$. Тогда $\lambda_0 X = M_f X \Leftrightarrow M_f X -\lambda_0 X = 0 \Leftrightarrow M_f X -\lambda_0 E X = 0\Leftrightarrow(M_f  -\lambda_0 E) X = 0$.
		\item Скаляр $\lambda_0$ является собственным значением линейного оператора $f$ $\Longleftrightarrow$ существует собственный вектор, соответствующий этому значению. А значит существует ненулевое решение матричного уравнения $(M_f - \lambda_0 E)X = 0$, что возможно лишь в случае, когда $det(M_f - \lambda E)X = 0.$
	\end{enumerate}
\end{Proof}\\
$\bullet$ \textit{Пусть $A$ --- квадратная матрица порядка $n$ над полем $P$, $\lambda$ --- переменная. Матрица $A - \lambda E$ называется \textbf{характеристической матрицей}, определитель $det(A - \lambda E)$ --- \textbf{характеристическим многочленом} матрицы $A$, уравнение $det(A - \lambda E) = 0$ --- \textbf{характеристическим уравнением} матрицы $A$.}\\\\
\textbf{\textit{Cвойства характ мн-ена:}}\begin{enumerate}
	\item \textit{Степень характеристического многочлена матрицы равна порядку матрицы.}
	\item \textit{Если матрица $A$ имеет порядок $n$, то старший коэффициент характеристического многочлена матрицы $A$ равен $(-1)^n$.}
\end{enumerate}
$\bullet$ \textit{Сумма элементов главной диагонали матрицы $A$ называется \textbf{следом матрицы} $A$. Обозначение: $SpA = (\alpha_{11} + \ldots + \alpha_{nn})$}.
\begin{enumerate}
	\item[3.] \textit{Коэффициент при $\lambda^{n-1}$ равен $(-1)^{n-1}SpA$.}
	\item[4.] \textit{Свободный член равен $detA$.}
\end{enumerate}
\newtheorem*{lem11_8_1}{Лемма}\begin{lem11_8_1}Характеристические многочлены подобных матриц равны.
\end{lem11_8_1}\begin{Proof}
	Если матрицы $A$ и $B$ подобны, то существует невырожденная матрица $S$ такая, что $A = S^{-1}BS$. Тогда $A-\lambda E = S^{-1}BS - \lambda S^{-1}ES = S^{-1}(B-\lambda E) S \Rightarrow det(A-\lambda E) = det S^{-1} det(B-\lambda E) det S = det(B-\lambda E)$.
\end{Proof}
\newtheorem*{cor11_8_1}{Следствие}\begin{cor11_8_1}Следы подобных матриц равны.
\end{cor11_8_1}
\newtheorem*{cor11_8_2}{Следствие}\begin{cor11_8_2}Определители подобных матриц равны.
\end{cor11_8_2}
Так как матрицы линейного оператора в различных базисах подобны, то их характеристические многочлены равны и, следовательно, зависят от самого оператора и не зависят от выбора базиса.\\\\
$\bullet$ \textit{Характеристический многочлен матрицы линейного оператора называется \textbf{характеристическим многочленом оператора}.}\\\\
$\bullet$ \textit{Из леммы следует, что собственное значение линейного оператора является корнем характеристического уравнения оператора. Его кратность называется \textbf{алгебраической кратностью} собственного значения.}\\\\
Так как степень характеристического многочлена линейного оператора пространства $V$ равна $\dim V$, то сумма алгебраических кратностей собственных значений линейного оператора не превышает размерности пространства $V$.






\section {Геометрическая кратность собственного значения.}
Пусть $f$ --- линейный оператор $n$-мерного векторного пространства $V$ над полем $P$.
\newtheorem*{lem11_9_1}{Лемма}\begin{lem11_9_1}Если $a_1,\dots,a_k$ --- собственные векторы линейного оператора $f$, соответствующие одному собственному значению $\lambda_0$, то любая ненулевая линейная комбинация этих векторов также является собственным вектором, соответствутющим собственному значению $\lambda_0$.
\end{lem11_9_1}\begin{Proof}
	$f(\alpha_1 a_1+ \ldots +\alpha_n a_k) = \alpha_1 f(a_1) + \ldots + \alpha_n f(a_k) = \alpha_1 \lambda_0 a_1 + \ldots + \alpha_k \lambda_0 a_k = \lambda_0(\alpha_1 a_1+ \ldots +\alpha_n a_k) $
\end{Proof}
\newtheorem*{cor11_9_1}{Следствие}\begin{cor11_9_1}Множество, состоящее из нулевого вектора и всех
	собственных векторов линейного оператора $f$, соответствующих одному собственному значению $\lambda_0$, является
	подпросторанством пространства $V$.
\end{cor11_9_1}
$\bullet$ \textit{Подпространство, состоящее из нулевого вектора и всех собственных векторов линейного оператора, соответствующих одному собственному значению $\lambda_0$, называется \textbf{собственным подпространством} оператора, соответствующим собственному значению $\lambda_0$. Обозначение: $L(\lambda_0)$.}\\\\
$\bullet$ \textit{\textbf{Геометрической кратностью} собственного значения линейного оператора называется размерность собственного подпространства, соответствующего этому собственному значению.}\\\\
\textit{\textbf{Свойства геометрической кратности собственного значения:}}\begin{enumerate}
	\item \textit{Геометрическая кратность $r_0$ собственного значения $\lambda_0$ равна} $$r_0 = \dim V - \rank (M_f-\lambda_0 E).$$
	\begin{Proof}
		Вектор $X$ принадлежит собственному подпространству $L(\lambda_0) \Longleftrightarrow$ его координатный столбец является решением матричного уравнения $(M_f-\lambda_0 E)X = 0\Rightarrow$ размерность $L(\lambda_0)$ равна размерности пространства решений линейной системы $(M_f-\lambda_0 E)X = 0$, то есть равна  $\dim V - \rank (M_f-\lambda_0 E)$.
	\end{Proof}
	\item \textit{Геометрическая кратность собственного значения линейного оператора не превышает его алгебраической кратности}.\begin{Proof} Пусть собственное значение $\lambda_0$ имеет геометрическую кратность равную $r_0$ и пусть система векторов $(a_1, \dots, a_r)$ --- базис собственного подпространства $L(\lambda_0)$. Дополним его до базиса всего пространства $V$ и построим матрицу $M_f$. Так как векторы $(a_1, \dots, a_r)$ --- собственные векторы оператора $f$, соответствующие собственному значению $\lambda_0$, то $f(a_i) = \lambda_0 a_i,\ i = \overline{1, r}.$ Тогда матрица $M_f$ имеет вид\\
		$M_f = \left( \begin{tabular}{c|c}
			$\begin{tabular}{cc} $\begin{matrix} \lambda_0 & 0 & \dots & 0 \\ 0 & \lambda_0 & \ddots & 0 \\ 0 & 0 & \dots & \lambda_0 \end{matrix}$ \end{tabular}$ & $B$ \\ \hline $\begin{matrix} 0 & \ 0 & \ \dots & 0  \end{matrix}$ & $C$ \end{tabular} \right) = \bigg(\begin{tabular}{c|c}
			$\begin{tabular}{cc} $\lambda_0 E_r$ \end{tabular}$ & $B$ \\ \hline 0 & $C$
		\end{tabular}\bigg)$, строим характеристичекий многочлен:
		$det(M_f - \lambda E) = det((\lambda_0 - \lambda) E_r) det(C - \lambda E) = (\lambda - \lambda_0)^{r} det(C - \lambda E) \Rightarrow$ алгебраическая кратность собственного значения $\lambda_0$ не меньше $r.$
	\end{Proof}
\end{enumerate}
\newtheorem*{cor11_9_2}{Следствие}\begin{cor11_9_2}Сумма геометрических кратностей собственных занчений линейного оператора пространства $V$ не превосходит размерности пространства $V$.
\end{cor11_9_2}\begin{Proof}
	$ \sum\limits_i r_i \ \leqslant \sum\limits_i k_i \ \leqslant n = \dim  V \quad \boxtimes$
\end{Proof}
\newtheorem*{th11_9_1}{Теорема}\begin{th11_9_1}Линейный оператор векторного пространства $V$ является оператором простой структуры $\Longleftrightarrow$
	сумма геометрических кратностей собственных значений оператора равна размерности пространства $V$.
\end{th11_9_1}\begin{Proof}
	$\Rightarrow$) Пусть $f$ --- операторо простой структуры. Тогда существует базис $\asys$ пространства $V$, состоящий из собственных векторов этого оператора. Обозначим через $\lambda_1, \dots, \lambda_k$ собственные значения оператора $f$, а через $s_1, \dots, s_k$ --- количество собственных векторов в базисе $A$, соответствующих этим собственным значениям. Тогда $s_1 + \ldots + s_k = \dim  V = n$. Так как количество векторов в линейно независимой системе векторов не может превышать размерности пространства, то количество собственных векторов в базисе $A$, соответствующих собственному значению $\lambda_i$, не превышает размерности собственного подпространства $L(\lambda_i))$, то есть $s_i$ $\leqslant$ $r_i$, где $r_i$ --- геометрическая кратность собственного значения $\lambda_i$ $\Rightarrow$ [по предыдущему следствию] $s_1 + \ldots + s_k \leqslant r_1 + \ldots + r_k$ $\leqslant n \Rightarrow r_1 + \ldots + r_k \leqslant n\Rightarrow r_1 + \ldots + r_k = n$.\\\\
	$ \Leftarrow$) Пусть сумма геометрических кратностей собственных значений линейного оператора $f$ равна $n$. Тогда объединение базисов собственных подпространств линейного оператора состоит из $n$ векторов, которые являются собственным векторами и линейно независимы, так как соответствуют различным собственным значениям $\Rightarrow$ построенная система векторов является базисом пространства $V$, состоящим из собственных векторов $f \Rightarrow$ оператор $f$ --- оператор простой структуры.
\end{Proof}











\chapter{Полиномиальные матрицы}

\section{Эквивалентность полиномиальных матриц. Каноническая форма полиномиальной матрицы.}
$\bullet$ \textit{\textbf{Полиномиальным матрицами} называются матрицы, элементами которых являются многочлены над некоторым полем $P$.}\\\\
В дальнейшем будем рассматривать лишь квадратные полиномиальные матрицы с переменной $\lambda$ вида: $$A(\lambda) = \begin{pmatrix} a_{11}(\lambda)& \dots & a_{1n}(\lambda) \\ \vdots & \ddots & \vdots \\ a_{n1}(\lambda) & \dots & a_{nn}(\lambda) \end{pmatrix}.$$
$\bullet$ \textit{\textbf{Элементарными преобразованиями} строк полиномиальной матрицы называются следующие два преобразования:}
\begin{enumerate}
	\item \textit{умножение строки полиномиальной матрицы на ненулевой элемент поля $P$;}
	\item \textit{прибавление к строке полиномиальной матрицы другой ее строки, умноженной на произвольный многочлен $f(\lambda)\in P[\lambda]$}.
\end{enumerate}
\textit{Элементарные преобразования столбцов полиномиальной матрицы определяются аналогично.}\\\\
$\bullet$ \textit{Если матрица $B(\lambda)$ может быть получена из матрицы $A(\lambda)$ в результате применения к ней конечного числа элементарных преобразований строк и столбцов, то говорят, что матрица $B(\lambda)$ \textbf{эквивалентна} матрице $A(\lambda)$. (Обозначение: $B(\lambda)$ $\sim$ $A(\lambda)$)}\\\\
Эквививалентость полиномиальных матриц рефлексивна, транзитивна и симметрична.\\\\
$\bullet$ \textit{Диагональная матрица $K(\lambda) = diag(f_1(\lambda),\dots,f_n(\lambda))$ называется \textbf{канонической}, если}
\begin{enumerate}
	\item \textit{ненулевые многочлены $f_i(\lambda)$ имеют старший коэффициент 1;}
	\item \textit{каждый диагональный многочлен $f_i(\lambda),\ i<n$, является делителем следующего элемента $f_{i+1}(\lambda)$}.
\end{enumerate}
\newtheorem*{th12_1_1}{Теорема}\begin{th12_1_1} Для любой полиномиальной матрицы существует эквивалентная каноническая матрица.
\end{th12_1_1}
\begin{Proof}
	Пусть $A(\lambda)$ --- полиномиальная матрица порядка $n$. Если матрица $A(\lambda)$ нулевая, то она каноническая матрица. Пусть матрица $A(\lambda)$ ненулевая. Докажем существование эквивалентной канонической матрицы по индукции по порядку матрицы $n$.\\\\
	Пусть $n$ = 1. Тогда $A(\lambda)$ = $(a(\lambda))$ (матрица состоящая из одного элемента). Умножим $A(\lambda)$ на элемент поля $P$, обратный старшему коэффициенту многочлена $a(\lambda)$ и получим каноническую, эквививалентную исходной.\\\\
	Пусть $n$ > 1 и пусть для любой матрицы порядка $n-1$ существует эквивалентная каноническая. Покажем, что матрица $A(\lambda)$ может быть приведена элементарными преобразованиями к канонической матрице:\\
	Пусть $S$ --- класс эквививалентости, содержащий матрицы, эквивалентные матрице $A(\lambda)$. Возьмём из $S$ матрицу $B(\lambda)$ такую, что\begin{enumerate}
		\item $b_{11}(\lambda) \not= 0$;
		\item старший коэффициент $b_{11}$ равен 1;
		\item степень многочлена $b_{11}(\lambda)$ является наименьшей среди степеней всех многочленов, являющихся элементами, образующими выбранную матрицу из $S$.
	\end{enumerate}
	Разделим многочлен $b_{12}(\lambda)$ с остатком на $b_{11}(\lambda)$, то есть $b_{12}(\lambda) = b_{11}(\lambda)q(\lambda) + r(\lambda)$, где $\deg \ r(\lambda) < \deg \ b_{11}(\lambda)$ или $r(\lambda) = 0$. Затем ко второму столбцу $B(\lambda)$ прибавим первый столбец, умноженный на $(-q(\lambda))$. Получим матрицу $B_1(\lambda)$, у которой элемент первой строки второго столбца равен $r(\lambda)$. Преобразование является элементарным $\Rightarrow B_1(\lambda) \sim B(\lambda) \Rightarrow B_1(\lambda) \in S$.\\
	Если $r(\lambda) \not= 0$, то в $S$ существует матрица $B_1(\lambda)$, у которой один из элементов имеет степень меньшую, чем степень $b_{11}(\lambda)$, что противоречит тому, что $\deg (b_{11}(\lambda))$ наименьшая в $S \Rightarrow r(\lambda) = 0 \Rightarrow$ первый элемент второго столбца $B_1$ равен 0.\\
	Проведем аналогичные преобразования для остальных элементов первой строки и первого столбца матрицы $B(\lambda)\Rightarrow$ получим матрицу\begin{center}
		$\widetilde{B}(\lambda)$ = $\left( \begin{tabular}{c|c}
			$\begin{tabular}{cc} $b_{11}(\lambda)$ \end{tabular}$ & $\begin{matrix} 0\quad&\dots &\quad0 \end{matrix}$ \\ \hline $\begin{matrix} 0 \\ \vdots \\ 0 \end{matrix}$ & $\begin{matrix} c_{22}(\lambda) & \dots & c_{2n}(\lambda)  \\ \vdots & \ddots & \vdots \\ c_{n2}(\lambda) & \dots & c_{nn}(\lambda) \end{matrix}$ \end{tabular} \right)$ = $\left( \begin{tabular}{c|c}
			$\begin{tabular}{cc} $b_{11}(\lambda)$ \end{tabular}$ & $0$ \\ \hline $\begin{matrix} 0  \end{matrix}$ & $C(\lambda)$ \end{tabular} \right)$.
	\end{center}
	Матрица $C(\lambda)$ --- полиномиальная матрица порядка $n-1$ $\Rightarrow$ по индуктивному предположению имеет эквивалентную каноническую матрицу $\widetilde{C}(\lambda) = diag (\widetilde{C}_{2}(\lambda), \dots, \widetilde{C}_{n}(\lambda))$.\\ 
	Если эти преобразования применить к матрице $\widetilde{B}(\lambda)$, то они не будут менять элементы первой строки и первого столбца и приведут к диагональной матрице\\ $K(\lambda) = diag (b_{11}(\lambda), \widetilde{C}_{2}(\lambda), \dots, \widetilde{C}_{n}(\lambda)).$ Покажем, что она является канонической.\\		
	Старшие коэффициенты ненулевых многочленов матрицы $A(\lambda)$ равны 1 и $\forall i\quad \widetilde{C}_i(\lambda)$ делит $\widetilde{C}_{i+1}(\lambda)$. Покажем, что $b_{11}(\lambda)$ делит $ \widetilde{C}_1(\lambda)$. \\
	Прибавим к первой строке вторую. Преобразование элементарное, поэтому эта матрица $\in S$. По доказанному выше, для любой матрицы из $S$, первый элемент которой $b_{11}$, все элементы первой строки делятся на $b_{11} \Rightarrow \widetilde{C}_{2}(\lambda) | b_{11} \Rightarrow\widetilde{C}_{2}(\lambda)$ --- каноническая матрица. А значит и матрица $K(\lambda)$ каноническая. 
\end{Proof}\\\\
$\bullet$ \textit{Каноническая матрица $K(\lambda)$, эквивалентная матрице $A(\lambda)$, называется \textbf{канонической формой} матрицы $A(\lambda)$}.















\section {Единственность канонической формы. Система НОД миноров полиномиальных матриц.}
$\bullet$ \textit{Пусть ранг полиномиальной матрицы $A(\lambda)$ равен $r$. Тогда все миноры матрицы $A(\lambda)$ порядка выше $r$ равны нулю, а среди миноров порядка $k \in \{1, \dots, r\}$ существует ненулевой. Система многочленов $d_1(\lambda)$, $\dots$, $d_n(\lambda)$ называется \textbf{системой НОД миноров} матрицы $A(\lambda)$, если при $k \in \{1, \dots, r\}$ многочлен $d_k(\lambda)$ равен НОД всех миноров $k$-ого порядка матрицы $A(\lambda)$ со старшим коэффициентом 1, а при $k \in \{r+1, \dots, n\}$ $d_k(\lambda)$ = 0.}
\newtheorem*{lem12_2_1}{Лемма}\begin{lem12_2_1}Системы НОД миноров эквивалетных полиномиальных матриц совпадают.
\end{lem12_2_1}\begin{Proof}
	Покажем, что НОД миноры не изменяются при элементарных преобразованиях строк:
	\begin{enumerate}
		\item Пусть $i$-ая строка матрицы $A(\lambda)$ умножена на ненулевой скаляр $\alpha$. Тогда миноры, в которые эта строка не входит, не изменяются, а миноры, в которые входят элементы этой строки, умножаются на $\alpha$. Но НОД системы многочленов не меняется, если некоторые из этих многочленов умножаются на ненулевую константу.
		\item Пусть матрица $B(\lambda)$ получена из матрицы $A(\lambda)$ прибавлением к $i$-ой строке элементов $j$-ой строки, умноженных на многочлен $c(\lambda)$. При этом миноры, в которые не входят элементы $i$-ой строки, не изменяются. Не меняются также миноры, в которые входят одновременно и элементы $i$-ой строки, и элементы $j$-ой. Рассмотрим миноры, в которые входят элементы $i$-ой строки, но не входят элементы $j$-ой, то есть
		\begin{multline*}
			|a_{i1}(\lambda) + c(\lambda)a_{j1}(\lambda) + \ldots + a_{in}(\lambda) + a_{jn}(\lambda) c(\lambda)| =\\= |\underbrace{a_{i1}(\lambda) \dots a_{in}(\lambda)}_{M_1}| + c(\lambda)|\underbrace{a_{j1}(\lambda) +\ldots + a_{jn}(\lambda)}_{M_2}| = M_1 + c(\lambda)M_2.
		\end{multline*}		
		Заметим, что определители $M_1$ и $M_2$ с точностью до знака являются минорами матрицы $A(\lambda)$. Следовательно, все миноры матрицы $B(\lambda)$ либо равны минорам матрицы $A(\lambda)$, либо линейно выражаются через них.
		\par\bigskip
		
		Пусть $d^{A}_k (\lambda)$ и $d^{B}_k (\lambda)$ --- $k$-тые многочлены в системе НОД миноров матриц $A(\lambda)$ и $B(\lambda)$ соответственно. И пусть $\rank  A(\lambda) = r$. Тогда если $k \in \{r+1, \dots, n\}$, то $d^{A}_k (\lambda)$ = 0 $\Rightarrow$ все миноры $k$-того порядка матрицы $A(\lambda)$ равны 0 $\Rightarrow$ все миноры $k$-того порядка матрицы $B(\lambda)$ также равны 0 $\Rightarrow$ $d^{B}_k (\lambda)$ = 0.
		
		Пусть $k \in \{1, \dots, r\}$. Тогда многочлен $d^{A}_k (\lambda)$ --- НОД миноров $k$-того порядка матрицы $A(\lambda)$ $\Rightarrow$ он делит все миноры $k$-того порядка матрицы $A(\lambda)$ $\Rightarrow$ он делит все миноры $k$-того порядка и матрицы $B(\lambda)$ $\Rightarrow$ $d^{A}_k (\lambda)$ делит НОД миноров $k$-того порядка матрицы $B$, то есть $d^{A}_k (\lambda)$ | $d^{B}_k (\lambda)$.
		
		Заметим, что матрица $A(\lambda)$ может быть получена из матрицы $B(\lambda)$ с помощью обратного элементарного преобразования $\Rightarrow$ $d^{B}_k (\lambda)$ | $d^{A}_k (\lambda)$, и так как их старшие коэффициенты равны 1, то $d^{B}_k (\lambda)$ = $d^{A}_k (\lambda)$. \end{enumerate}
\end{Proof} 
\newtheorem*{th12_2_1}{Теорема}\begin{th12_2_1}Каноническая форма полиномиальной матрицы определяется однозначно.
\end{th12_2_1}\begin{Proof}
	Пусть $A(\lambda)$ --- произвольная полиномиальная матрица, $\rank A(\lambda) = r$, $d_1(\lambda),\dots,d_n(\lambda)$ --- система НОД миноров матрицы $A(\lambda)$ и матрица $K(\lambda) = diag(f_1(\lambda),\dots,f_n(\lambda))$ --- ее некоторая каноническая форма. Тогда $K(\lambda)\sim A(\lambda) \Rightarrow$ их системы НОД миноров совпадают
	\begin{align*}
		&d_1(\lambda) = f_1(\lambda), \\
		&d_2(\lambda) = f_1(\lambda)f_2(\lambda),\\
		&d_3(\lambda)=f_1(\lambda)f_2(\lambda)f_3(\lambda),\\
		&\dots\dots\dots\dots\dots\dots\dots\dots\\
		&d_r(\lambda) = f_1(\lambda)f_2(\lambda)\dots f_r(\lambda),\\
	\end{align*}
	Так как $\rank A(\lambda) = r$, то $d_{r+1}=\ldots=d_n(\lambda) = 0$, следовательно,  $f_{r+1}(\lambda) = \ldots = f_n(\lambda) = 0$. Так как в противном случае у матрицы $K(\lambda)$ существует ненулевой минор порядка выше $n$.\\
	Таким образом, диагональные элементы матрицы $K(\lambda)$ однозначно определены системой НОД миноров матрицы $A(\lambda)$\\
	$$\begin{cases}
		&f_1(\lambda) = d_1(\lambda),\\
		&f_2(\lambda) = \dfrac{d_2(\lambda)}{d_1(\lambda)},\\
		&f_3(\lambda) = \dfrac{d_3(\lambda)}{d_2(\lambda)},\\
		&\dotfill\\
		&f_r(\lambda) = \dfrac{d_r(\lambda)}{d_{r-1}(\lambda)}.
	\end{cases}$$
\end{Proof}\\\\
$\bullet$ \textit{Диагональные элементы $f_1(\lambda),\dots,f_n(\lambda)$ канонической формы $K(\lambda)$ полиномиальной матрицы $A(\lambda)$ называются \textbf{инвариантными множителями} матрицы $A(\lambda)$.}\\\\
Полученные в ходе доказательства системы формул устанавливают связь между системой НОД миноров и системой
инвариантных множителей полиномиальной матрицы ранга  $r$.
\newtheorem*{cor12_2_1}{Следствие (критерий эквивалентности полиномиальныъ матриц)}\begin{cor12_2_1}Две полиномиальные матрицы эквивалентны $\Longleftrightarrow$ их системы инвариантных множителей совпадают.
\end{cor12_2_1}
\newtheorem*{cor12_2_2}{Следствие (критерий эквивалентности полиномиальныъ матриц)}\begin{cor12_2_2}Две полиномиальные матрицы эквивалентны $\Longleftrightarrow$ их системы НОД миноров совпадают.
\end{cor12_2_2}











\section{Система элементарных делителей полиномиальной матрицы.}
$\bullet$ \textit{Любой многочлен $f(\lambda)$ $\in$ $P[\lambda]$ единственным образом представим в виде}
$$f(\lambda) = a \cdot (g_1(\lambda))^{k_1} \ (g_2(\lambda))^{k_2} \ldots (g_s(\lambda))^{k_s},$$ \textit{где $a\in P$ --- старший коэффициент многочлена $f(\lambda)$, $g_i(\lambda)$ --- различные неприводимые многочлены со старшим коэффициентом 1. Многочлены $(g_i(\lambda))^{k_i}$ называются \textbf{элементарными делителями} многочлена $f(\lambda)$)}.\\\\
$\bullet$ \textit{\textbf{ Системой элементарных делителей} полиномиальной матрицы называется
	совокупность элементарных делителей всех непостоянных инвариантных
	множителей полиномиальной матрицы. При этом каждый элементарный
	делитель включается в эту систему столько раз, сколько он является
	элементарным делителем инвариантных множителей.}
\newtheorem*{th12_3_1}{Теорема (критерий эквивалентности полиномиальных матриц)}\begin{th12_3_1}Две полиномиальные матрицы эквивалентны $\Longleftrightarrow$ совпадают их системы элементарных делителей и ранги.
\end{th12_3_1}\begin{Proof}
	$\Rightarrow$) Если две полиномиальные матрицы одного порядка эквивалентны, то\begin{enumerate}
		\item Их системы НОД миноров совпадают, следовательно, совпадают и их ранги.
		\item Их инвариантные множители совпадают.
	\end{enumerate}
	Значит, их системы элементарных делителей совпадают.\\\\
	$\Leftarrow$) Пусть полиномиальная матрица $A(\lambda)$ имеет порядок $n$, ранг $r$ и систему элементарных делителей равную $S$. Покажем, что можно однозначно построить $K(\lambda)$: Если $r < n$, то $f_{r+1} = \ldots = f_n(\lambda) = 0$.\\\\
	Пусть система $S$ состоит из степеней неприводимых многочленов $p_1(\lambda), \dots, p_s(\lambda)$. Так как многочлен $f_r(\lambda)$ делится на каждый из предыдущих многочленов $f_i(\lambda)$, то он равен произведению всех многочленов $f_i(\lambda)$ с максимальной степенью среди всех имеющихся в системе $S$. Удалив из системы $S$ элементарные делители, порождаемые многочленом $f_r(\lambda)$, получим новую систему многочленов $S_1$, с помощью которой аналогичным образом восстановим многочлены $f_{r-1}(\lambda)$ и так далее.\\\\\
	В результате построения мы исчерпаем всю систему $S$, так как  произведение многочленов системы элементарных делителей равно произведению ненулевых инвариантных множителей, при этом возможны два случая:
	\begin{enumerate}
		\item На последнем шаге находим $f_1(\lambda)$ и построение закончено;
		\item На последнем шаге получаем $f_j(\lambda)$, где $j$ > 1. Тогда $f_1(\lambda)$ = $\dots$ = $f_{j-1}(\lambda)$ = 1.
	\end{enumerate}
\end{Proof}
\newtheorem*{th12_3_2}{Теорема}\begin{th12_3_2}Система элементарных делителей диагональной полиномиальной матрицы равна объединению систем элементарных делителей ее диагональных элементов. При этом каждый элементарный
	делитель включается в эту систему столько раз, сколько раз он является элементарным
	делителем диагональных многочленов.
\end{th12_3_2}\begin{Proof}
	Рассмотрим диагональную полиномиальную матрицу $A(\lambda)$ порядка $n$ такую, что $A(\lambda)$ = $diag\{g_1(\lambda), \dots, g_n(\lambda)\}$. Без ограничения общности считаем, что старшие коэффициенты у ненулевых многочленов $g_i(\lambda)$ равны 1 и $\begin{cases}
		g_i(\lambda)$ $\not=$ 0, $i = \overline{1, r},\\
		g_i(\lambda)$ = 0, $i = \overline{r+1, n}.
	\end{cases}$\\
	Тогда $\rank  A(\lambda)$ = $r$. Пусть $f_1(\lambda), \dots, f_n(\lambda)$ --- система инвариантных множителей матрицы $A(\lambda)$ и $d_1(\lambda), \dots, d_n(\lambda)$  --- система НОД миноров матрицы $A(\lambda)$. Так как $\rank  A(\lambda) = r$, то $d_i(\lambda)$ = $f_i(\lambda)$ = 0 $\forall i = \overline{r+1, n}$. А так как $A(\lambda)$ имеет лишь один ненулевой минор
	$r$-ого порядка, то $d_r(\lambda)$ = $g_1(\lambda)\cdot \dots\cdot g_r(\lambda)$ = $f_1(\lambda)\cdot \dots\cdot f_r(\lambda)$ $\Rightarrow$ элементарные делители $g_i(\lambda)$ и $f_i(\lambda)$ являюстя степенями одних и тех же неприводимых многочленов $p_1(\lambda), \dots, p_s(\lambda)$. Разложим $g_i(\lambda)$ по $p_i(\lambda)$:\begin{center}
		$g_1(\lambda) = (p_1(\lambda))^{\alpha_{11}} \cdot (p_2(\lambda))^{\alpha_{12}} \cdot \ldots \cdot (p_k(\lambda))^{\alpha_{1k}}$,\\
		$\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots$\\
		$g_r(\lambda) = (p_1(\lambda))^{\alpha_{r1}} \cdot (p_2(\lambda))^{\alpha_{r2}} \cdot \ldots \cdot (p_k(\lambda))^{\alpha_{rk}}$.
	\end{center}
	Каждую из систем чисел ($\alpha_{1i}, \dots, \alpha_{ri})$ переобозначим следующим образом:\\
	Через $\beta_{1i}$ обозначим минимульное среди них число, $\beta_{i2}$ --- следующее в порядке возрастания и так далее, а через $\beta_{ir}$ --- максимальное среди всех чисел. Тогда\begin{center}
		$d_1(\lambda) = (p_1(\lambda))^{\beta_{11}} (p_2(\lambda))^{\beta_{12}}\cdot \ldots \cdot (p_k(\lambda))^{\beta_{1k}}$,\\
		$d_2(\lambda) = (p_1(\lambda))^{\beta_{11} + \beta_{21}} (p_2(\lambda))^{\beta_{12} + \beta_{22}} \cdot \ldots \cdot (p_k(\lambda))^{\beta_{1k} + \beta_{2k}}$,\\
		$\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots$\\
	\end{center}
	Тогда \begin{center}
		$f_1(\lambda) = d_1(\lambda) = (p_1(\lambda))^{\beta_{11}}\cdot \ldots\cdot (p_k(\lambda))^{\beta_{1k}}$,\\
		$f_2(\lambda) = \dfrac {d_2(\lambda)}{d_1(\lambda)} = (p_1(\lambda))^{\beta_{21}} \cdot\ldots\cdot (p_k(\lambda))^{\beta_{2k}}$,\\
		$\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots$\\
		$f_r(\lambda) = \dfrac {d_r(\lambda)}{d_{r-1}(\lambda)} = (p_1(\lambda))^{\beta_{r_1}} \cdot\ldots\cdot (p_k(\lambda))^{\beta_{r_k}}$.
	\end{center}
	Значит, система элементарных делителей инвариантных множителей матрицы $A(\lambda)$ состоит из тех же многочленов, что и объединение СЭД ее диагональных элементов.
\end{Proof} 
\newtheorem*{th12_3_3}{Теорема}\begin{th12_3_3}Система элементарных делителей блочнодиагональной полиномиальной матрицы равна объединению систем элементарных делителей ее диагональных блоков.
\end{th12_3_3}
\begin{Proof}
	Пусть $A(\lambda)$ --- блочнодиагональная матрица вида $A(\lambda) = diag (A_1(\lambda), \dots, A_s(\lambda))$, где $A_i(\lambda)$ --- полиномиальная квадратная матрица порядка $n_i$. Каждая матрица $A_i(\lambda)$ имеет единственную каноническую форму $K_i(\lambda) = diag (f_{1i}(\lambda), \dots, f_{n_ii}(\lambda)).$ Тогда система элементарных делителей блока $A_i(\lambda)$ равна объединению систем элементарных делителей многочленов $f_{ij}(\lambda)$. Элементарные преобразования матриц $A_i(\lambda)$ можно рассматривать, как элементарные преобразования матрицы $A(\lambda)$, затрагивающие лишь строки и столбцы, в которых расположен блок $A_i(\lambda)$. Значит, $A(\lambda) \sim diag(f_{11}(\lambda),\dots,f_{n_1n}(\lambda),f_{12}(\lambda),\dots,f_{n_ss}(\lambda))$. Следовательно, система элементарных делителей ее диагональных элементов, с одной стороны, является объединением систем элементарных делителей диагональных блоков $A_i(\lambda)$, а с другой стороны, по предыдущей теореме, системой элементарных делителей всей матрицы $A(\lambda)$.
\end{Proof}










\section{Унимодулярные матрицы.}
$\bullet$ \textit{Полиномиальная матрица называется \textbf{унимодулярной}, если её определитель равен отличному от нуля скаляру.}\\\\
\textit{\textbf{Свойства унимодулярных матриц:}}
\begin{enumerate}
	\item \textit{Полиномиальная матрица унимодулярна $\Longleftrightarrow$ все её инвариантные множители равны 1, то есть она эквивалентна единичной матрице.}
	\begin{Proof}
		Полиномиальная матрица унимодулярна $\Longleftrightarrow d_n(\lambda) = 1 \Longleftrightarrow
		f_1(\lambda)\dots f_n(\lambda) = 1 \Longleftrightarrow f_i(\lambda) = 1\quad \forall i.$
	\end{Proof}
	\item \textit{Полиномиальная матрица унимодулярна $\Longleftrightarrow$ она обратима во множестве полиномиальных матриц.}
	\begin{Proof}
		$\Rightarrow$) Пусть полиномиальная матрица $A(\lambda)$ унимодулярна. Тогда $detA(\lambda)\in P, \ne 0 \Rightarrow \dfrac 1{detA}\in P, \ne 0$. Так как матрица $A(\lambda)$ полиномиальная, ее алгебраические дополнения являются многочленами. Следовательно, присоединенная матрица $\widetilde{A}(\lambda)$ также является полиномиальной $\Rightarrow \dfrac 1{detA}\widetilde{A}(\lambda)$ является полиномиальной матрицей и обратной матрице $A(\lambda)$.\\\\
		$\Leftarrow$) Пусть матрица $A(\lambda)$ имеет обратную полиномиальную матрицу  $A^{-1}(\lambda)$. Тогда $A^{-1}(\lambda) \cdot A(\lambda) = E\Rightarrow detA(\lambda)det A^{-1}(\lambda) = 1$. Так как матрицы $A(\lambda), A^{-1}(\lambda)$ являются полиномиальными, их определители --- многочлены $\Rightarrow detA(\lambda)$ --- ненулевой элемент поля $P$, то есть матрица $A(\lambda)$ унимодулярна.
	\end{Proof}
\end{enumerate}
$\bullet$ \textit{Полиномиальные матрицы вида}
$$S_i(\alpha) = \bordermatrix{
	& & & & i & & & \cr
	& 1 & \dots & 0 & 0 & 0 & \dots & 0 \cr
	& \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \cr
	& 0 & \dots & 1 & 0 & 0 & \dots & 0 \cr
	i & 0 & \dots & 0 & \alpha & 0 & \dots & 0 \cr
	& 0 & \dots & 0 & 0 & 1 & \dots & 0 \cr
	& \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \cr
	& 0 & \dots & 0 & 0 & 0 & \dots & 1 \cr},\quad \alpha \in P\backslash\{0\},$$
$$S_{i,j}(f(\lambda)) = \bordermatrix{
	& & & j & & & & \cr
	& 1 & \dots & 0 & \dots & 0 & \dots & 0 \cr
	& \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \cr
	& 0 & \dots & 1 & \dots & 0 & \dots & 0 \cr
	& \vdots & \ddots & \vdots & \ddots & \vdots & \ddots & \vdots \cr
	i & 0 & \dots & f(\lambda) & \dots & 1 & \dots & 0 \cr
	& \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \cr
	& 0 & \dots & 0 & \dots & 0 & \dots & 1 \cr}, f(\lambda)\in P[\lambda]$$ \textit{называются \textbf{элементарными}.}\\\\
Элементарные преобразования строк (столбцов) полиномиальной матрицы эквивалентны
умножению матрицы слева (справа) на соответствующую элементарную матрицу.
\newtheorem*{th12_4_1}{Теорема (критерий эквивалентности полиномиальных матриц)}\begin{th12_4_1}Две полиномиальные матрицы $A(\lambda)$ и $B(\lambda)$ эквивалентны $\Longleftrightarrow$ существуют унимодулярные матрицы $U(\lambda)$ и $V(\lambda)$ того же порядка такие, что $$B(\lambda) = U(\lambda)A(\lambda)V(\lambda).$$
\end{th12_4_1}\begin{Proof}
	Матрицы $A(\lambda)$ и $B(\lambda)$ эквивалентны $\Longleftrightarrow$ существует конечное число элементарных преобразований строк и столбцов, переводящих матрицу $A(\lambda)$ в $B(\lambda)$. Каждое элементарное преобразование эквивалентно домножению матрицы слева (справа) на соответствующую элементарную матрицу. Но каждая элементарная матрица является унимодулярной, и произведение унимодулярных матриц также является унимодулярной матрицей, то есть $$\underbrace{U_1\dots U_s}_{U(\lambda)}A(\lambda)\underbrace{V_1\dots V_k}_{V(\lambda)}=B(\lambda).$$
\end{Proof}
\newtheorem*{cor12_4_1}{Следствие}\begin{cor12_4_1}Полиномиальная матрица унимодулярна $\Longleftrightarrow$ она представима в виде произведения элементарных матриц.
\end{cor12_4_1}\begin{Proof}
	Матрица $A(\lambda)$ является унимодулярной матрицей $\Longleftrightarrow$ $A(\lambda)\sim E$ $\Longleftrightarrow$ $\exists U(\lambda), V(\lambda): A(\lambda) = U(\lambda)EV(\lambda) = U(\lambda)V(\lambda)$, причем матрицы $U(\lambda)$ и $V(\lambda)$ --- произведения элементарных матриц.
\end{Proof} 












\section{Матричные многочлены.}
$\bullet$ \textit{\textbf{Матричным многочленом}  с переменной $\lambda$ называется выражение вида} $$A(\lambda) = A_m\lambda^m + \ldots + A_1\lambda + A_0 = \sum\limits_{i=0}^mA_i\lambda^i,$$ \textit{где $A_i$ --- квадратные матрицы одного и того же порядка над полем $P$.}\\\\
$\bullet$ \textit{Порядок матриц $A_i$ называется \textbf{порядком} матричного многочлена $A(\lambda)$.}\\\\
$\bullet$ \textit{Если $A_m \ne 0$, то число $m$ называется \textbf{степенью} матричного многочлена.}\\\\
Любая полиномиальная матрица представима в виде матричного многочлена, и, наоборот, любой матричный многочлен представим в виде полиномиальной матрицы.\\\\
$\bullet$ \textit{Матричный многочлен называется \textbf{регулярным}, если определитель его мтаршего коэффициента не равен нулю, то есть $det A_m \ne 0$.}\\\\
$\bullet$ \textit{Два матричных многочлена называются \textbf{равными}, если матрицы, стоящие в этих многочленах при одинаковых степенях переменной $\lambda_i$ равны.}\\\\
Пусть $A(\lambda) = \sum\limits_{i=0}^mA_i\lambda^i$ и $B(\lambda) = \sum\limits_{i=0}^kB_i\lambda^i$, при $m\geqslant k$ --- матричные многочлены.\\\\
$\bullet$ \textit{\textbf{Суммой} матричных многочленов $A(\lambda)$ и $B(\lambda)$ называется многочлен} $$(A+B)(\lambda)=\sum\limits_{i=0}^m(A_i+B_i)\lambda^i.$$ $\bullet$ \textit{\textbf{Произведением} матричных многочленов $A(\lambda)$ и $B(\lambda)$ называется многочлен} $$(AB)(\lambda)=\sum\limits_{i=0}^m\sum\limits_{j=0}^k(A_iB_j)\lambda^{i+j}.$$
Степень суммы матричных многочленов не превосходит максимальной из степеней слагаемых. Степень произведения матричных многочленов не превосходит суммы степеней множителей. Если один из множителей является регулярным многочленом, то степень произведения равна сумме степеней множителей.
\newtheorem*{th12_5_1}{Теорема}\begin{th12_5_1}
	Для произвольного матричного многочлена $A(\lambda)$ и регулярного многочлена $B(\lambda)$\begin{enumerate}
		\item \textit{существует единственная пара многочленов $Q(\lambda)$ и $R(\lambda)$ таких, что $$A(\lambda) = Q(\lambda)B(\lambda) + R(\lambda),$$ причем или $R(\lambda) = 0$, или $\deg R(\lambda)< \deg B(\lambda)$;}
		\item \textit{существует единственная пара многочленов $Q'(\lambda), R'(\lambda)$ таких, что $$A(\lambda) = B(\lambda)Q'(\lambda) + R'(\lambda),$$ причем $R'(\lambda) = 0$ или $\deg R'(\lambda)< \deg B(\lambda)$;}
	\end{enumerate}
\end{th12_5_1}
$\bullet$ \textit{Матричные многочлены $Q(\lambda)$ и $R(\lambda)$ в представлении $A(\lambda) = Q(\lambda)B(\lambda) + R(\lambda)$ называются соответственно \textbf{частным и остатком при делении справа} матричного многочлена $A(\lambda)$ на регулярный многочлен $B(\lambda)$. Аналогично многочлены $Q'(\lambda),R'(\lambda)$ называются \textbf{частным и остатком при делении слева} многочлена $A(\lambda)$ на $B(\lambda)$.}\\\\
Пусть $A(\lambda) = \sum\limits_{i=0}^kA_i\lambda^i$ --- матричный многочлен порядка $n$, $S$ --- квадратная матрица над полем $P$ порядка $n$.\\\\
$\bullet$ \textit{Матрица $A_R(S) = \sum\limits_{i=0}^mA_iS^i$ называется \textbf{правым значением}, а $ A_L(S) = \sum\limits_{i=0}^mS^iA_i$ --- \textbf{левым значением} многочлена $A(\lambda)$ при замене $\lambda$ матрицей $S$.}\\\\
\textit{\textbf{Свойства правых значений:}}\begin{enumerate}
	\item $A(\lambda) = B(\lambda) \Rightarrow A_R(S) = B_R(S), \forall S$;
	\item  $(A+B)_R(S) = A(S)_R + B(S)_R, \forall S$ \textit{(для произведения неверно).}
\end{enumerate}
Вообще говоря, утверждение $(AB)_R(S) \ne A_R(S)B_R(S)$ неверно только в частном случае, так как $\sum\limits_{i = 0}^m\sum\limits_{j = 0}^kA_iB_iS^{i+j} \ne \sum\limits_{i = 0}^m\sum\limits_{j = 0}^kA_iS^iB_jS^j$.
\newtheorem*{th12_5_3}{Обобщенная теорема Безу}\begin{th12_5_3}
	Остаток при делении справа (слева) матричного многочлена $A(\lambda)$ на многочлен $(\lambda E - C)$ равен правому(левому) значению $A(\lambda)$ при $\lambda = C$.
\end{th12_5_3}\begin{Proof}
	Так как многочлен $(\lambda E - C)$ является регулярным, то существуют такие $Q(\lambda), R(\lambda)$, что $A(\lambda) = Q(\lambda)(\lambda E -C) +R(\lambda)$, где или $R(\lambda)=0$, или $\deg R(\lambda) < \deg (\lambda E -C) = 1$, то есть матрица $R(\lambda) \in P_{n,n}$.\\ Пусть $\deg A(\lambda) = m > 0$. Тогда $\deg Q(\lambda) = m-1$, так как $\deg (Q(\lambda)(\lambda E - C)) = \deg A(\lambda)$. Отсюда получаем
	$$Q(\lambda) = Q_{m-1}\lambda^{m-1} + \ldots + Q_1\lambda + Q_0,$$
	$$A(\lambda) = A_m\lambda^m + \ldots + A_1\lambda + A_0 = (Q_{m-1}\lambda^{m-1} + \ldots + Q_1\lambda + Q_0)(\lambda E - C) + R.$$ Приравниваем коэффициенты при соответствующих степенях $\lambda$:\\\\
	$\begin{cases}
		\lambda^m : A_m = Q_{m-1},\\
		\lambda^{m-1} : A_{m-1} = -Q_{m-1}C + Q_{m-2},\\
		\dotfill\\
		\lambda : A_1 = -Q_1 C + Q_0,\\
		\lambda^0 : A_0 = -Q_0 C + R;
	\end{cases}$, выразим значение $R$:\\\\
	$R = A_0 + Q_0 C = A_0 + (A_1 +Q_1 C)C = A_0 + A_1C + Q_1C^2 =\ldots =A_0 + A_1C + A_2C^2 + \ldots + Q_{m-1}C^m =[Q_{m-1} = A_m]=A_0 + A_1C + A_2C^2 + \ldots + A_mC^m = A_R(C)$.
\end{Proof}







\section{Критерий подобия матриц.}
\newtheorem*{th12_6}{Теорема (критерий подобия матриц)}\begin{th12_6}Две квадратные матрицы над полем $P$ одного порядка подобны
	$\Longleftrightarrow$ их характеристические матрицы эквивалентны.
\end{th12_6}\begin{Proof}
	$\Rightarrow)$ Пусть квадратные матрицы матрицы $A$ и $B$ одного порядка $n$ подобны, то есть существует матрица $S$ такая, что $A = S^{-1}BS$. Тогда $A-\lambda E = S^{-1} BS - \lambda S^{-1}ES = S^{-1}(B-\lambda E)S$. Так как $S$ --- невырожденная унимодулярная матрица над полем $P$, то и матрица $S^{-1}$ также является полиномиальной унимодулярной матрицей. По теореме об унимодулярных матрицах, $(A-\lambda E)\sim(B-\lambda E)$.\\\\
	$\Leftarrow)$ Пусть $(B-\lambda E)\sim(A-\lambda E)$. Тогда, по теореме об унимодулярных матрицах, существуют унимодулярные матрицы $U(\lambda), V(\lambda)$ такие, что $(B-\lambda E) = U(\lambda)\cdot (A-\lambda E)\cdot V(\lambda)$. Так как матрица $U(\lambda)$ унимодулярна, то, по свойству существования обратной во множестве полиномиальной матрицы, существует полиномиальная матрица $U^{-1}(\lambda)$ такая, что $U^{-1}(\lambda)\cdot(B-\lambda E) = (A-\lambda E)\cdot V(\lambda)$.\\\\
	Разделим $U^{-1}(\lambda)$ и $V(\lambda)$ на регулярные $(A-\lambda E), (B-\lambda E)$ слева и справа соответственно, то есть представим в виде:
	$$\begin{cases}
		U^{-1}(\lambda) = (A-\lambda E)\cdot U_1(\lambda) + U_2,\\
		V(\lambda) = V_1(\lambda)\cdot (B-\lambda E) +V_2.
	\end{cases}$$Заметим, что $U_2, V_2$ --- квадратные матрицы порядка $n$, так как их степень должна быть нулевой. Тогда подставим и получим
	$$((A-\lambda E)U_1(\lambda) + U_2)(B-\lambda E) = (A-\lambda E)(V_1(B-\lambda E) + V_2),$$
	$$\underbrace{(A-\lambda E)}_{\deg =1}(U_1(\lambda) - V_1(\lambda))\underbrace{(B-\lambda E)}_{\deg =1} = \underbrace{(A-\lambda E)V_2-U_2(B-\lambda E)}_{const};$$
	Если $U_1 - V_1 \ne 0$, то степень слева $\geqslant 2$, а справа $\leqslant 1$, что является противоречием. Следовательно, $U_1 - V_1 = 0$, значит значение справа равно нулю $\Rightarrow$ $(A-\lambda E)V_2 - U_2(B-\lambda E) = 0 \Leftrightarrow (A-\lambda E)V_2 = U_2(B-\lambda E)$. Приравняем коэффициенты при соответствующих степенях:\begin{center}
		при $\lambda^1 : -EV_2 = U_2(-E)\Rightarrow V_2 = U_2,$\\
		при $\lambda^0 : AV_2 = U_2 B$;
	\end{center}
	Покажем, что матрица $U_2$ невырожденная. Для этого разделим $U(\lambda)$ слева на $(B-\lambda E)$, то есть представим в виде
	$U(\lambda) = (B-\lambda E) U_3(\lambda) + U_4(\lambda)$ и домножим слева на $U^{-1}(\lambda)$. Тогда получим
	$U^{-1}(\lambda)U(\lambda) = U^{-1}(B-\lambda E) U_3(\lambda) + U^{-1}(\lambda)U_4(\lambda)\Rightarrow\\ E = (A-\lambda E)V(\lambda)U_3(\lambda) + ((A-\lambda E) U_1(\lambda) + U_2) U_4(\lambda)\Rightarrow E = \underbrace{(A-\lambda E)}_{\deg =1}\underbrace{(VU_3 + U_1 U_4)}_{Q} + U_2 U_4$.\\ Если $Q\ne 0$, то слева $\deg E = 0$, а справа степень $\geqslant 1$, что является противоречием. Значит, $Q= 0\Rightarrow E = U_2U_4\Rightarrow det U_2 \ne 0\Rightarrow\exists U_2^{-1}\Rightarrow$ из того, что $AV_2 = U_2 B$, получаем $B = U_2^{-1}AU_2$, то есть матрица $A$ подобна матрице $B.$
\end{Proof}






\section{Минимальный многочлен матрицы.}
$\bullet$ \textit{Пусть $$f(\lambda) = \alpha_m \lambda^m + \alpha_{m-1} \lambda^{m-1} + \ldots + \alpha_1 \lambda + \alpha_0$$ --- некоторый многочлен над полем $P$, $A$ --- квадратная матрица порядка $n$ над полем $P$. Матрица $$f(A) = \alpha_m A^m + \alpha_{m-1} A^{m-1} + \ldots + \alpha_1 A + \alpha_0 E$$ называется \textbf{значением} многочлена $f$ при $\lambda = A.$}\\\\
\textit{\textbf{Свойства значений многочленов:}}\begin{enumerate}
	\item $(f+g) (A) = f(A) + g(A)$;
	\item $(f\cdot g) (A) = f(A) \cdot g(A)$.\begin{Proof}
		Так как нет других матриц кроме матрицы $A$.
	\end{Proof}
\end{enumerate}
$\bullet$ \textit{Ненулевой многочлен $f(\lambda)$ называется \textbf{аннулирующим} многочленом матрицы $A\in P_{n,n}$, если $f(A) = 0_{n,n}$.}\\\\
Так как $P_{n,n}$ --- векторное пространство размерности $n^2$, то система матрицы $A^{n^2}, A^{n^2 - 1},\dots$, $A^1, A^0$ является линейно зависимой. Следовательно, существует нетривиальная линейная комбинация $\alpha_{n^2}A^{n^2} + \ldots + \alpha_1A^1 + \alpha_0A^0 = 0_{n,n}$. Отсюда получаем, что любой многочлен с теми же коэффициентами является ануллирующим для матрицы $A$ и для любой матрицы $A$ существует аннулирующий многочлен.\\\\
$\bullet$ \textit{\textbf{Минимальным многочленом} называется аннулирующий многочлен, старший коэффициент которого равен 1, а степень минимальная из встепеней всех ненулевых аннулирующих многочленов матрицы.}\\\\
\textit{\textbf{Свойства минимального многочлена:}}\begin{enumerate}
	\item \textit{Минимальный многочлен определён однозначно}.\begin{Proof}
		Пусть матрица $A$ имеет два различных минимальных многочлена: $m_1(\lambda),\ m_2(\lambda)$. Рассмотрим многочлен $m(\lambda) = m_1(\lambda) - m_2(\lambda)$.\begin{enumerate}
			\item $m(\lambda) \ne 0$;
			\item $m(\lambda)$ --- аннулирующий многочлен для матрицы $A$, так как $m(A) = m_1(A) - m_2(A) = 0_{n,n} - 0_{n,n} = 0_{n,n}$;
			\item Так как старшие коэффициенты $m_1(\lambda)$ и $m_2(\lambda)$ равны 1 и $\deg \ m_1(\lambda) = \deg \ m_2(\lambda)$, то степень $m(\lambda)$ меньше степеней $m_1(\lambda)$и $m_2(\lambda)$. Получаем противоречие с тем, что степени $m_1(\lambda)$и $m_2(\lambda)$ наименьшие среди всех степеней аннулирующий многочленов. Следовательно, $m_1(\lambda)=m_2(\lambda)$.
		\end{enumerate} 
	\end{Proof}
	\item \textit{Многочлен является аннулирующим многочленом матрицы $\Longleftrightarrow$ он
		делится на минимальный многочлен этой матрицы.}\begin{Proof}
		$\Rightarrow)$ Пусть многочлен $f(\lambda) $ аннулирующий, а $m(\lambda)$ --- минимальный многочлен. Разделим $f(\lambda)$ на $m(\lambda)$ с остатком:\\
		$f(\lambda) = m(\lambda) \cdot q(\lambda) + r(\lambda)$, где $r(\lambda) = 0$ или $\deg  (r(\lambda)) < \deg  (m(\lambda))$.\\
		Тогда $\underbrace{f(A)}_{0} = \underbrace{m(A)}_{0} \cdot q(A) + r(A) \Rightarrow r(A) = 0$. Если $r(\lambda) \ne 0$, то многочлен $r(\lambda)$ аннулирующий, степень которого меньше степени $m(\lambda)$, что является противоречием. Значит, $r(\lambda) = 0$, то есть $m(\lambda)$ делит $f(\lambda)$.\\\\
		$\Leftarrow)$ Любой многочлен $f(\lambda)$ представимый в виде $f(\lambda)=m(\lambda)q(\lambda)$ является аннулирующим для матрицы $A$, так как $f(A) = m(A)q(A) = 0$, то есть многочлен $f(\lambda)$ является аннулирующим.
	\end{Proof}
\end{enumerate}
\newtheorem*{th12_7_1}{Теорема}\begin{th12_7_1}Минимальный многочлен матрицы равен последнему инвариантному
	множителю ее характеристической матрицы.
\end{th12_7_1}\begin{Proof}
	Пусть $d_1(\lambda), \dots, d_n(\lambda)$ и $f_1(\lambda) , \dots, f_n(\lambda)$ --- системы НОД миноров и инвариантных множителей матрицы $A-\lambda E$ соответственно, $d_i(\lambda) \ne 0\ \forall i = \overline{1,n}$. Обозначим через $B(\lambda)$ матрицу, присоединенную к матрице $A-\lambda E$, то есть $(A-\lambda E)^{-1} = \dfrac{1}{det(A-\lambda E)}\cdot B(\lambda)$. Тогда по свойству присоединенной матрицы $$B(\lambda)(A-\lambda E) = det (A-\lambda E) E_n.\eqno (12.7.1)$$ Матрица $B(\lambda)$ по определению состоит из алгебраических дополнений элементов матрицы $A - \lambda E$, которые с точностью до знака являются минорами $(n-1)$-го порядка этой матрицы. Следовательно, матрица $B(\lambda)$ представима в виде $B(\lambda) = d_{n-1}(\lambda)C(\lambda)$, где $C(\lambda)$ --- матрица НОД элементов, которые равны 1, так как $d_{n-1}(\lambda)$ --- НОД. С другой стороны, $det(A-\lambda E) = (-1)^n d_n(\lambda)$ и при этом $f_n(\lambda) = \dfrac{d_n(\lambda)}{d_{n-1}(\lambda)}$. Следовательно, $d_n(\lambda) = d_{n-1}(\lambda) f_n(\lambda)$ и $det(A-\lambda E) = (-1)^n d_{n-1} (\lambda) f_n(\lambda)$. \\Полученные равенства подставим в $(12.7.1)$: $$d_{n-1} (\lambda) C(\lambda) (A-\lambda E) = (-1)^n d_{n-1}(\lambda) f_n(\lambda) E_{n,n}.$$ Так как ранг матрицы $A-\lambda E$ равен порядку этой матрицы (потому что ее определитель равен характеристическому многочлену), то многочлен $d_{n-1}(\lambda)$ ненулевой. Следовательно, мы можем сократить на данный многочлен и получить $C(\lambda) (A-\lambda E) = (-1) f_n(\lambda) E_{n,n}.$ Тогда $$f_n(\lambda) E_n = (-1)^n C(\lambda) (A-\lambda E). \eqno (12.7.2)$$
	Пусть $f_n(\lambda) = \alpha_k \lambda^k + \ldots + \alpha_1 \lambda + \alpha_0$. Тогда $f_n(\lambda) E = (\alpha_k E)\lambda^k + \ldots + (\alpha_1 E) \lambda + \lambda_0 E$ --- матричный многочлен. И из уравнения (12.7.2) следует, что $f_n(\lambda) E$ делится справа на матричный многочлен $A - \lambda E$ (так как он равен произведению матричных многочленов). Тогда по обобщенной теореме Безу его правое значение при $\lambda = A$ равно нулю, то есть $f_n(\lambda) = \alpha_k \lambda^k + \ldots + \alpha_1 \lambda + \alpha_0 = \alpha_k A^k + \ldots + \alpha_1 A + \alpha_0 E$. Следовательно, многочлен $f_n(\lambda)$ аннулирующий.\\
	Покажем, что многочлен $f_n(\lambda)$ минимальный. Так как он аннулирующий, то он делится на минимальный многочлен $m(\lambda)$, то есть представим в виде $f_n(\lambda) = q(\lambda) m(\lambda)$, причем, так как $f_n(\lambda)$ и $m(\lambda)$ --- многочлены со старшим коэффициентом 1, то многочлен $q(\lambda)$ также имеет старший кожффициент равный 1.\\
	Так как многочлен $m(\lambda)$ аннулирующий, то $m(A) = 0$. Следовательно, $m(A)E = 0$ и матричный многочлен $m(\lambda)E$ имеет правое значение равное нулю. Тогда по обобщенной теореме Безу $A-\lambda E$ делит $m(\lambda)E$, то есть существует $D(\lambda)$ такое, что $m(\lambda)E = D(\lambda)(A-\lambda E)\Rightarrow f_n(\lambda) E = q(\lambda) m(\lambda) E =q(\lambda)D(\lambda)(A-\lambda E)$. Отсюда следует, что многочлен $q(\lambda) D(\lambda)$ является частным при делении $f_n(\lambda)$ справа на $(A-\lambda E)$, и все элементы частного делятся на $q(\lambda)$. Но из (12.7.2) следует, что частное при делении $f_n(\lambda)E$ справа на $(A-\lambda E)$ равно $(-1)^nC(\lambda)$. Тогда многочлен $q(\lambda)$ делит каждый элемент матрицы $C(\lambda)$. Но НОД элементов матрицы $C(\lambda)$ равен 1, поэтому $q(\lambda) = const$, а так как старший коэффициент $q(\lambda)$ равен 1, то и $q(\lambda) = 1$. Следовательно, $f_n(\lambda) = m(\lambda)$ и многочлен $f_n(\lambda)$ является минимальным.
\end{Proof}
\newtheorem*{cor12_7_1}{Следствие (теорема Гамильтона-Кэли)}\begin{cor12_7_1}Характеристический многочлен матрицы является аннулирующим для нее.
\end{cor12_7_1}\begin{Proof}
	Так как при элементарных преобразованиях определитель матрицы может измениться лишь на постоянный множитель, то $det(A-\lambda E) = det(\diag (f_1(\lambda),\dots, f_n(\lambda)))$ и $det(A-\lambda E)$ может отличаться от $f_1(\lambda),\dots, f_n(\lambda)$ лишь на постоянный множитель. Следовательно, все многочлены $f_i(\lambda)$, в том числе и $f_n(\lambda)$ делят многочлен $det(A-\lambda E)$. А так как $f_n(\lambda)$ --- минимальный многочлен, то многочлен $det(A-\lambda E)$ является аннулирующим многочленом матрицы $A$.
\end{Proof}
\newtheorem*{cor12_7_2}{Следствие}\begin{cor12_7_2}Минимальные многочлены подобных матриц равны.
\end{cor12_7_2}\begin{Proof}
	Матрицы $A$ и $B$ подобны $\Longleftrightarrow (A - \lambda E) \sim (B - \lambda E) \Longleftrightarrow$ системы инвариантным множителей матрицы $A$ и $B$ совпадают, а также совпадают и минимальные многочлены.
\end{Proof}





\section{Жорданова нормальная форма матрицы.}
$\bullet$ \textit{Квадратная матрица над полем $P$ порядка $k$ вида}
$$J_k(\lambda_0) = \begin{pmatrix}
	\lambda_0 & 1 & 0 & \dots & 0 \\
	0 & \lambda_0 & 1 & \dots & 0 \\
	\vdots & \vdots & \vdots & \ddots & \vdots\\
	0 & 0 & 0 & \dots & 1 \\
	0 & 0 & 0 & \dots & \lambda_0 \end{pmatrix} 
$$ \textit{--- называется \textbf{жордановой клеткой}. Блочнодиагональная матрица называется \textbf{жордановой матрицей}, если ее диагональные блоки --- жордановы клетки.}
\newtheorem*{lem12_8_1}{Лемма}\begin{lem12_8_1} Система элементарных делителей характеристической матрицы жордановой клетки $J_k(\lambda_0)$ состоит из одного многочлена $(\lambda - \lambda_0) ^ k$.
\end{lem12_8_1}\begin{Proof}Построим характеристическую матрицу жордановой клетки.
	$$J_k(\lambda_0) - \lambda E = \begin{pmatrix} \lambda_0 - \lambda & 1 & 0 & \dots & 0 & 0 \\ 
		0 & \lambda_0 - \lambda & 1 & \dots & 0 & 0 \\ 
		\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\ 0 & 0 & 0 & \dots & \lambda_0 - \lambda & 1 \\ 0 & 0 & 0 & \dots & 0 & \lambda_0 - \lambda \end{pmatrix}.$$
	Система НОД миноров имеет вид: $d_1(\lambda) = 1, d_2(\lambda) = 1,\dots, d_{k-1}(\lambda) = 1, d_k(\lambda) =det(J_k(\lambda_0) - \lambda E)= (\lambda - \lambda_0)^k$ (или $(\lambda_0 - \lambda)^k$ в зависимости от четности).\\\\
	Система инвариантных множителей имеет вид: $f_1 = d_1 = 1, f_2 = \dfrac{d_2}{d_1} = 1,\dots, f_{k-1}=\dfrac{d_{k-1}}{d_{k-2}} = 1, f_k(\lambda) = \dfrac{d_{k}}{d_{k-1}} = (\lambda - \lambda_0)^k$ --- система элементарных делителей	.
\end{Proof}
\newtheorem*{cor12_8_2}{Следствие}\begin{cor12_8_2}Система элементарных делителей характеристической матрицы жордановой матрицы $J = \diag [J_{k_1}(\alpha_1), J_{k_2}(\alpha_2) \dots J_{k_s}(\alpha_s)]$ состоит из многочленов $(\lambda - \lambda_i)^{k_i}$ с учетом их повторения.
\end{cor12_8_2}\begin{Proof}
	Система элементарных делителей блочнодиагональной матрицы есть объединение систем элементарных делителей ее диагональных блоков с учетом повторения. А характеристическая матрица жордановой матрицы является является блочнодиагональной матрицей, у которой по диагонали стоят характеристические матрицы жордановых клеток: $J_k(\lambda) - \lambda E = \diag \{J_{k_1}(\lambda_1)-\lambda E,\dots,J_{k_s}(\lambda_s)-\lambda E \}$.
\end{Proof}
\newtheorem*{th12_8_1}{Теорема}\begin{th12_8_1}Квадратная матрица $A\in P_{n,n}$ имеет подобную жорданову матрицу $\Longleftrightarrow$ ее характеристический многочлен разложим над полем $P$ в произведение многочленов первой степени.
\end{th12_8_1}\begin{Proof}
	$\Rightarrow)$ Если матрица $A$ имеет подобную матрицу $J$, то их характеристические многочлены равны, то есть $det(A- \lambda E) = det(J - \lambda E)$. Но так как матрица $J$ треугольная, то матрица $(J-\lambda E)$ тоже трeугольная. Следовательно, характеристический многочлен матрицы $J$ равен произведению диагональных элементов матрицы $(J-\lambda E)$, которые имеют вид ($\lambda_i - \lambda$). Тогжа и характеристический многочлен матрицы $A$ есть произведение многочленов  ($\lambda_i - \lambda$).\\\\
	$\Leftarrow)$ Пусть характеристический многочлен матрицы $A$ разложим в виде произведения многочленов первой степени. Так как при элементарных преобразованиях определитель может
	измениться лишь на постоянный множитель, то характеристический многочлен матрицы $A$
	равен с точностью до знака произведению инвариантных множителей матрицы $A - \lambda E$. Следовательно, каждый инвариантный множитель матрицы $A - \lambda E$ также разложим в произведение многочленов первой степени. Тогда система элементарных делителей матрицы $A - \lambda E$ состоит из многочленов $(\lambda - \lambda_i)^{k_i}$, причем $k_1 + \dots + k_s = n$. Заметим, что такую же систему элементарных делителей имеет характеристическая матрица жордановой матрицы $J$, и при этом порядок матрицы $J$ равен порядку матрицы $A$. Исходя из этого ранги матриц $A - \lambda E$ и $J -\lambda E$ совпадают.\\
	Таким образом, матрицы $A - \lambda E$ и $J -\lambda E$ эквивалентны, так как их ранги и системы элементарных делителей совпадают. А значит матрицы $A$ и $J$ подобны.
\end{Proof}\\\\
\textbf{Следствие.} \textit{Любая квадратная матрица над полем $\mathbb{C}$ имеет подобную жорданову матрицу.}\\\\
$\bullet$ \textit{Жорданова матрица, подобная матрице $A$, называется \textbf{жордановой нормальной формой} матрицы $A$}.
\newtheorem*{th12_8_2}{Теорема}\begin{th12_8_2}Две жордановы матрицы подобны $\Longleftrightarrow$ они состоят из одних и тех же жордановых клеток.
\end{th12_8_2}\begin{Proof}
	$J_1$ подобна $J_2 \Longleftrightarrow J_1 - \lambda E \sim J_2 - \lambda E \Longleftrightarrow$ равны их ранги и системы элементарных делителей $\Longleftrightarrow$ они состоят из одних и тех же жордановых клеток.
\end{Proof}
\newtheorem*{cor12_8_3}{Следствие}\begin{cor12_8_3}Жорданова нормальная форма матрицы определена с точностью до порядка следования диагональных блоков.
\end{cor12_8_3}













\section{Теорема о количестве клеток жордановой нормальной формы матрицы.}

\newtheorem*{th12_9}{Теорема}\begin{th12_9}Пусть $A$ --- квадратная матрица над полем $P$, $J$ --- ее жорданова нормальная форма. Тогда\begin{enumerate}
		\item диагональными элементами матрицы $J$ являются собственные значения матрицы $A$, то есть все жордановы клетки матрицы $J$ соответствуют собственным значениям матрицы $A$;
		\item сумма порядков жордановых клеток матрицы $J$, соответсвующих собственному значению $\lambda_0$, равна алгебраической кратности собственного значения $\lambda_0$;
		\item количество $l(\lambda_0)$ жордановых клеток матрицы $J$, соответствующих собственному значению $\lambda_0$, равна геометрической кратности собственного значения $\lambda_0$: $$l(\lambda_0) = n - \rank (A - \lambda_0 E);$$
		\item количество $l(\lambda_0)$ жордановых клеток матрицы $J$ порядка $n$, соответствующих собственному значению $\lambda_0$, равно $$l_r(\lambda_0) = \rank (A - \lambda_0 E)^{r - 1} - 2\cdot \rank (A - \lambda_0 E)^{r} + \rank (A - \lambda_0 E)^{r + 1}.$$
	\end{enumerate}
\end{th12_9}\begin{Proof}\begin{enumerate}
		\item Так как матрица $A$ подобна матрице $J$, то их характеристические многочлены равны, то есть $det(A - \lambda E) = det(J - \lambda E)$. Следовательно, они имеют одни и те же корни такие, что корни характеристического многочлена матрицы $A$ --- собственные значения матрицы $A$, а корни характеристического многочлена $J$ совпадают с диагональными элементами $J$, так как матрица $J$ треугольная. А значит все жордановы клетки матрицы $J$ соответствуют собственным значениям матрицы $A$.
		\item Если $\lambda_0$ --- корень характеристического уравнения $A-\lambda E$ кратности $k$, то $\lambda_0$ является корнем и характеристического уравнения матрицы $J$ той же кратности. Следовательно, ровно $k$ диагональных элементов равны $\lambda_0$. Но количество диагональных равных $\lambda_0$ элементов у $J$ совпадает c суммой порядков жордановых клеток, соответствующих значению $\lambda_0$.
		\item Рассмотрим жорданову клетку $$J_k(\alpha) = \begin{pmatrix}
			\alpha & 1 & 0 & 0 & \dots & 0 & 0\\
			0 & \alpha & 1 & 0 & \dots & 0 & 0\\
			0 & 0 & \alpha & 1 & \dots & 0 & 0\\
			\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
			0 & 0 & 0 & 0 &\dots& \alpha & 1\\
			0 & 0 & 0 & 0 &\dots& 0 & \alpha
		\end{pmatrix}$$ и построим матрицу  $$J_k(\alpha) - \lambda_0 E =
		\begin{pmatrix} \alpha - \lambda_0 & 1 & 0 & 0 & \dots & 0 & 0 \\
			0 & \alpha - \lambda_0 & 1 & 0 & \dots & 0 & 0 \\
			0 & 0 & \alpha - \lambda_0 & 1 & \dots & 0 & 0 \\
			\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
			0 & 0 & 0 & 0 & \dots & \alpha - \lambda_0 & 1 \\
			0 & 0 & 0 & 0 & \dots & 0 & \alpha - \lambda_0
		\end{pmatrix}.$$
		Отсюда следует, что $\rank  (J_k(\alpha) - \lambda_0 E) = \begin{cases}
			k,\ \alpha \ne \lambda_0,\\
			k - 1,\ \alpha = \lambda_0.
		\end{cases}$\\
		Так как матрица $J$ блочнодиагональная, то ее ранг равен сумме рангов ее диагональных блоков. Значит ранг матрицы $J - \lambda_0 E$ отличается от порядка этой матрицы на столько единиц, сколько жордановых клеток соответствует значению $\lambda_0$. Тогда количество клеток $l$ матрицы $J$, соответствуюзих значению $\lambda_0$, равно $l = n - \rank (J-\lambda_0 E)$. Так как матрица $A$ подобна матрице $J$, то существует невырожденная матрица $S$ такая, что $J = S^{-1}AS\Rightarrow J - \lambda_0 E = S^{-1}AS - \lambda_0 S^{-1}ES$, то есть матрицы $J-\lambda_0 E$ и $A-\lambda_0 E$ подобны и их ранги равны. Тогда количество клеток $l = n - \rank (A - \lambda_0 E)$ и равно геометрической кратности собственного значения $\lambda_0$.
		\item Рассмотрим матрицу $(J_k(\alpha) - \lambda_0 E)^s$ такую, что если $\lambda_0 \ne \alpha$, то $det(J_k(\alpha) - \lambda_0 E)\ne 0 \Rightarrow det(J_k(\alpha) - \lambda_0 E)^s\Rightarrow$ это базисный минор и $\rank (J_k(\alpha) - \lambda_0 E) = k\quad \forall s \in \mathbb{N}$. А в случае, если $\alpha = \lambda_0$, получаем матрицу $$J_k(\lambda_0) - \lambda_0 E = 
		\begin{pmatrix} 
			0 & 1 & 0 & 0 & \dots & 0 & 0 \\
			0 & 0 & 1 & 0 & \dots & 0 & 0 \\
			0 & 0 & 0 & 1 & \dots & 0 & 0 \\
			\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
			0 & 0 & 0 & 0 & \dots & 0 & 1 \\
			0 & 0 & 0 & 0 & \dots & 0 & 0
		\end{pmatrix},$$ для которой $\rank (J_k(\lambda_0) - \lambda_0 E) = k-1$. Возведем эту матрицу в квадрат и получим матрицу $$(J_k(\lambda_0) - \lambda_0 E)^2 = \begin{pmatrix} 
			0 & 0 & 1 & 0 & \dots & 0 & 0 \\
			0 & 0 & 0 & 1 & \dots & 0 & 0 \\
			0 & 0 & 0 & 0 & \dots & 0 & 0 \\
			\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
			0 & 0 & 0 & 0 & \dots & 0 & 0 \\
			0 & 0 & 0 & 0 & \dots & 0 & 0
		\end{pmatrix},$$ для которой $\rank (J_k(\lambda_0) - \lambda_0 E)^2 = k-2$. Проводя аналогичные рассуждения, придем к тому, что $$\rank (J_k(\lambda_0) - \lambda_0 E)^s = \begin{cases}
			k - s,\ k > s,\\
			0,\ k \leqslant s.
		\end{cases}$$
		Количество клеток $J$, соответствующих собственному значению $\lambda_0$ и имеющих порядок $r$ и больше, равно $\rank (J-\lambda_0 E)^{r-1} - \rank (J-\lambda_0 E)^r$. Аналогично количество клеток $J$, соответствующих собственному значению $\lambda_0$ и имеющих размер $r + 1$ и больше, равно $\rank (J-\lambda_0 E)^{r} - \rank (J-\lambda_0 E)^{r+1}$. Следовательно, количество клеток, имеющих порядок $r$, равно $l_r(\lambda_0) = \rank (J - \lambda_0 E)^{r - 1} - 2\cdot \rank (J - \lambda_0 E)^{r} + \rank (J - \lambda_0 E)^{r + 1}$.\\
		Так как матрицы $A$ и $J$ подобны, то по пункту 3 получаем, что матрицы $A-\lambda_0 E$ и $J - \lambda_0 E$ также подобны, то есть существует невырожденная матрица $S$ такая, что $J-\lambda_0 E = S^{-1}(A - \lambda_0 E) S\Rightarrow (J-\lambda_0 E)^r = S^{-1}\cdot (A-\lambda_0 E)\cdot S \cdot S^{-1}\cdot (A-\lambda_0 E)\cdot S\cdot \ldots \cdot S^{-1}\cdot (A-\lambda_0 E)\cdot S = S^{-1} (A-\lambda_0 E)^r S$. Следовательно, так как матрицы $J - \lambda_0 E$ и $A - \lambda_0 E$ подобны и их ранги равны, то $l_r(\lambda_0) = \rank (A - \lambda_0 E)^{r - 1} - 2\cdot \rank (A - \lambda_0 E)^{r} + \rank (A - \lambda_0 E)^{r + 1}$.
	\end{enumerate}
\end{Proof}






\section{Жорданов базис.}
Пусть $V$ --- векторное пространство над полем $P$, $f$ --- линейный оператор пространства $V$, $M$ --- матрица оператора $f$ в некотором базисе $E$ (не обязательно каноническом).\\\\
$\bullet$ \textit{\textbf{Жордановым базисом} для линейного оператора $f$ называется базис пространства $V$, в котором матрица $M$ этого оператора $f$ является жордановой.}\\\\
$\bullet$ \textit{Система векторов $(a_1, \dots, a_k)$ называется \textbf{жордановой цепочкой} оператора $f$, соответствующей собственному значению $\lambda_0$, если вектор $b_1$ является собственным вектором оператора $f$ и}\begin{align*}
	&f(a_1) = \lambda_0 a_1,\\
	&f(a_2) = \lambda_0 a_2 + a_1,\\
	&f(a_3) = \lambda_0 a_3 + a_2,\\
	&\dots\dots\dots\dots\dots\dots\\
	&f(a_n) = \lambda_0 a_k + a_{k-1}.
\end{align*}
\textit{При этом вектор $a_1$ называется \textbf{началом цепочки}, а векторы $a_2,\dots,a_k$ называются соответственно первым, вторым и так далее векторами, \textbf{присоединенными} к вектору $a_1$. Если начало цепочки является собственным вектором, соответствующим собственному значению $\lambda_0$, то и цепочка называется соответствующей собственному значению $\lambda_0$.}\\\\
\textit{\textbf{Свойства жордановых цепочек:}}
\begin{enumerate}
	\item \textit{Пусть $(a_1, \dots, a_k)$ --- жорданова цепочка оператора $f$, соответствующая собственному значению $\lambda_0, X_1,\dots,X_k$ --- координатные столбцы векторов $a_i$ в базисе $E$ Тогда}
	$$(M - \lambda_0 E) X_i = \begin{cases}X_{i-1}, i>1,\\0, i = 1. \end{cases}$$
	\begin{Proof}
		Пусть $ i = 1$. Тогда $f(a_1) = \lambda_0 (a_1) \Longleftrightarrow M_f X_1 = \lambda_0 X_1 \Longleftrightarrow M_f X_1 - \lambda_0 X_1 = 0 \Longleftrightarrow (M_f -  \lambda_0) X_1 = 0$. \\\\
		Пусть $ i > 1$. Тогда $f(a_i) = \lambda_0 a_i + a_{i-1} \Longleftrightarrow M X_i = \lambda_0 X_i + X_{i-1} \Longleftrightarrow M X_i - \lambda_0 X_i = X_{i-1} \Longleftrightarrow (M - \lambda_0) X_i = X_{i-1}$.
	\end{Proof}
\end{enumerate}
\newtheorem*{cor12_10_1}{Следствие}\begin{cor12_10_1}	$(M - \lambda_0 E)^s X_i = \begin{cases}X_{i-s}, i > s,\\ 0, i \leqslant s. \end{cases}$\begin{Proof}
		$(M - \lambda_0 E)^s X_i = (M - \lambda_0 E)^{s-1}(M - \lambda_0 E) X_i = (M - \lambda_0 E)^{s-1} X_{i-1}.$
	\end{Proof}
\end{cor12_10_1}
\begin{enumerate}
	\item[2.] \textit{Жорданова цепочка является линейнонезависимой системой векторов.}\begin{Proof}
		От противного:
		
		Пусть система $A(a_1, \dots, a_k)$ является жордановой цепочкой и предположим, что она линейно зависима. Тогда существует нетривиальная линейная комбинация равная нулевому вектору: $\alpha_1 a_1 + \ldots + \alpha_k a_k = 0_v$. Пусть $X_i$ --- координатные столбцы векторов $a_i$ в базисе $M$ и $\alpha_s$ --- ненулевой коэффициент с наибольшим индексом, то есть отбросим все коэффициенты с индексом большим, чем $s$. Возьмем линейную комбинацию $ \alpha_1 a_1 + \ldots + \alpha_s x_s = 0$ и домножим ее слева на $(M - \lambda_0 E)^{s-1}\cdot(a_1 + \ldots + \alpha_s x_s) = 0$, то есть $(M - \lambda_0 E)^{s-1}$. Тогда все векторы до последнего станут нулевыми по первому свойству, а последний вектор будет равен единице, то есть $\alpha_s x_1 = 0$, но так как вектор $a_1$ является началом жордановой цепочки, то он ненулевой, то есть $x_1 \ne 0$, значит $\alpha_s = 0$, что является противоречием. Значит жорданова цепочка линейно независимая.
	\end{Proof}
	
	\item[3.] \textit{Cистема векторов, состоящая из жордановых цепочек линейного оператора, является линейно независимой $\Longleftrightarrow$ линейно независимы входящие в нее собственные векторы этого оператора.}\begin{Proof}
		$\Rightarrow)$ Если система векторов, состоящая из жордановых цепочек, линейно независимая, то ее подсистема, состоящая из первых векторов (начал цепочек), также линейно независимая.\\\\
		$\Leftarrow)$ Для упрощения будем рассматривать систему $(A,B)$, где $A(a_1,\dots,a_{k_1})$, $B(b_1,\dots,b_{k_2})$ --- жордановы цепочки, и пусть система $(a_1, b_1)$ линейно независима.\\
		Предположим, что система $(A,B)$ линейно зависима. Тогда существует нетривиальная линейная комбинация $\alpha_1 a_1 + \ldots + \alpha_{k_1} a_{k_1} + \beta_1 a_1 + \ldots + \beta_{k_2} a_{k_2} = 0_v$. Обозначим через $X_i$ и $Y_i$ координатные столбцы векторов $a_i$ и $b_i$ соответственно. Тогда $$\underbrace{\alpha_1 x_1 + \ldots + \alpha_{k_1} x_{k_1}}_{X} + \underbrace{\beta_1 y_1 + \ldots + \beta_{k_2} y_{k_2}}_{Y} = 0_v.\eqno(12.10.1)$$ Рассмотрим два случая: \begin{enumerate}
			\item Пусть системы $A$ и $B$ соответствуют одному и тому же собственному значению $\lambda_0$. Тогда обозначим $s_1 = max\{i\ |\ \alpha_i \ne 0\}, s_2 = max\{i\ |\ \beta_i \ne 0\}, s = max\{s_1, s_2\}$. Домножим уравнение (12.10.1) слева на $(M-\lambda_0 E)^{s-1}$ и получим систему $\begin{cases}
				\alpha_{s_1} x_1 = 0,\ s_1 > s_2,\\
				\beta_{s_2} y_1 = 0,\ s_2 > s_1,\\
				\alpha_s x_1 + \beta_s y_1 = 0,\ s_1 = s_2 = s;
			\end{cases}$, которая является нетривиальной линейной комбинацией координатных столбцов $a_1$ и $b_1$, что противоречит их линейной независимости.
			\item Пусть системы $A$ и $B$ соотвествуют различных собственным значениям $\lambda_1$ и $\lambda_2$. Так как система (12.10.1) нетривиальная, то среди собственных значений $\lambda_i$ существуют ненулевые, так как в противном случае мы имеели бы нетривиальную линейную комбинацию системы $B$ равную нулю, что противоречит второму свойству.\\
			Домножим уравнение (12.10.1) слева на $(M - \lambda_2 E)^{k_2}$ и получим $(M-\lambda_2 E)^{k_2}X + \underbrace{(M-\lambda_2 E)^{k_2}Y}_{=0} = 0\Rightarrow$ $$(M-\lambda_2 E)^{k_2}X = 0.\eqno(12.10.2)$$
			Так как $\lambda_1 \ne \lambda_2$, то $(\lambda - \lambda_1)^{k_1}$ и $(\lambda - \lambda_2)^{k_2}$ --- взаимно простые многочлены. Тогда по критерию взаимно простых многочленов $$\exists \varphi(\lambda), \psi(\lambda) : \varphi(\lambda)(\lambda - \lambda_1)^{k_1}+\psi(\lambda)(\lambda - \lambda_2)^{k_2} = 1.$$ Вычислим значения многочленов в обеих частях равенства при $\lambda = M$: $$\varphi(M)(M-\lambda_1 E)^{k_1} + \psi(M)(M - \lambda_2 E)^{k_2} = E.$$ Домножим это уравнение справа на $X$: $$\underbrace{\varphi(M)(M-\lambda_1 E)^{k_1}X}_{=0} + \underbrace{\psi(M)(M - \lambda_2 E)^{k_2}X}_{=0} = X.$$ Следовательно, $0 = X$, $X$ является нетривиальной линейной комбинацией координатных столбцов векторов жордановой цепочки равной $0_v$, и система $A$ явялется линейно зависимой, что противоречит с условием.
		\end{enumerate}
	\end{Proof}
\end{enumerate}
\newtheorem*{th12_10_1}{Теорема}\begin{th12_10_1}Базис пространства $V$ является жордановым для оператора $f \Longleftrightarrow$ он состоит из жордановых цепочек.
\end{th12_10_1}\begin{Proof}
	$\Leftarrow)$ Пусть базис $B(B_1,\dots, B_s)$ пространства $V$ состоит из жордановых цепочек $B_i(b_{i1},\dots, b_{ik_i})$, соответствующих собственному значению $\lambda_i$. Тогда\\
	\begin{center}
		$f(b_{i1}) = \lambda_i b_{i1}$,\\
		$f(b_{i2}) = \lambda_i b_{i2} + b_{i1}$,\\
		$f(b_{i3}) = \lambda_i b_{i3} + b_{i2}$,\\
		$\dots\dots\dots\dots\dots\dots\dots$
	\end{center} Пометим данную цепочку уравнений через (12.10.3). Построим матрицу оператора $f$ в базисе $B$:
	\par\bigskip
	\begin{center}
		$M_f^B$ =
		$\left(  \begin{tabular}{c|c|c} 
			$\begin{matrix} \lambda_1 & 1 & 0 & \dots & 0 \\ 0 & \lambda_1 & 1 & \dots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \dots & 1 \\ 0 & 0 & 0 & \dots & \lambda_1 \end{matrix}$ & 0 & \dots \\
			\hline
			0 & $J_{k_2}$ & \dots \\
			\hline
			\dots & \dots & $J_{k_s}$ 
		\end{tabular} \right )$ = $$=\diag (J_{k_1}(\lambda_1), J_{k_2}(\lambda_2),\ldots, J_{k_s}(\lambda_s)).\eqno(12.10.4)$$
	\end{center} Следовательно, получили жорданову матрицу (12.10.4).
	$\Rightarrow)$ Если матрица оператора $f$ в базисе $B$ является жордановой вида (12.10.4), то для векторов базиса $B$ справедливо равенство (12.10.3). Следовательно, базис $B$ состоит из жордановых цепочек.
\end{Proof}
\newtheorem*{cor12_10_2}{Следствие}\begin{cor12_10_2}Количество жордановых цепочек в жордановом базисе оператора $f$, соответствующих собственному значению $\lambda_0$,
	совпадает с количеством жордановых клеток, соответствующих $\lambda_0$, а длины цепочек --- с порядками этих клеток.
\end{cor12_10_2}
\newtheorem*{cor12_10_3}{Следствие}\begin{cor12_10_3}Все жордановы базисы оператора состоят из одного и того же числа цепочек одной и той же длины.
\end{cor12_10_3}
\newtheorem*{cor12_10_4}{Следствие}\begin{cor12_10_4}Матрица $S$, трансформирующая матрицу $M$ в ее жорданову матрицу $J$, является матрицей перехода от базиса $E$ к жорданову базису $B$.
\end{cor12_10_4}














\section{Нормальные формы Фробениуса матрицы.}

\textit{Пусть $f(\lambda)$ --- многочлен положительной степени над полем $P$ вида} $$f(\lambda) = \lambda^n + \alpha_{n-1}\lambda^{n-1} + \alpha_{n-2}\lambda^{n-2} + \ldots + \alpha_0.$$ \textit{Матрица}
$$F = \begin{pmatrix}
	0 & 0 & 0 & \dots & 0 & -\alpha_0\\
	1 & 0 & 0 & \dots & 0 & -\alpha_1\\
	0 & 1 & 0 & \dots & 0 & -\alpha_2\\
	\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
	0 & 0 & 0 & \dots & 0 & -\alpha_{n-2}\\
	0 & 0 & 0 & \dots & 1 & -\alpha_{n-1}\\
\end{pmatrix}$$ \textit{называется \textbf{сопровождающей} матрицей для многочлена $f(\lambda)$.}\\\\
$\bullet$ \textit{Блочнодиагональная матрица $F = \diag [F_1,\dots,F_k]$ называется \textbf{матрицей Фробениуса}, если диагональные блоки $F_i$ являются сопровождающими матрицами для многочленов $f_i(\lambda)$, каждый из которых делит последующий.}
\newtheorem*{lem12_11}{Лемма}\begin{lem12_11}Если $F$ --- сопровождающая матрица для многочлена $f(\lambda)$, то характеристическая матрица $(F-\lambda E)$ иммет систему инвариантных множителей $1,\ldots,1, f(\lambda)$.
\end{lem12_11}\begin{Proof}
	Построим матрицу $F-\lambda E$:\\
	$$F-\lambda E = \begin{pmatrix}
		-\lambda & 0 & 0 & \dots & 0 & -\alpha_0\\
		1 & -\lambda & 0 & \dots & 0 & -\alpha_1\\
		0 & 1 & -\lambda & \dots & 0 & -\alpha_2\\
		\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
		0 & 0 & 0 & \dots & -\lambda & -\alpha_{n-2}\\
		0 & 0 & 0 & \dots & 1 & -\alpha_{n-1} - \lambda\\
	\end{pmatrix}.$$
	Система НОД миноров этой матрицы следующая: $d_1(\lambda) = 1, d_2(\lambda) = 1, \dots, d_{k-1}(\lambda) = 1$.\\\\
	Тогда $det (F-\lambda E) = (-\alpha_{k-1} - \lambda)(-1)^{2k}(-\lambda)^{k-1} + (-\alpha_{k-2})(-1)^{2k-1}(-\lambda)^{k} + \ldots + (-\alpha_{1} )(-1)^{k+2}(-\lambda)+(-\alpha_{0})(-1)^{k+1} =(-1)^k(\lambda^k + \alpha_{n-1}\lambda^{n-1} + \alpha_{n-2}\lambda^{n-2} + \ldots + \alpha_1\lambda + \alpha_0)= (-1)^kf(\lambda)$.\\\\
	Система инвариатных множителей имеет следующий вид: $f_1(\lambda) = 1, f_2(\lambda) = 1,\dots, f_{k-1}(\lambda) = 1, f_k(\lambda) = f(\lambda)$.
\end{Proof}
\newtheorem*{cor12_11_1}{Следствие}\begin{cor12_11_1}Если $F = \diag [F_1,\dots,F_s]$ --- матрица Фробениуса, сопровождающая систему многочленов $f_1(\lambda),\dots,f_k(\lambda)$, то матрица $F-\lambda E$ имеет систему инвариантных множителей $1,\dots,1,f_1(\lambda),\dots,f_k(\lambda)$
\end{cor12_11_1}\begin{Proof}
	Матрица $(F-\lambda E)$ является диагональной и имеет вид $\diag [F_1-\lambda E,\dots,F_s-\lambda E]$. С помощью элементарных преобразований приведем ее к виду $\diag (1,\dots,1,f_1(\lambda),1,\dots,1,f_2(\lambda),\dots,\\ 1,\dots,1,f_s(\lambda))$, которую, в свою очередь, можно привести к виду $\diag (1,\dots,1,f_1(\lambda),\dots,f_s(\lambda)).$\\
	Так как матрица $F$ --- матрица Фробениуса, то любой элемент $f_i(\lambda)$ делит элемент $f_{i+1}(\lambda)$. Следовательно, полученная матрица является канонической и ее диагональ состоит из инвариантных множителей.
\end{Proof}
\newtheorem*{th12_11_1}{Теорема}\begin{th12_11_1}Для любой квадратной матрицы над произвольным полем существует и только одна подобная матрица Фробениуса.
\end{th12_11_1}\begin{Proof}\begin{enumerate}
		\item Существование.\\ Пусть $A$ --- квадратная матрица поряка $n$, и пусть матрица $(A-\lambda E)$ имеет систему инвариантных множителей $1,\dots,1,f_1(\lambda),\dots, f_s(\lambda)$. Построим блочнодиагональную матрицу $F = \diag [F_1,\dots,F_s]$, где $F_i$ --- сопровождающие матрицы для многочленов $f_i(\lambda)$. Так как многочлены $f_i(\lambda)$ являются инвариантными множителями матрицы $A-\lambda E$, то многочлены $f_i(\lambda)$ делят многочлены $f_{i+1}(\lambda)$. Следовательно, $F$ --- матрица Фробениуса.\\ Так как произведение инвариантных множителей с точностью до знака совпадает с характеристическим многочленом, то сумма степеней всех $f_i(\lambda)$ равна степени характеристического многочлена матрицы $A$. Тогда матрица $F$ имеет тот же порядок, что и матрица $A$. И по следиствию из леммы матрица $(F-\lambda E)$ имеет ту же систему инвариантных множителей, что и матрица $(A-\lambda E)\Rightarrow (A-\lambda E) \sim (F-\lambda E)\Rightarrow$ матрицы $F$ и $A$ подобны.\\
		\item Единственность.\\ Пусть матрица $A$ имеет две подобные матрицы Фробениуса $F_1$ и $F_2$. Если матрицы $F_1$ и $F_2$ подобны, то $(F_1-\lambda) \sim (F_2-\lambda E)$. Тогда они имеют одну и ту же систему инвариантных множителей и $F_1$ и $F_2$ сопровождают одну и ту же систему многочленов. Следовательно, они состоят из одних и тех же блоков, а значит матрицы $F_1$ и $F_2$ равны.
	\end{enumerate}
\end{Proof}\\ 
$\bullet$ \textit{Матрица Фробениуса, подобная $A$, называется \textbf{нормальной формой Фробениуса} (естественной нормальной формой) матрицы $A$.}
\newtheorem*{lem12_11_2}{Лемма}\begin{lem12_11_2}Пусть $p(\lambda)$ --- неприводимый многочлен, $H$ --- сопровождающая матрица для многочлена $(p(\lambda))^k$. Тогда система элементарных делителей матрицы $H-\lambda E$ состоит из одного многочлена $(p(\lambda))^k$.
\end{lem12_11_2}\begin{Proof}
	Так как матрица $H$ является сопровождающей для многочлена $(p(\lambda))^k$, то система инвариантных множителей имеет вид $1,\ldots,1,(p(\lambda))^k$. Следовательно, система элементарных делителей матрицы $H-\lambda E$ совпадает с системой элементарных делителей многочлена $(p(\lambda))^k$. А так как многочлен $p(\lambda)$ неприводимый, то он и образует систему элементарных делителей самого себя.
\end{Proof}
\newtheorem*{th12_11_2}{Теорема}\begin{th12_11_2}Пусть $A$ --- некоторая матрица над полем $P$, $h_1(\lambda),\dots,h_k(\lambda)$ --- система элементарных делителей матрицы $A-\lambda E$. Тогда матрица $A$ подобная блочнодиагональной матрице $H = \diag [H_1,\dots,H_k]$, где $H_i$ --- сопровождающая матрица для многочлена $h_i(\lambda)$.
\end{th12_11_2}\begin{Proof}
	Матрица $H-\lambda E$ является блочнодиагональной. Следовательно, ее система элементарных делителей является объединением систем элементарных делителей ее диагональных блоков. Но каждый блок $H_i$ --- сопряженная матрица элементарного делителя $h_i(\lambda)$, который является степенью неприводимого многочлена, а значит блоки $H_i - \lambda E$ по лемме имеют систему элементарных делителей $h_i(\lambda)$. Значит, матрицы $H-\lambda E$ и $A-\lambda E$ имеют одинаковую систему элементарных делителей, и ранги этих матриц равны их порядку. Тогда по критерию эквивалентности матриц $(H-\lambda E)\sim (A-\lambda E)$, и матрицы $H$ и $A$ подобны.
\end{Proof}\\\\
$\bullet$ \textit{Блочногодиагональная матрица $H$, подобная матрице $A$, называется \textbf{второй нормальной формой Фробениуса} (квазиестественной нормальной формой) матрицы $A$, если ее диагональные клетки являются сопровождающими матрицами для элементарных делителей матрицы $A-\lambda E$.}\\\\
Вторая нормальная форма Фробениуса определяется с точностью до порядка следования диагональных клеток.





\chapter{Квадратичные формы}
\section{Эквивалентность квадратичных форм.}
Пусть $P$ --- или поле \textbf{действительных} чисел, или поле \textbf{комлексных} чисел.\\\\
$\bullet$ \textit{\textbf{Квадратичной формой} от $n$ переменных $x_1 \dots x_n$ над полем $P$ называется многочлен от этих переменных с коэффициентами из поля $P$, каждый член которого относительно переменных имеет вторую степень, то есть} $$f(x_1, \dots, x_n) = \sum\limits_{i,j=1}^n \alpha_{ij} x_i x_j, \quad \alpha_{ij}\in P$$
Любую действительную, комплексную квадратичную форму можно представить в симметрическом виде, то есть в виде квадратичной формы, коэффициенты которой удовлетворяют условию $\alpha_{ij} = \alpha_{ji}$.\\\\
$\bullet$ \textit{Матрицу коэффициентов $A(\alpha_{ij})$ симметрического вида квадратичной формы называют \textbf{матрицей квадратичной формы}. \textbf{Рангом квадратичной формы} называется ранг матрицы квадратичной формы.}\\\\
Матрица квадратичной формы всегда симметрическая.\\\\
Если переменные $x_i$ записать в столбец $X = \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}$ то квадратичная форма в матричном виде может быть записана следующим образом: $f(X) = X^T A X.$\\\\
$\bullet$ \textit{Пусть $X = \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}$ и $Y = \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix}$ --- две системы переменных. Замена переменных $x_i$ на $y_i$ по формулам вида:}
$$\begin{cases} x_1 = s_{11} y_1 + \ldots + s_{1n} y_n, \\ \dotfill \\ x_n = s_{n1} y_1 + \ldots + s_{nn} y_n; \\ \end{cases}\quad s_{ij}\in P,$$ \textit{называется \textbf{линейным преобразованием переменных} над полем $P$. Матрица $S(s_{ij})$ называется \textbf{матрицей преобразования}. Преобразование называется \textbf{невырожденным}, если $detS\ne 0$.}\\\\
Линейное преобразование переменных в матричном виде можно записать следующим образом: $X = SY$.\\\\
$\bullet$ \textit{Квадратичная форма $f(X) = f(x_1, \dots, x_n)$ называется \textbf{эквивалентной} квадратичной форме $g(Y) = g(y_1, \dots, y_n)$, если существует линейное невырожденное преобразование переменных $X = SY$ переводящее $f(X)$ в $g(Y)$, то есть такое, что $f(SY) = g(Y)$}.
\newtheorem*{lem13_1}{Лемма}\begin{lem13_1}Эквивалентность квадратичных форм рефлексивна, симметрична, транзитивна.
\end{lem13_1}
\begin{Proof}
	\begin{enumerate}
		\item Рефлексивность: $f(X) \sim f(X)$, так как линейное невырожденное преобразование $X=EX$ переводит квадратичную форму в эту же квадратичную форму.
		\item Симметричность: Пусть $f(X) \sim g(X)$, то есть существует невырожденное линейное преобразование $X = SY : f(SY) = g(Y)$. Так как матрица $S$ --- невырожденная, то существует $S^{-1}: Y = S^{-1}X$ --- линейное преобразование и при этом $g(S^{-1}X) = f(S(S^{-1}X)) = f(X)$, то есть $g(X) \sim f(X)$.
		\item Транзитивность: Пусть $f(X) \sim g(Y), \quad g(Y) \sim h(Z)$. Тогда существуют линейные преобразования $X = S_1 Y,\ Y = S_2 Z : f(S_1Y) = g(Y),\ g(S_1Z) = h(Z)\Rightarrow h(Z) = g(S_2Z) = f(S_1(S_2Z)) = f((S_1S_2)Z)$, то есть $X=(S_1S_2)Z$ переводит квадратичную форму $f$ в $h$ и так как матрицы $S_1$ и $S_2$ невырожденные, то матрица $S_1S_2$ также невырожденная, следовательно, $f(X) \sim h(Z).$
	\end{enumerate}
\end{Proof}\\
$\bullet$ \textit{Квадратные матрицы $A$ и $B$ называются \textbf{конгруэнтными}, если существует невырожденная матрица $S$ такая, что $B = S^T A S$}.
\newtheorem*{th13_1}{Теорема}\begin{th13_1} Квадратичные формы
	$f(X)$ и $g(Y)$ эквивалентны $\Longleftrightarrow$ их матрицы конгруэнтны.
\end{th13_1}
\begin{Proof} Пусть $A_f$ и $A_g$ --- матрицы квадратичных форм $f(X)$ и $g(Y)$ соответственно. Тогда $f(X) = X^T A_f X,\ g(Y) = Y^T A_g Y.$ \\\\Квадратичные формы $f(X)$ и $g(Y)$ эквивалентны $\Longleftrightarrow \exists S, detS\ne0: X = SY,\ f(SY) = g(Y)$, то есть $(SY)^T A_f (SY) = Y^T A_g Y \Rightarrow Y^T(S^T A_f S) Y = Y^T A_g Y.$ (На $Y^T$ и $Y$ сокращать нельзя)\\\\
	Покажем, что $S^T A_f S = A_g$. Для этого покажем, что они симметрические. Матрица $A_g$ симметрическая по определению. $(S^T A_f S)^T = [A_f$ --- симметрическая$] = S^T A^T_f(S^T)^T = S^T A_f S$ --- также симметрическая $\Rightarrow A_g$ и $S^T A S$ --- матрицы одной и той же квадратичной формы $\Rightarrow  A_g=S^T A S$.
\end{Proof}
\newtheorem*{cor13_1}{Следствие}\begin{cor13_1}Ранги эквивалентных квадратичных форм равны.
\end{cor13_1}








\section{Канонический вид квадратичной формы.}
$\bullet$ \textit{Квадратичная форма называется \textbf{\textit{канонической}}, если ee матрица диагональная, то есть каноническая квадратичная форма имеет вид}
$\alpha_{11}x_1^2 + \alpha_{22}x_2^2 + \ldots + \alpha_{nn}x_n^2$.
\newtheorem*{th13_2_1}{Теорема}\begin{th13_2_1}Для любой квадратичной формы существует эквивалентная каноническая квадратичная форма.
\end{th13_2_1}\begin{Proof}
	Докажем по индукции по количеству переменных в квадратичной форме.\begin{enumerate}
		\item Пусть $n=1$. Тогда квадратичная форма каноническая.
		\item Пусть теорема верна для меньше $n$ переменных. Покажем для квадратичной формы $f(x_1,\dots,x_n) = \sum\limits_{i,j = 1}^n\alpha_{ij}x_ix_j$, где $\alpha_{ij} = \alpha_{ji}$. Для этого рассмотрим два случая:\begin{enumerate}
			\item среди коэффициентов $\alpha_{ij}$ есть ненулевые. Тогда с помощью перенумерации переменных получим квадратичную форму, у которой $\alpha_{11} \ne 0$. Учитываем, что перенумерация --- невырожденное линейное преобразование, матрица которого мономиальная, значит, невырожденная. Следовательно, получаем экивалентную квадратичную форму $f(x_1,\dots,x_n) = \alpha_{11}x_1^2 + \alpha_{12}x_1 x_2 + \ldots + \alpha_{1n}x_1 x_n + \alpha_{21}x_2x_1 + \ldots + \alpha_{n1}x_nx_1 + \sum\limits_{i,j=2}^n \alpha_{ij} x_i x_j = \alpha_{11}(x_1^2 + 2\dfrac{\alpha_{12}}{\alpha_{11}}x_1x_2 + \ldots + 2\dfrac{\alpha_{1n}}{\alpha_{11}}x_1x_n + (\dfrac{\alpha_{12}}{\alpha_{11}}x_2 + \ldots + \dfrac{\alpha_{1n}}{\alpha_{11}}x_n)^2) - \alpha_{11}(\dfrac{\alpha_{12}}{\alpha_{11}}x_2 + \ldots + \dfrac{\alpha_{1n}}{\alpha_{11}}x_n)^2 + \sum\limits_{i,j=2}^n \alpha_{ij} x_i x_j=\\$
			$\alpha_{11}(\dfrac{\alpha_{12}}{\alpha_{11}}x_2 + \ldots + \dfrac{\alpha_{1n}}{\alpha_{11}}x_n)^2 + g(x_2,\dots,x_n)$, где $g$ --- квадратичная форма, зависящая от $n-1$ переменных.\\ По индуктивному предположению для квадратичной формы $g$ существует эквивалентная квадратичная форма $\widetilde{g}(y_1,\dots,y_n)= \beta_{22}y_2^2 + \ldots +\beta_{nn}y_n^2$. А значит существует невырожденное преобразование $$\begin{cases} y_2 = S_{22}x_2 + \ldots + S_{2n} x_n, \\  \dotfill \\ y_n = S_{n2}x_2 + \ldots + S_{nn} x_n; \end{cases},$$ которое переводит $\widetilde{g}$ в $g$. Тогда преобразование 	$$\begin{cases} y_1 = x_1 + \frac{\alpha_{12}}{\alpha_{11}}x_2 + \ldots + \frac{\alpha_{1n}}{\alpha_{11}}x_n,\\ y_2 = S_{22}x_2 + \ldots + S_{2n} x_n, \\ \dotfill \\ y_n = S_{n2}x_2 + \ldots + S_{nn} x_n; \end{cases}$$ линейное и невырожденное. Построим матрицу преобразования: \\$$det \left( \begin{tabular}{c|c}
				$\begin{tabular}{cc} 1 \end{tabular}$ & $\begin{matrix} \dfrac{\alpha_{12}}{\alpha_{11}} & \dots & \dfrac{\alpha_{1n}}{\alpha_{11}} \end{matrix}$ \\ \hline $\begin{matrix} 0 \end{matrix}$ & $\begin{matrix} S_{22} & \dots & S_{2n} \\ \vdots & \ddots & \vdots \\ S_{n2} & \dots & S_{nn} \end{matrix}$ \end{tabular} \right) = 1\cdot detS_{n-1} \ne 0.$$ Причем, применив это преобразование к квадратичной форме, получим\\ $f(y_1,\dots,y_n) = \alpha_{11}y_1^2 + \beta_{22} y_2^2 + \ldots + \beta_{nn} y_n^2\Rightarrow f\sim g$.
			\item Пусть среди диагональных элементов $a_{ii}$ нет ненулевых. И пусть все остальные элементы также нулевые. Тогда квадратичная форма каноническая. \\Пусть среди всех коэффициентов $\exists \alpha_{ij}\ne 0$, $i\ne j$. Перенумеруем так, чтобы этот коэффициент был равен $\alpha_{12}$. Применим следующее невырожденное преобразование
			$$\begin{cases} x_1 = y_1 + y_2, \\ x_2 = y_1 - y_2, \\ x_i = y_i, \quad i = \overline{3,n}; \end{cases},$$ при этом
			$2\alpha_{12} x_1 x_2 = 2\alpha_{12} (y_1 - y_2) (y_1 + y_2) = 2 \alpha_{12} y_1^2 - 2\alpha_{12} y_2^2$, и для него существует эквивалентная квадратичная форма. Так как мы получили ненулевой угловой коэффициент, то можем перейти к случаю (a).
		\end{enumerate}
	\end{enumerate}
	
	
\end{Proof}\\\\
$\bullet$ \textit{\textbf{Каноническим видом} квадратичной формы называется эквивалентная ей каноническая форма.}








\section{Формула Якоби канонического вида квадратичной формы.}
Рассмотрим квадратичную форму $f(x_1,\dots,x_n) =\sum\limits^n_{i,j=1}\alpha_{ij}x_ix_j,$ где $\alpha_{ij} = \alpha_{ji}.$ Матрица $A = (\alpha_{ij})$ является матрицей квадратичной.\\\\
$\bullet$\textit{\textbf{ Угловыми минорами} матрицы $A(\alpha_{ij})$ называются миноры} $$\Delta_1 = \alpha_{11},\ \Delta_2 = \begin{vmatrix} \alpha_{11} & \alpha_{12}\\ \alpha_{21} & \alpha_{22} \end{vmatrix}, \dots,\Delta_k = \begin{vmatrix} \alpha_{11} & \dots & \alpha_{1k}\\ \vdots&\ddots&\vdots \\ \alpha_{k1} & \dots & \alpha_{kk} \end{vmatrix}, \dots, \Delta_n = det A.$$
\newtheorem*{th13_3_1}{Теорема (Формула Якоби)}\begin{th13_3_1}Если матрица квадратичной формы $f(x_1,\dots,x_n)$ имеет ненулевые угловые миноры, то $$f\sim\dfrac{1}{\Delta_1}y_1^2+\dfrac{\Delta_1}{\Delta_2}y_2^2+ \ldots +\dfrac{\Delta_{n-1}}{\Delta_n}y_n^2$$
\end{th13_3_1}
\begin{Proof}
	Так как $\Delta_1 \ne 0$, то $\alpha_{11}\ne0$, то квадратичная форма $f$ представим в виде\\$f(x_1,\dots,x_n) = \alpha_{11}x_1^2 + 2\alpha_{12}x_1x_2 + \ldots + 2\alpha_{1n} x_1 x_n + \sum\limits^n_{i,j=2}\alpha_{i}x_ix_j = \dfrac{1}{\alpha_{11}}(\alpha_{11}^2x_1^2 + 2x_1\alpha_{11}\cdot(\alpha_{12}x_2 + \ldots + \alpha_{1n}x_n) + (\alpha_{12}x_2 + \ldots + \alpha_{1n}x_n)^2) - \underbrace{\dfrac{1}{\alpha_{11}}(\alpha_{12}x_2 + \ldots + \alpha_{1n}x_n)^2 + \sum\limits^n_{i,j=2}\alpha_{i}x_ix_j}_{g(x_2,\dots,x_n)} =\dfrac{1}{\alpha_{11}}(\alpha_{11}x_1 + \alpha_{12}x_2 + \ldots + \alpha_{1n}x_n)^2 +  g(x_2,\dots,x_n)$, где $g(x_2,\dots,x_n)$ --- квадратичная форма.\\\\
	Обозначим матрицу квадратичной формы $g$ через $B(\beta_{ij}), \forall i,j = \overline{2,n}$. Вычислим элементы $\beta_{ij}: g(x_2,\dots,x_n) = f - \dfrac{1}{\alpha_{11}}(\alpha_{11}x_1+ \ldots +\alpha_{1n}x_n)^2\Rightarrow\beta_{ij} = \alpha_{ij} - \dfrac{\alpha_{1i}\alpha_{1j}}{\alpha_{11}}.$\\\\
	С другой стороны, если к первому столбцу матрицы $A$ применить алгоритм Гаусса, то есть получить из матрицы $A$ матрицу $A' = \begin{pmatrix} \alpha_{11} & \alpha_{12} & \dots & \alpha_{1n} \\ 0 & \alpha'_{22} & \dots & \alpha'_{2n}  \\ \dots & \dots & \dots & \dots \\ 0 & \alpha'_{n2} & \dots & \alpha'_{nn} \end{pmatrix}$, где элементы матрицы $A'$ следующие: $\alpha'_{ij} = \alpha_{ij} + \alpha_{1j}(-\dfrac{\alpha_{i1}}{\alpha_{11}}) = \alpha_{ij} - \dfrac{\alpha_{1j}\alpha_{i1}}{\alpha_{11}} = \alpha_{ij} - \dfrac{\alpha_{1j}\alpha_{1i}}{\alpha_{11}} = \beta_{ij}$, то матрица $A'$ имеет вид $A' = \left( \begin{tabular}{c|c}
		$\begin{tabular}{cc} $\alpha_{11}$ \end{tabular}$ & $\alpha_{12}\dots\alpha_{1n}$\\ \hline $\begin{matrix} 0  \end{matrix}$ & $B$ \end{tabular} \right)$\\\\
	Таким образом, процесс выделения полных квадратов в квадратичной форме $f$ совпадает с первым шагом метода Гаусса. При этом элементы первой строки матрицы $A'$ совпадают с коэффициентами при переменных $x_i$ в выделенном квадрате. Элемент, обратный первому элементу $A'$, совпадает с коэффициентом при выделенном квадрате, а остальные --- совпадают с элементами квадратичной формы $g$. Заметим, что преобразования не меняют угловых миноров: $\Delta_2 = \alpha_{11}\alpha'_{22}\ne 0\Rightarrow \alpha'_{22}\ne 0\Rightarrow$ мы можем применять следующие шаги метода Гаусса. Спустя $n-1$ шаг мы получим матрицу $C (\gamma_{ij})$, которая является верхнетреугольной $\Rightarrow$ она невырожденная $\Rightarrow f(x_1,\ldots,x_n) = \dfrac{1}{\gamma_{11}}\underbrace{(\gamma_{11}x_1 + \ldots+\gamma_{1n}x_n)^2}_{y_1} + \ldots + \dfrac{1}{\gamma_{nn}}(\gamma_{nn}x_n)^2\Rightarrow$ квадратичная форма $f$ может быть получена из $\widetilde{f}(y_1,\ldots,y_n) = \dfrac{1}{\gamma_{11}}y_1^2+...+\dfrac{1}{\gamma_{nn}}y_n^2$ при помощи  преобразований: $\begin{cases} y_1 = \gamma_{11}x_1 + \ldots + \gamma_{1n}x_n,\\
		y_2 = \gamma_{22}x_2 + \ldots + \gamma_{2n}x_n,\\
		\dotfill\\
		y_n = \qquad\qquad\quad\;\gamma_{nn}x_n;
	\end{cases}$\\
	При этом матрица этого преобразования совпадает с матрицей $C$, и, следовательно, невырожденная $\Rightarrow$ преобразование также невырожденное $\Rightarrow f\sim \widetilde{f}$. Выражаем коээфициенты $\widetilde{f}$ через элементы матрицы $A$. Матрица $C$ получена из матрицы $A$ путем преобразований, не изменяя угловые миноры $\Rightarrow$\\	
	$$\Delta_1 = \gamma_{11},\quad \Delta_2 = \gamma_{11}\gamma_{22},\quad \dots,\quad \Delta_n = \gamma_{11}\gamma_{22}\dots\gamma_{nn};$$
	$$\dfrac{1}{\gamma_{11}}=\dfrac{1}{\Delta_1}, \quad\dfrac{1}{\gamma_{22}}=\dfrac{\gamma_{11}}{\Delta_2} = \dfrac{\Delta_1}{\Delta_2},\quad  \dots,\quad \dfrac{1}{\gamma_{nn}} = \dfrac{\Delta_{n-1}}{\Delta_n}.$$
\end{Proof}  













\section{Нормальный вид квадратичной формы.}
$\bullet$ \textit{Комплексная каноническая квадратичная форма $f(x_1,\dots,x_n)$ называется \textbf{нормальной}, если она имеет вид}
$$x_1^2+ \ldots +x_r^2+0\cdot x_{r+1}^2+ \ldots +0\cdot x_n^2,\quad 0\leqslant r\leqslant n.$$
\newtheorem*{th13_4_1}{Теорема}\begin{th13_4_1}Для любой комплексной квадратичной формы существует единственная эквивалентная нормальная квадратичная форма.
\end{th13_4_1}\begin{Proof}\begin{enumerate}
		\item Существование. Для любой квадратичной формы существует эквивалентная ей каноническая форма $\alpha_1 x_1^2+ \ldots +\alpha_n x_n^2$. Перенумеруем переменные так, чтобы $\alpha_i$ было ненулевым для $i=\overline{1,r}$ и $\alpha_i=0$ для $i=\overline{r+1,n}$. А затем применим к квадратичной форме линейное преобразование: $\begin{cases}
			y_i=\sqrt{\alpha_i} x_i,\ i=\overline{1,r},\\
			y_i=x_i,\ i=\overline{r+1,n};\\
		\end{cases}$, причем матрица этого преобразования невырожденная. В результате получили эквивалетную нормальную квадратичную форму.
		\item Единственность. Пусть каноническая квадратичная форма имеет две нормальные формы:$\begin{cases}
			f(x_1,\dots,x_n)=x_1^2+ \ldots +x_r^2,\\
			g(x_1,\dots,x_n)=x_1^2+ \ldots +x_q^2;
		\end{cases}$. Тогда $f \sim g \Rightarrow \rank f=\rank g\Rightarrow r=q \Rightarrow$ квадратичные формы $f(x_1,\dots,x_n)$ и $g(x_1,\dots,x_n)$ совпадают.
	\end{enumerate}
\end{Proof}\\\\
\textbf{Следствие.} \textit{Комплексные квадратичные формы эквивалентны $\Longleftrightarrow$ их ранги равны.}\\\\
$\bullet$ \textit{Действительная каноническая квадратичная форма $f(x_1,\dots,x_n)$ называется \textbf{нормальной}, если она имеет вид}
$$x_1^2+ \ldots +x_p^2-x_{p+1}^2-\ldots-x_r^2+0 \cdot x_{r+1}^2+ \ldots +0 \cdot x_n^2,\quad 0\leqslant p\leqslant r\leqslant n.$$
\newtheorem*{th13_4_2}{Теорема (закон инерции действительных квадратичных форм)}\begin{th13_4_2}Для любой действительной квадратичной формы существует единственная эквивалентная нормальная квадратичная форма.
\end{th13_4_2}\begin{Proof}\begin{enumerate}
		\item Существование.
		Любую дествительную квадратичную форму невырожденным линейным преобразованием переменных можно привести к эквивалентной канонической квадратичной форме вида $\alpha_1 x_1^2+ \ldots +\alpha_n x_n^2$. Перенумеруем в канонической квадратичной форме переменные следующим образзом: $\begin{cases}
			\alpha_i > 0 \quad i=\overline{1,p},\\
			\alpha_i < 0 \quad i=\overline{p+1,r},\\
			\alpha_i=0 \quad i=\overline{r+1,n};
		\end{cases}$. Выполнив еще одно невырожденное линейное преобразование, получим $\begin{cases}
			y_i=\sqrt{|\alpha_i|} x_i \quad i=\overline{1,r},\\
			y_i=x_i \quad i=\overline{r+1, n};
		\end{cases}$, что и является нормальной квадратичной формой.
		\item Единственность. Пусть квадратичная форма имеет две различные эквивалентные нормальные формы $f(x)$ и $g(y)$. Тогда $f(x) \sim g(y)\Rightarrow \rank  f = \rank  g\Rightarrow$ количество ненулевых коэффициентов одинаково, обозначим его с помощью $r$.
		$$\begin{cases}
			f(x)=x_1^2+ \ldots +x_p^2-x_{p+1}^2-\ldots-x_r^2,\\
			g(y)=y_1^2+ \ldots +y_q^2-y_{q+1}^2-\ldots-y_r^2.
		\end{cases}$$\\ Предположим, что $p\ne q$, и пусть $ p<q$. Так как $f\sim g$, то существует линейное преобразование $X=SY$, где матрица $S=(S_{ij})\in \mathbb{R}_{n,n}$ и является невырожденной. Тогда $f(SY) = g(Y)$. Построим линейную однородную систему уравнений 
		$$\begin{cases}
			S_{11} y_1+ \ldots +S_{1q} y_q=0,\\
			\dotfill\\
			S_{p1} y_1+ \ldots +S_{pq} y_q=0;
		\end{cases}$$ Так как $p<q$ то ранг матрицы меньше числа неизвестных. Следовательно, существуют ненулевые решения $(\tilde y_1 ,\dots, \tilde y_q)$. Построим столбец этих решений $\tilde Y=
		\begin{pmatrix}
			\tilde y_1 \\
			\dots\\
			\tilde y_q\\
			\dots\\
			0\\
		\end{pmatrix}$ и вычислим $\tilde X=S \tilde Y=
		\begin{pmatrix}
			S_{11}\tilde y_1+ \ldots +S_{1q}\tilde y_q\\
			\dotfill\\
			S_{n1}\tilde y_1+ \ldots +S_{nq}\tilde y_q
		\end{pmatrix}$. Тогда $\tilde x_1=\ldots=\tilde x_p=0$;\\\\
		Так как $f(SY)=g(Y)$, то $f(S \tilde Y)=g(\tilde Y)\Rightarrow f(\tilde X)=g(\tilde Y)\Rightarrow f(\tilde X)=\underbrace{-x_{p+1}^2-\ldots-x_r^2}_{\leqslant0} = g(\tilde Y)=\underbrace{y_1^2+ \ldots +y_q^2}_{>0}$, что является противоречием, так как $y_i$ --- ненулевое решение системы уравнений, следовательно, $p = q$.
	\end{enumerate}
\end{Proof}\\\\
$\bullet$ \textit{\textbf{Нормальным видом квадратичной формы} называется эквивалентная ей нормальная квадратичная форма}.\\\\
$\bullet$ \textit{Число положительных и отрицательных коэффициентов нормального вида действительной квадратичной формы называются соответственно \textbf{положительным} и \textbf{отрицательным индексом инерции квадратичной формы}. Разность положительного и отрицательного индексов инерции называется \textbf{сигнатурой} квадратичной формы.}












\section{Знакоопределенные действительные квадратичные формы.}
$\bullet$ \textit{Действительная квадратичная форма $f(x_1,\dots,x_n)$ называется \textbf{положительноопределенной (отрицательноопределнной)}, если для любых чисел $\gamma_1,\dots,\gamma_n, \gamma_i\, \in\, \mathbb{R}$, не обращающихся одновременно в 0, $f(\gamma_1,\dots,\gamma_n)>0$  $(f(\gamma_1,\dots,\gamma_n)<0)$.}
\newtheorem*{th13_5_1}{Теорема}\begin{th13_5_1}Квадратичная форма $f(x_1,\dots,x_n)$ является положительноопределенной $\Longleftrightarrow$ она эквивалентна нормальной квадратичной форме $$g(y_1,\dots,y_n) = y_1^2 + \ldots + y_n^2.$$
\end{th13_5_1}\begin{Proof}
	$\Rightarrow)$ Пусть $f(x)$ --- положительноопределенная квадратичная форма, и она эквивалентна нормальной квадратичной форме $g(y_1,\dots,y_n)=\alpha_1 y_1^2+ \ldots +\alpha_n y_n^2$. Предположим, что $\alpha_n \ne 1$, то есть $\alpha_n = -1$ или $\alpha_n = 0$. Тогда для  $e_n=\begin{pmatrix}
		0\\
		\vdots\\
		0\\
		1\\
	\end{pmatrix}$ выполняется $g(e_n)=\alpha_n \leqslant 0$. Так как $f \sim g$, то существует линейное невырожденное преобразование $X=SY$ такое, что $f(SY)=g(Y) \Rightarrow f(S e_n) = g(e_n) \leqslant 0$, но так как $Se_n = \begin{pmatrix}
		S_{1n}\\
		\vdots\\
		S_{nn}\\
	\end{pmatrix}$ (последний столбец матрицы $S$) и матрица $S$ невырожденная, то столбец $Se_n$ ненулевой, что является противоречием с тем, что квадратичная форма $f$ положительноопределенная.\\\\
	$\Leftarrow)$ Пусть $f \sim g(y_1,\dots,y_n)=y_1^2+ \ldots +y_n^2$. Тогда существует линейное невырожденное преобразование $Y=SX$ такое, что $g(SX)=f(X)$.
	Квадратичная форма $g(y)$ положительноопределенная, следовательно, для любого ненулевого столбца $\text{Г}\in\mathbb{R}$ $g(S\text{Г})\geqslant0,$ причем $g(S\text{Г})=0 \Leftrightarrow S\text{Г}=0\Rightarrow \text{Г}=S^{-1}\cdot 0 = 0$. Таким образом, $\forall\text{Г}\in\mathbb{R}^n\backslash\{0\}\ f(\text{Г}) = g(S\text{Г}) >0\Rightarrow$ квадратичная форма $f$ является положительноопределенной.
\end{Proof}
\newtheorem*{cor13_5_1}{Следствие}\begin{cor13_5_1}Все положительноопределенные квадратичные формы являются эквивалентными.
\end{cor13_5_1}
\newtheorem*{cor13_5_2}{Следствие}\begin{cor13_5_2}Матрица положительноопределенной квадратичной формы имеет положительный опеределитель.
\end{cor13_5_2}\begin{Proof}
	Так как положительноопределенная квадратичная форма $f(x)$ эквивалентна нормальной квадратичной форме, то матрица $A_f$ этой квадратичной формы конгруэентна матрице $E$, то есть существует невырожденная матрица $S$ такая, что $A_f=S^{T} E S = S^{T} S\Rightarrow det A_f = detS^{T} S = detS^{T} \cdot detS=  (detS)^2>0$.
\end{Proof}
\newtheorem*{th13_5_2}{Критерий Сильвестра}\begin{th13_5_2}
	Действительная квадратитчная форма является положительноопределенной $\Longleftrightarrow$ все угловые миноры ее матрицы положительны.
\end{th13_5_2}\begin{Proof}
	$\Rightarrow )$ Пусть квадратичная форма $f(x_1,\dots,x_n) = 
	\sum_{i,j=1}^n \alpha_{ij} x_i x_j$ является положительноопределенной. Тогда квадратичная форма $f_m(x_1,\dots,x_m) = \sum_{i,j=1}^m \alpha_{ij} x_i x_j,\ 1\leqslant m\leqslant n$ также является положительноопределенной. Так как если бы существовала последовательность $(\gamma_1,\dots,\gamma_m)\ne 0$, для которой $f_m(\gamma_1,\dots,\gamma_m)\leqslant0$, то последовательность $(\gamma_1,\dots,\gamma_m,0,\dots,0)$ также была бы ненулевая и $f(\gamma_1,\dots,\gamma_m,0,\dots,0)= f_m(\gamma_1,\dots,\gamma_m)\leqslant0\Rightarrow$ матрицы квадратичных форм имеют положительный определеитель, а определитель $f_m$ равен угловому минору $\Delta_m$.\\\\
	$\Leftarrow)$ Если все угловые миноры $\Delta_i$ положительны, то по формуле Якоби эта квадратичная форма эквивалентна канонической квадратичной форме с положительными коэффициентами, которая явлется положительноопределенной. Следовательно, квадратичная форма $f(x)$ также положительноопределенная.
\end{Proof}\\\\
\textbf{Следствие.} \textit{Действительная квадратичная форма является отрицательноопределенной $\Longleftrightarrow$ все угловые миноры ее матрицы четного порядка положительны, нечетного порядка --- отрицательны.}\\\\
$\bullet$ \textit{Минор матрицы называется \textbf{главным}, если номера строк и номера столбцов, в которых он расположен, совпадают.}
\newtheorem*{cor13_5_4}{Следствие}\begin{cor13_5_4}Все главные миноры матрицы положительноопределенной квадратичной формы положительны.
\end{cor13_5_4}













\chapter{Евклидовы и унитарные пространства}
\section{Определение и простейшие свойства евклидовых пространств.}
$\bullet$ \textit{Пусть $V$ --- векторное пространство над полем $\mathbb{R}$. Отображение $V \times V \rightarrow \mathbb{R}$, ставящее в соответствие упорядоченной паре векторов
	$(a,b) \in V$ число $ab \in \mathbb{R}$, называется \textbf{скалярным произведением}, если} $\forall\alpha \in \mathbb{R}, a, b, c \in V$\begin{enumerate}
	\item $ab=ba$,
	\item $(\alpha a)b=\alpha(ab)$,
	\item $(a+b)c=ac+bc$,
	\item $\forall a \ne 0_v, a\cdot a>0$
\end{enumerate}
$\bullet$ \textit{Действительное векторное пространство с определенным на нем скалярным произведением называется \textbf{евклидовым пространством}}.\\\\
\textit{\textbf{Простейшие свойства скалярного произведения:}}\begin{enumerate}
	\item $a(\alpha b)=\alpha(ab)\quad \forall a,b \in V, \alpha \in \mathbb{R}$
	\item $a(b+c)=ab+ac\quad\forall a,b,c\in V$
	\item $a\cdot 0_v=0_v \cdot a =0_{\mathbb{R}}\quad \forall a \in V$
	\begin{Proof}
		$a\cdot 0_v+ a\cdot 0_v= a(0_v+0_v)=a \cdot 0_v \Rightarrow  a\cdot 0_v=0$
	\end{Proof}
\end{enumerate}
$\bullet$ \textit{Пусть $\bsys$ --- некоторая система векторов евклидова пространства. Матрица $G_B=
	\begin{pmatrix}
		b_1 b_1 & b_1 b_2 & \dots & b_1 b_n\\
		b_2 b_1 & b_2 b_2&\dots& b_2 b_n\\
		\vdots&\vdots&\ddots&\vdots&\\
		b_n b_1 &b_n b_2 &\dots&b_n b_n
	\end{pmatrix}\in \mathbb{R}_{n,n}$, составленная из скалярных произведений векторов системы $B$, называется \textbf{матрицей Грама} системы $B$.}\\\\
Из определения скалярного произведения следует, что матрица Грама является симметрической.\\\\
$\bullet$ \textit{Если $B$ --- базис пространства $V$, то матрица} $G_B$ \textit{называвется \textbf{матрицей скалярного произведения} в базисе $B$.}
\newtheorem*{th14_1_1}{Теорема}\begin{th14_1_1}Если $G_B$ --- матрица скалярного произведения пространства $V$ в базисе $B$, $ X=\begin{pmatrix}
		x_1\\
		\vdots\\
		x_n
	\end{pmatrix}$, 
	$ Y=\begin{pmatrix}
		y_1\\
		\vdots\\
		y_n
	\end{pmatrix}$ --- координатные столбцы векторов $x, y\in V$ в базисе $B$, то $xy=X^T G_B Y $.
\end{th14_1_1}\begin{Proof}
	$xy=(\sum\limits_{i=1}^n x_i b_i)(\sum\limits_{j=1}^n y_j b_j)= \sum\limits_{i,j=1}^n(x_i b_i)(y_j b_j)=\sum\limits_{i,j=1}^nx_i y_j(b_i b_j) =\sum\limits_{i=1}^n x_i (\sum\limits_{j=1}^nb_i b_j)y_j = X^T G_B Y$
\end{Proof}
\newtheorem*{cor14_1_1}{Следствие}\begin{cor14_1_1}Матрица скалярного произведения является матрицей положительно
	определённой квадратичной формы.
\end{cor14_1_1}\begin{Proof} Так как $\forall a \ne 0\quad a\cdot a > 0$, то и координатный столбец вектора $a$ также ненулевой. Следовательно, $a\cdot a= X_a^T G_B X_a >0\Rightarrow$ квадратичная форма $f(x_1,\dots, x_n)=X^T G_B X >0$, где $X = \begin{pmatrix}
		x_1\\ \vdots\\ x_n
	\end{pmatrix}$, положительноопределенная. А так как матрица $G_B$ симметрическая, то она является матрицей квадратичной формы.
\end{Proof}
\newtheorem*{cor14_1_2}{Следствие}\begin{cor14_1_2}Определитель матрицы скалярного произведения положителен.
\end{cor14_1_2}\begin{Proof}
	Так как матрица скалярного произведения является матрицей положительно определенной квадратичной формы, то по критерию Сильвестра все угловые миноры положительны, в том числе и определитель матрицы.
\end{Proof}
\newtheorem*{th14_1_2}{Теорема (Критерий Грама линейной зависимости системы векторов)}\begin{th14_1_2}Система
	векторов $B(b_1,\dots,b_k)$ линейно зависима $\Longleftrightarrow$ определитель её
	матрицы Грама $G_B$ равен нулю.
\end{th14_1_2}\begin{Proof}
	$\Rightarrow)$ Пусть система векторов $B$ линейно зависима $\Rightarrow$ существует тривиальная линейная комбинация $\alpha_1 b_1+ \ldots +\alpha_k b_k =0_v$. Домножим ее справа на $b_1,\dots,b_k$. Тогда\\
	$$\begin{cases}	\alpha_1b_1b_1+ \ldots +\alpha_kb_1 b_k=0,\\
		\dotfill\\
		\alpha_1b_k b_1+ \ldots +\alpha_kb_k b_k=0;\\
	\end{cases}\Rightarrow\begin{pmatrix}
		b_1 b_1 & \dots & b_1 b_k\\
		\vdots&\ddots&\vdots&\\
		b_k b_1 &\dots&b_k b_k
	\end{pmatrix}\begin{pmatrix}
		\alpha_1\\
		\vdots\\
		\alpha_k
	\end{pmatrix} = \begin{pmatrix}
		0\\
		\vdots\\
		0
	\end{pmatrix}\Rightarrow G_B X = 0.$$\\ Линейная однородная система уравнений имеет ненулевое решение $\Rightarrow$ определитель матрицы Грама нулевой.\\\\
	$\Leftarrow)$ Пусть $G_B = 0$. Пойдем от противного. Если система векторов $\bsys$ линейно независима, то она является базисом линейной оболочки системы $B$, а линейная оболочка $L(B)$ --- подпространство $V$, то есть сама является подпространством. Следовательно, $G_B$ --- матрица скалярного произведения в пространстве $L(B)$ $\Rightarrow detG_B > 0$ --- противоречие.
\end{Proof}

\newtheorem*{th14_1_3}{Теорема}\begin{th14_1_3}
	Если $G_A, G_B$ --- матрицы скалярного произведения в базисах $A, B$ соответственно, то $G_B = S^TG_AS$, где $S = S_{A\rightarrow B}$.
\end{th14_1_3}\begin{Proof}
	Пусть $x,y \in V$ --- произвольные векторы. $X_A,X_B,Y_A,Y_B$ --- координатные столбцы соответственно в базисах $A$ и $B$ базисах, $X_A = SX_B,\ Y_A = SY_B$. Тогда \\$xy = (X_A)^TG_AY_A = (SX_B)^TG_A(SY_B) = X^T_B(S^TG_AS)Y_B\Rightarrow$ элемент, стоящий в $i$-ой строке и $j$-ом столбце, $b_i b_j = S^TG_AS$. По определению матрицы Грама, элемент, стоящий в $i$-ой строке и $j$-ом столбце, также равен $b_ib_j$. Следовательно, $S^TG_AS = G_B$.
\end{Proof}







\section{Ортогональные векторы.}
Пусть $V$ --- евклидово пространство.\\\\
$\bullet$ \textit{Векторы $a$ и $b$ называются \textbf{ортогональными}, если их скалярное произведение равно нулю. Система векторов $\bsys$ называется \textbf{ортогональной}, если все её векторы попарно ортогональны, то есть $b_ib_j = 0$ для всех $i\ne j$.}\\\\
Матрица Грама ортогональной системы векторов диагональная, причем на диагонали расположены положительные действительные числа.
\newtheorem*{lem14_2_1}{Лемма}\begin{lem14_2_1}Ортогональная система ненулевых векторов линейно независима.
\end{lem14_2_1}\begin{Proof}
	Поскольку матрица ортогональной системы векторов Грама диагональная, то её определитель больше нуля. Следовательно, по критерию Грама, система векторов является линейно независимой.
\end{Proof}
\newtheorem*{th14_2_1}{Теорема}\begin{th14_2_1}В любом евклидовом пространстве существует ортогональный базис.\end{th14_2_1}\begin{Proof}
	Пусть система векторов $\bsys$ является базисом пространства $V$. Построим $\asys$ следующим образом:\begin{enumerate}
		\item $a_1=b_1\ne0$;
		\item $a_2=b_2+\alpha a_1$; $a_2\cdot a_1 = 0$\\
		Тогда $\underbrace{a_2 a_1}_{0} = a_1b_2 + \alpha \underbrace{(a_1a_1)}_{0}\Rightarrow \alpha = - \dfrac{(b_2, a_1)}{(a_1, a_1)}$.\\ Причем, так как $a_1 = b_1$ и коэффициент при $b_2 = 1$, то вектор $a_2$ является нетривиальной линейной комбинацией двух векторов системы $B$, следовательно, $a_2\ne 0_v$.
		\item $a_3=b_3+ \alpha_1 a_1 + \alpha_2 a_2$; $a_1\cdot a_3=a_2\cdot a_3 = 0$\\ Тогда $\alpha_1=- \dfrac{(b_3, a_1)}{(a_1, a_1)},\ \alpha_2= - \dfrac{(b_3,a_2)}{(a_2, a_2)}$.\\
		Причем, так как $a_1$ и $a_2$ линейно выражаются через $b_1$ и $b_2$ и коэффициент при $b_3 = 1$, то вектор $a_3$ является нетривиальной линейной комбинацией векторов $b_1,b_2$ и $b_3$, следовательно, $a_3\ne 0_v$.
	\end{enumerate}
	
	Продолжая рассуждения подобным образом, на $k$-ом шаге получим: $a_k=b_k + \sum\limits_{i=1}^{k-1} \alpha_i a_i, \, \alpha_i = - \dfrac{(b_k,a_i)}{(a_i,a_1)}\Rightarrow$ получим ортогональную систему векторов $A$, состоящую из ненулевых векторов $\Rightarrow$ она линейно независима по доказзанной выше лемме $\Rightarrow A$ --- ортогональный базис и в нём столько же векторов, сколько и в базисе $B$.
\end{Proof}\\\\
$\bullet$ \textit{Алгоритм построения ортогонального базиса из теоремы выше называется \textbf{процессом ортогонализации Грама-Шмидта}}.\\\\
Так как построенная на $k$-ом шаге процесса ортогонализации система векторов $(a_1,\dots,a_n)$ ортогональна и эквивалентна $(b_1, \dots, b_n)$, то в результате применения
процесса ортогонализации к линейно зависимой системе векторов, на некотором шаге получим нулевой вектор.
\newtheorem*{cor14_2_1}{Следствие}\begin{cor14_2_1}Любую ортогональную систему ненулевых векторов можно дополнить до ортогонального базиса.
\end{cor14_2_1}\begin{Proof}
	Если ортогональная система векторов линейно независима, то её можно дополнить до базиса пространства. Если к этому базису применить процесс ортогонализации, то первые векторы построенной системы будут совпадать с исходной ортогональной системой. Следовательно, полученный ортогональный базис является дополнением до ортогонального базиса исходной ортогональной системы векторов.
\end{Proof}










\section{Длина вектора. Ортонормированный базис.}
Пусть $V$ --- евклидово пространство.\\\\ $\bullet$ \textit{\textbf{Длиной} вектора $a\in V$ называется число $|a|=\sqrt{a \cdot a}\geqslant0$.}\\\\
\textit{\textbf{Свойства длины вектора:}}\begin{enumerate}
	\item  $|a|=0 \Longleftrightarrow a=0_v$.
	\begin{Proof}
		Следует из определения скалярного произведения и его простейших
		свойств.
	\end{Proof}
	\item $|\alpha a|=|\alpha|\cdot |a|,\quad \forall a,b\in V, \alpha \in \mathbb{R}$.
	\begin{Proof}
		$|\alpha a|^2 = (\alpha\cdot a )\cdot(\alpha\cdot a) = (\alpha\cdot\alpha)\cdot(a\cdot a) = |\alpha|^2\cdot|a|^2$. Извлечем корень и получим $|\alpha a|=|\alpha|\cdot |a|$.
	\end{Proof}
	\item \textbf{Неравенство Коши-Буняковского:}
	$|ab| \leqslant |a||b|,$ \textit{причем} $|ab|=|a||b| \, \Longleftrightarrow \, a,b$\textit{ --- линейно зависимы.}
	\begin{Proof}
		Для любых векторов $a,b$ пространства $V$ матрица Грама имеет неотрицательный определитель. Причем определитель матрицы Грама равен нулю в том и только в том случае, когда векторы $a,b$ линейно зависимы. Тогда $detG=\begin{vmatrix}
			aa&ab\\
			ba&bb\\
		\end{vmatrix} = (a\cdot a)\cdot(b\cdot b)-(a\cdot b)^2=|a|^2\cdot |b|^2 - |a\cdot b|^2 \geqslant 0\Rightarrow |ab|^2\leqslant|a|^2|b|^2$. Извлечем корень и получим $|ab|\leqslant|a||b|$, причем равенство выполняется при условии, что векторы $a$ и $b$ линейно зависимы.
	\end{Proof}
	\item \textbf{Неравенство треугольника:}
	$|a+b| \leqslant |a|+|b|,\quad \forall a,b \in V$.\begin{Proof}
		$|a+b|^2=(a+b)\cdot(a+b)=a \cdot a+ b \cdot b + 2ab \leqslant |a|^2+|b|^2+2|a\cdot b| \leqslant |a|^2+|b|^2 + 2|a|\cdot|b| = (|a| + |b|)^2$. Извлечем корень и получим $|a+b| \leqslant |a|+|b|$.
	\end{Proof}
	\item \textbf{Теорема Пифагора:}
	\textit{Если векторы $a$ и $b$ ортогональны, то} $|a+b|^2=|a|^2+|b|^2$.\begin{Proof} Пусть векторы $a$ и $b$ ортогональны. Тогда их скалярное произведение равно нулю. Следовательно, $|a+b|^2=(a+b)\cdot(a+b)=a \cdot a + b \cdot b +2ab=|a|^2+|b|^2$.
	\end{Proof}
\end{enumerate}
Из неравенства Коши-Буняковского следует\\\\
$\bullet$ \textit{В евклидовом пространстве \textbf{углом} между векторами $a$ и $b$ называется величина $\varphi = arccos\dfrac{ab}{|a|\cdot|b|}\in [0, \pi]$.}\\\\
$\bullet\ $\textit{Вектор называется \textbf{ нормированным}, если его длина равна 1}.\\\\
$\bullet\ $\textit{Система векторов называется \textbf{ортонормированной}, если она ортогональна и каждый ее вектор нормированный}.\\\\
Матрица Грама ортонормированной системы векторов ялвяется единичной. Пусть $B$ --- ортнормированный базис и $x,y$ --- произвольные векторы пространства $V$ с координатными столбцами $X = \begin{pmatrix}
	\alpha_1\\ \vdots \\ \alpha_n
\end{pmatrix}, Y = \begin{pmatrix}
	\beta_1\\ \vdots \\ \beta_n
\end{pmatrix}$ в базисе $B$. Тогда $xy = X^TG_BY = [G_B = E] = X^T Y = \alpha_1 \beta_1 + \ldots + \alpha_n \beta_n$.
\newtheorem*{th14_3_1}{Теорема}\begin{th14_3_1}В любом евклидовом пространстве существует ортонормированный базис.
\end{th14_3_1}\begin{Proof}
	В любом евклидовом пространстве $V$ существует ортогональный базис $\bsys$. Пронормируем каждый вектор $b_i$ этого базиса, то есть домножим на $\dfrac{1}{|b_i|}$, и получим систему векторов $B'=(b_1',\dots,b_n')$, для векторов которой справедливо следующее\\
	$b_i' b_j'=\dfrac{1}{|b_i|}b_i \cdot \dfrac{1}{|b_j|}b_j=\dfrac{1}{|b_i||b_j|} (b_i, b_j) = \begin{cases}
		\dfrac{1}{|b_i||b_j|} (b_i, b_j) = 1,\ i = j,\\
		\dfrac{1}{|b_i||b_j|}\cdot 0 = 0,\ i\ne j;
	\end{cases} \Rightarrow$ система векторов $B'$ ортонормированная и  линейно независимая, так как является ортогональной системой ненулевых векторов. При этом количество векторов в этой системе равно количеству векторов в базисе $B$ пространства. Следовательно, эта система сама является базисом.
\end{Proof}\\\\
$\bullet$ \textit{Действительная матрица $S\in\mathbb{R}_{n,n}$ называется \textbf{ортогональной}, если $S^T S= S S^T=E\  (S^{-1}=S^T)$.}
\newtheorem*{th14_3_2}{Теорема}\begin{th14_3_2}Матрица перехода $S_{A\rightarrow B}$ от ортонормированного базиса $A$ к ортонормированному базису $B$ ортогональна.
\end{th14_3_2}\begin{Proof}
	Пусть $G_A, G_B$ --- матрицы скалярного произведения в ортонормированных базисах $A$ и $B$ соответственно. Тогда $G_B=S^T G_A S,$ где $S=S_{A \rightarrow B}$. А так как базисы ортонормированные, то $G_A = G_B = E$. И получаем $E=S^T E S=S^T S$.
\end{Proof}











\section{Ортогональное дополнение подпространства.}
$\bullet$ \textit{Пусть $V$ --- евклидово пространство, $U$ --- его подпространство. Множество всех векторов пространства $V$, ортогональных каждому вектору из $U$, называется \textbf{ортогональным дополнением} подпространства $U$ (Обозначение: $U^\perp$).}
\newtheorem*{th14_4_1}{Теорема}\begin{th14_4_1}Ортогональное дополнение любого подпространства $U$ векторного пространства $V$ также является подпространством, причем $U \cap U^{\perp}=\{0_v\}$.
\end{th14_4_1}\begin{Proof}
	Так как нулевой вектор ортогонален любому вектору пространства $V$, то он ортогонален и любому вектору подпространства $U$. Следовательно, $0_v\in U^\perp$, значит $U^\perp$ --- непустое множество.\\\\
	Пусть векторы $x,y\in U^\perp \Rightarrow \forall a\in U\quad ax = ay = 0\Rightarrow\begin{cases}
		a(x+y) = ax + ay = 0 + 0 = 0,\\
		a(\alpha x) = \alpha(ax) = \alpha\cdot 0 = 0;
	\end{cases}\Rightarrow (x+y), \alpha x\in U^\perp \Rightarrow U^\perp$ --- подпространство $V$.\\\\
	Покажем, что $U \cap U^{\perp}=\{0_v\}$. Пусть вектор $x\in U\cap U^\perp\Rightarrow x\in U,\ x\in U^\perp\Rightarrow$ вектор $x$ ортогонален самому себе $\Rightarrow x\cdot x = 0 \Rightarrow x = 0_v$.
\end{Proof}
\newtheorem*{th14_4_2}{Теорема об ортогональном разложении пространства}\begin{th14_4_2}
	Для любого подпространства $U$ векторное пространство $V$ представимо в виде $V= U \oplus U^{\perp}$
\end{th14_4_2}\begin{Proof} Пусть $U$ --- тривиальное подпространство.\begin{enumerate}
		\item Если $U = \{0_v\}$, то $U^\perp = V$, так как нулевой вектор ортогонален любому вектору.
		\item Если $U = V$, то $U^\perp = \{0_v\}$, так как $U\cap U^\perp = \{0_v\}\Rightarrow V = V + \{0_v\} = U + U^{\perp}$.
	\end{enumerate}
	Пусть $U$ --- нетривиальное подпространство. Тогда в нем существует ортонормированный базис  $B(b_1,\dots,b_k)$. Дополним его до ортогонального базиса пространства $V$ и получим систему векторов $(b_1,\dots,b_k, a_1,\dots,a_s)$. Тогда, так как любой вектор из линейной оболочки $L(a_1,\dots,a_s)$ ортогонален любому вектору вектору из линейной оболочки $L(b_1,\dots,b_k)$, то $(\sum\limits_{i=1}^s\alpha_ia_i)(\sum\limits_{j=1}^k\beta_jb_j) = \sum\limits_{i=1}^s\sum\limits_{j=1}^k(\alpha_i\beta_j)(a_ib_j) = 0\Rightarrow L(a_1,\dots,a_s)\subseteq U^\perp$. А так как $(b_1,\dots,b_k,a_1,\dots,a_s)$ --- базис, то любой вектор $x \in V$ представим в виде $x = \underbrace{\beta_1b_1 + \ldots + \beta_k b_k}_{\in U} +\underbrace{ \alpha_1 a_1 + \ldots + \alpha_s a_s}_{\in U^\perp} \Rightarrow U + U^\perp = V$, а так как $U\cap U^\perp = \{0_v\}$, то сумма подпространств прямая.
\end{Proof}
\newtheorem*{cor14_4_1}{Следствие}\begin{cor14_4_1}$\dim  U+\dim  U^{\perp}= \dim  V$
\end{cor14_4_1}
\begin{Proof}
	Доказательство следует из формулы Грассмана.
\end{Proof}
\newtheorem*{cor14_4_2}{Следствие}\begin{cor14_4_2}$(U^{\perp})^{\perp}=U$.
\end{cor14_4_2}\begin{Proof} Так как предыдущее следствие справедливо для любого подпространства, то оно српаведливо и для $U^\perp$. Тогда, по предыдущему следствию,
	$\begin{cases}
		\dim  U = \dim  V - \dim  U^\perp\\
		(\dim  U^\perp)^\perp = \dim V - \dim  U^\perp
	\end{cases}\Rightarrow \dim  U = \dim  (U^\perp)^\perp$, причем любой вектор $a \in U$ ортогонален любому вектору $b \in U^\perp\Rightarrow U\subseteq (U^\perp)^\perp\Rightarrow$ по теореме о монотонности размерности, $U = (U^\perp)^\perp$
\end{Proof}\\\\
$\bullet$ \textit{Так как для любого подпространства $U$ векторное пространство $V$ представимо в виде $V = U\oplus U^\perp$, то любой вектор $a\in V$ представим единственным образом в виде $a = a_1 + a_2$, где $a_1\in U,\ a_2 \in U^\perp$. Вектор $a_1$ называется \textbf{ортогональной проекцией} вектора $a$ на подпространство $U$, вектор $a_2$ --- \textbf{ортогональной составляющей} вектора $a$ относительно подпространства $U$.}










\section{Ортогональный оператор.}
Пусть $V$ --- евклидово пространство.\\\\
$\bullet$ \textit{Линейный оператор $f: V \to V$ называется \textbf{ортогональным}, если $f(x)\cdot f(y) = x\cdot y$ для всех $x,y \in V$.}\\\\
\textit{ \textbf{Свойства ортогонального оператора:}}\begin{enumerate}
	\item \textit{Линейный оператор пространства $V$ является ортогональным $\Longleftrightarrow |f(x)| = |x|$ для всех $x \in V$.}\begin{Proof}
		$\Rightarrow)$ Пусть $f$ -- ортогональный оператор $\Rightarrow |f(x)|^2 = f(x) \cdot f(x) = x \cdot x = |x|^2\Rightarrow |f(x)| = |x|.$\\\\
		$\Leftarrow$) Пусть $f$ сохраняет длины всех векторов $\Rightarrow \forall x,y \in V\ |x + y| = |f(x+y)| = |f(x) + f(y)|. $\\
		$|x + y|^2 = (x+y)(x+y) = x x + 2xy + y y = |x|^2 + 2xy + |y|^2.\\ |f(x + y)|^2  =  |f(x)^2|+ 2 f(x)f(y) + |f(y)^2| = |x|^2 + |y|^2 + 2f(x)f(y) \Rightarrow xy = f(x)f(y).$
	\end{Proof}
	\item \textit{Модуль собственного значения ортогонального оператора равен 1.}
	\begin{Proof}
		Пусть $x_0$ -- собственный вектор, соответствующий собственному значению $\lambda_{0}$, то есть $f(x_0) = \lambda_{0}x_0 \Rightarrow |x_0| = |f(x_0)| = |\lambda_{0}x_0| = |\lambda_{0}|\cdot|x_0|$. Так как вектор $x_0$ собственный, то он ненулевой $\Rightarrow |x_0| \ne 0\Rightarrow|\lambda_0| = 1$.
	\end{Proof}
	\item \textit{Ортогональный оператор биективен.}\begin{Proof}
		$\forall x \in Ker\ f \quad f(x) = 0_v \Rightarrow |x| = |f(x)| = |0_{v}| = 0 \Rightarrow x= 0_{v} \Rightarrow Ker\ f = \{ 0_{v} \}  \Rightarrow f$ --- иньекция $ \Rightarrow f$ --- сюрьекция, а, значит, и биекция.
	\end{Proof}
\end{enumerate}
\newtheorem*{th14_5_1}{Теорема}\begin{th14_5_1}Линейный оператор является ортогональным $\Longleftrightarrow$ он ортонормированный базис отображает в ортонормированный.
\end{th14_5_1}
\begin{Proof}
	$\Rightarrow$) Так как $f$ ортогональный оператор, он ортонормированный базис $\bsys$ отобразит в ортонормированную систему векторов. А так как ортогональный оператор сохраняет длины и скалярные произведения, то система векторов $f(B)$ состоит из попарно ортогональных ненулевых векторов, а, значит, является линейно независимой системой и содержит столько же векторов, сколько и базис. Следовательно, система сама является базисом.\\\\
	$\Leftarrow$) Пусть оператор $f$ переводит ортонормированный базис $\bsys$ в ортонормированный базис $f(B) = (f(b_1), \dots, f(b_n))$, и пусть для произвольных векторов $x,y \in V$ $X,Y$ --- координатные столбцы в базисе $B$. Тогда $xy = X^TG_BY = X^TY$.\\ Если $X = \begin{pmatrix}\alpha_1\\ \vdots \\ \alpha_n\end{pmatrix}$, то $x = \alpha_1b_1+ \ldots + \alpha_nb_n \Rightarrow f(x)= \alpha_1f(b_1)+ \ldots + \alpha_nf(b_n) \Rightarrow $ вектор $f(x)$ в базисе $f(B)$ имеет координатный столбец равный $X$. Аналогично для $Y$. Следовательно, $f(x)f(y) = X^TG_{f(B)}Y = X^TY = xy $.
\end{Proof}
\newtheorem*{th14_5_2}{Теорема}\begin{th14_5_2}Линейный оператор является ортогональным $\Longleftrightarrow$ его матрица в ортонормированном базисе ортогональная.
\end{th14_5_2}\begin{Proof} $\Rightarrow)$ Пусть $\bsys$ --- ортонормированный базис пространства $V$, $A = M_{f}^B$, $A_i$ --- столбцы матрицы $A$. Тогда $A_i$ -- координатный столбец вектора $f(b_i)$ в базисе $B \Rightarrow f(b_i)f(b_j) = A_i^TG_BA_j = A_i^TA_j$, то есть равно элементу матрицы $A^TA$, стоящему в $i$-й строке и $j$-м столбце.\\ С другой стороны, так как оператор $f$ ортогональный, то $f(b_i)f(b_j)=
	\begin{cases}
		0,\text{ если } i \neq j,\\
		1,\text{ если } i = j;
	\end{cases} \Rightarrow A^TA = E$, то есть матрица ортогональная.\\\\
	$\Leftarrow$) Пусть матрица $M^B_f$ оператора $f$ в ортонормированном базисе $B$ равна ортогональной матрице $A$. Так как базис $B$ ортонормированный, то $G_B = E$. \\Пусть $x, y \in V$ --- произвольные векторы c координатными столбцами $X$ и $Y$ в базисе $B$ соответственно. Тогда $xy = X^TG_BY = X^TY.\\$
	С другой стороны, векторы $f(x)$ и $f(y)$ в базисе $B$ имеют координатные столбцы $AX$ и $AY$ соответственно $\Rightarrow f(x) f(y) = (AX)^TG_BAY = X^T(A^TA)Y=[A^TA=E] = X^TY = xy$.
\end{Proof}














\section{Самосопряженный оператор.}
Пусть $V$ --- евклидово пространство.\\\\
$\bullet$ \textit{Линейный оператор $f:{V \rightarrow V}$ называется \textbf{самосопряжённым}, если $f(x)\cdot y=x\cdot f(y)$ для всех векторов $x, y\in V$.}
\newtheorem*{th14_6_1}{Теорема}\begin{th14_6_1}Линейный оператор является самосопряженным $\Longleftrightarrow$ его матрица в ортонормированном базисе симметрическая.
\end{th14_6_1}\begin{Proof}
	$\Rightarrow)$ Пусть $\bsys$ --- ортонормированный базис пространства $V$, а матрица $A(\alpha_{ij})$ равна матрице оператора $f$ в базисе $B$, то есть $M_f^B$. Тогда $f(b_i)=\alpha_{1i}b_1+ \ldots +\alpha_{ni}b_n$. Из данного равенства имеем\\
	$\begin{cases}
		f(b_i)\cdot b_j=(\alpha_{1i}b_1+ \ldots +\alpha_{ni}b_n)\cdot b_j = \alpha_{ji},\\
		f(b_j)\cdot b_i=(\alpha_{1j}b_1+ \ldots +\alpha_{nj}b_n)\cdot b_i=\alpha_{ij};
	\end{cases}\Rightarrow \alpha_{ij} = \alpha_{ji}$, так как оператор $f$ самосопряженный.\\\\
	$\Leftarrow)$ Пусть матрица $A$ оператора оператора $f$ в базисе $B$ является симметрической. Тогда $G_B=E\Rightarrow\forall x,y\in V\ X,Y$ --- координатные столбцы векторов $x$ и $y$ в базисе $B$ соответственно. Следовательно, $f(x)\cdot y = (AX)^TG_BY = X^TA^TY = X^TG_B(AY) = x\cdot f(y)$.
\end{Proof}\\\\
\textbf{\textit{Свойства самосопряженных операторов:}}\begin{enumerate}
	\item \textit{Характеристический многочлен самосопряженного оператора имеет с учетом кратности столько действительных корней, какова его степень.}
	\newtheorem*{cor14_6_1}{Следствие}\begin{cor14_6_1}Характеристический многочлен действительной симметрической матрицы порядка $n$ имеет с учетом кратности ровно $n$ действительных корней.
	\end{cor14_6_1}
	\item \textit{Собственные векторы самосопряженного оператора, соответствующие различным собственным значениям, ортогональны.}\begin{Proof}
		Пусть $a_1$, $a_2$ --- собственные векторы самосопряженного оператора $f$, соответствующие собственным значениям $\lambda_1$ и $\lambda_2$ таким, что $\lambda_1 \neq \lambda_2$. \\Тогда $f(a_1)=\lambda_1a_1$ и $f(a_2)=\lambda_2a_2$. При этом\\
		$\begin{cases}
			f(a_1)\cdot a_2=\lambda_1\cdot (a_1\cdot a_2),\\
			a_1\cdot f(a_2)=\lambda_2\cdot (a_1\cdot a_2);
		\end{cases}\Rightarrow$ так как оператор $f$ самосопряженный, то $f(a_1)\cdot a_2 = a_1\cdot f(a_2)\Rightarrow\lambda_1\cdot (a_1\cdot a_2)=\lambda_2\cdot (a_1\cdot a_2)\Rightarrow \underset{\ne 0}{(\lambda_1-\lambda_2)}\cdot (a_1\cdot a_2)=0\Rightarrow a_1\cdot a_2=0$. Следовательно, векторы $a_1$ и $a_2$ ортогональны.
	\end{Proof}
	\item \textit{Если $f$ --- самосопряженный оператор пространства $V$, $U$ --- инвариантное относительно $f$ подпространство, то подпространство $U^\perp$ также инвариантно относительно $f$.}\begin{Proof}
		Покажем, что $\forall x \in U^\perp\ f(x)\in U^\perp$.\\
		Так как $U$ --- инвариантное относительно оператора $f$ подпространство, то $\forall y\in U\ f(y) \in U\Rightarrow x\cdot f(y) = 0_v\Rightarrow f(x)\cdot y = 0_v$, так как оператор $f$ самосопряженный. Следовательно, $f(x)\in U^\perp$.
	\end{Proof}
\end{enumerate}
\newtheorem*{th14_6_2}{Теорема}\begin{th14_6_2}Для любого самосопряженного оператора $f$ существует ортонормированный базис пространства $V$, состоящий из собственных векторов оператора $f$.
\end{th14_6_2}\begin{Proof}
	Пусть $f$ --- самосопряженный оператор, и $\lambda_1, \dots, \lambda_s$ --- различные собственные значения оператора $f$. Обозначим через $L(\lambda_i)$ собственные подпространства оператора $f$, соответствующие собственным значениям $\lambda_i$. Построим в каждом из них ортонормированный базис $B_i$, а затем построим систему векторов $B(B_1, \dots, B_s)$. Так как собственные векторы самосопряженного оператора, соответствующие попарно различным собственным значениям, ортогональны, то система $B$ ортонормированная. Значит, система $B$ линейно независимая, так как является ортогональной системой ненулевых векторов. \\\\Покажем, что $B$ --- базис пространства $V$. От противного. Пусть система $B$ не является базисом $\Rightarrow L(B) \ne V \Rightarrow \dim (L(B)) = k < n$. При этом $L(B) \oplus (L(B))^\perp = V \Rightarrow \dim (L(B))^\perp\geqslant 1$.\\\\
	Покажем, что подпространство $L(B)$ инвариантно относительно оператора $f$. Пусть $x \in L(B)$. Тогда $x = \beta_1b_1 + \ldots + \beta_kb_k$, а $f(x) = \beta_1f(b_1) + \ldots + \beta_kf(b_k) = \beta_1\lambda_1b_1 + \ldots + \beta_k\lambda_kb_k$ --- линейная комбинация, а, значит, принадлежит линейной оболочке $L(B)\Rightarrow$ $L(B)$ --- инвариантное относительно оператора $f$ подпространство пространства $V \Rightarrow$ подпространство $(L(B))^\perp$ также инвариантно относительно $f$.\\\\		
	Значит, существует ограничение $f|_{(L(B))^\perp}$, которое также является самосопряженным оператором. Следовательно, у него существует собственное значение $\lambda_0$, а, значит, и собственный вектор $x_0$, соответствующий данному собственному значению. Тогда $f(x_0) = f|_{(L(B))^\perp}(x_0) = \lambda_0x_0$ $\Rightarrow x_0 $ --- собственный вектор и для оператора $f$, который принадлежит $L(B)$, а это противоречит тому, что подпространство $L(B)$ содержит все собственные векторы. Следовательно, система $B$ является базисом пространства $V$.
\end{Proof}






\section{Приведение действительной квадратичной формы к каноническому виду ортогональным преобразованием переменных.}
Пусть $$f(x_1,\dots,x_n) = \sum_{i,j=1}^n\alpha_{ij}x_ix_j,\quad\alpha_{i,j}\in\mathbb{R},\quad\alpha_{ij}=\alpha_{ji},$$
или $$f(X) = X^T A X,\quad X=\begin{pmatrix}x_1\\ \vdots \\ x_n\end{pmatrix},\quad A(\alpha_{ij})\in\mathbb{R}_{n,n},\quad\alpha_{ij}=\alpha_{ji},$$ --- некоторая действительная квадратичная форма.\\\\
$\bullet$ \textit{Линейное преобразование переменных $X=SY$ называется \textit{\textbf{ортогональным}}, если матрица $S$ ортогональная, то есть $S^TS = E$.}\\\\
Так как ортогональная матрица является невырожденной, то и ортогональное преобразование тоже невырожденное.
\newtheorem*{lem14_7}{Лемма}\begin{lem14_7}
	Для любой действительной симметричесокй матрицы $A$ существует ортогональная матрица $S$ такая, что матрица $S^TAS$ является диагональной, причем ее диагональные элементы равны собственным значениям матрицы $A$.
\end{lem14_7}
\begin{Proof}
	Так как отображение, ставящее в соответствие каждому линейному оператору пространства $\mathbb{R}$ его матрицу в базисе $Е$, является изоморфизмом, то $\exists$ линейный оператор $f\in \mathbb{R}^n$ : $M_f^E = A$. Если базис $E$ ортонормированный, то $f$ является самосопряженным оператором, так как матрица $A$ симметрическая. Следовательно, существует ортонормированный базис $B$, состоящий из собственных векторов $\Rightarrow M_f^B$ --- диагональная матрица, причем диагональными элементами являются собственные значения оператора $f$. Так как $M_f^B$ и $A$ являются матрицами одного линейного оператора в разных базисах, они подобны, то есть $\exists S: M_f^B = S^{-1}M_f^E S=S^{-1}AS$, где $S = S_{E\rightarrow B}$. А так как базисы $E,B$ ортонормированы, то матрица $S$ ортогональна и $M_f^B = S^TAS$.
\end{Proof}
\newtheorem*{th14_7_1}{Теорема}\begin{th14_7_1}Для любой действительной квадратичной формы $f(X)$ существует линейное ортогональное преобразование $X=SY$, переводящее $f$ в каноническую квадратичную форму $\lambda_1y_1^2 + \dots \lambda_ny_n^2$, причем числа $\lambda_i$ являются собственными значениями матрицы квадратичной формы $f$.
\end{th14_7_1}\begin{Proof}
	Пусть $A$ --- симметрическая матрица квадратичной формы $f$. Тогда $f(X)=X^TAX \Rightarrow$ существует ортогональная матрица $S$ такая, что
	$S^TAS = \diag (\lambda_1,\dots,\lambda_n) = D$.\\ Применим замену $X = SY$ с матрицей $S$ к квадратичной форме $f$ и получиам $f(SY)=(SY)^TA(SY)=Y^T(S^TAS)Y=Y^TDY=\lambda_1y_1^2 + \ldots + \lambda_ny_n^2$.
\end{Proof}
\newtheorem*{cor14_7}{Следствие (критерий положительной определенности квадратичной формы)}\begin{cor14_7}Действительная квадратичная форма является положительно определенной $\Longleftrightarrow$ все собственные значения ее матрицы положительны.
\end{cor14_7}





\section{Полярное разложение линейного оператора.}
\newtheorem*{lem14_8_1}{Лемма}\begin{lem14_8_1}Для любой действительной невырожденной матрицы $A$ квадратичная форма $f(X) = X^T(A^TA)X$ является положительно определенной.
\end{lem14_8_1}\begin{Proof}
	Поскольку матрица $A$ невырожденная, то линейная однородная система уравнений $AX = 0$ имеет лишь нулевое решение. Следовательно, для любого ненулевого столбца $X_0$ столбец $AX_0 = \begin{pmatrix}
		\gamma_1\\
		\vdots\\
		\gamma_n
	\end{pmatrix}$ является ненулевым. Тогда $f(X_0) = (X_0^TA^T)(AX_0) = (AX_0)^T(AX_0) = (\gamma_1,\dots,\gamma_n)\begin{pmatrix}
		\gamma_1\\
		\vdots\\
		\gamma_n
	\end{pmatrix} = \gamma_1^2 + \ldots + \gamma_n^2 > 0$, поскольку все $\gamma_i$ не обращаются в ноль одновременно. Следовательно, квадратичная форма является положительноопределенной.
\end{Proof}
\newtheorem*{cor14_8_1}{Следствие}\begin{cor14_8_1}Для любой квадратной действительной невырожденной матрицы $A$ собственные значения матрицы $A^TA$ положительные.
\end{cor14_8_1}\begin{Proof}
	Поскольку $(A^TA)^T = A^TA$, то есть при транспонировании матрица $A^TA$ не изменилась, то матрица $A$ симметрическая. Следовательно, она является матрицей квадратичной формы $f(X) = X^T(A^TA)X$, которая по предыщуей лемме является положительноопределенной. Тогда, по критерию положительной определенности квадратичной формы, все собственные значения матрицы $A^TA$ положительны.
\end{Proof}
\newtheorem*{lem14_8_2}{Лемма}\begin{lem14_8_2}Для любой квадратной невырожденной матрицы $A$ существует
	симметрическая матрица $C$ с положительными собственными значениями такая, что $A^TA=C^2$
\end{lem14_8_2}\begin{Proof}
	Поскольку матрица $A^TA$ симметрическая, существует ортогональная матрица $S$ такая, что матрица $S^TA^TAS$ является диагональной матрицей, содержащей все собственные значения матрицы $A^TA$ на диагонали.\\\\ Так как квадратичная форма с матрицей $A^TA$ является положительноопределенной по предыдущему следствию, то все собственные значения положительны. Тогда существует матрица $D = \diag (\sqrt{\lambda_1},\dots,\sqrt{\lambda_n})$, причем $D^2 = S^TA^TAS$. Следовательно, $A^TA = (S^T)^{-1}D^2S^{-1}$, и так как матрица $S$ ортогональная, то есть $S^{-1} = S^T$, то $A^TA = (S^{-1})^{-1}D^2S^T = SDEDS^T = (SDS^T)(SDS^T)$.\\\\ Обозначим через $C$ матрицу $SDS^T$, тогда получим $A^TA = C^2$, при этом $C$ --- симметрическая матрица, так как $C^T = (SDS^T)^T = SD^TS^T = $ [ диагональ матрицы $D$ не изменяется при транспонировании ] $ = SDS^T = C$. А так как матрица $S$ ортогональная, то $C = SDS^T = SDS^{-1}$. Следовательно, матрицы $C$ и $D$ подобны и имеют одинаковые собственные значения равные $\sqrt{\lambda_i}>0$.
\end{Proof}
\newtheorem*{th14_8}{Теорема}\begin{th14_8}Любая действительная невырожденная квадратная матрица представима в виде произведения отрогональной матрицы и симметрической матрицы с положительными собственными значениями.
\end{th14_8}\begin{Proof}
	Пусть $A$ --- произвольная действительная невырожденная матрица. Тогда существует симметрическая матрица $C$ с положительными собственными значениями такая, что $C^2 = A^TA$.\\\\
	Так как матрица $C$ невырожденная, существует обратная ей невырожденная матрица $C^{-1}$. Построим матрицу $U = AC^{-1}\Rightarrow A=UC$. Покажем, что матрица $U$ ортогональная: $U^TU = (AC^{-1})^TAC^{-1}=(C^{-1})^TA^TAC^{-1} = (C^{-1})^TC^2C^{-1} = (C^{-1})^TC = $ [так как матрица $C$ симметрическая] $ = (C^{-1})^TC^T = (CC^{-1})^T = E$.
\end{Proof}\\\\
\textbf{Следствие.} \textit{Любой биективный линейный оператор евклидового пространства представим в виде композиции ортогонального оператора и самосопряженного оператора с положительными собственными значениями.}\\\\
$\bullet$ \textit{Представление линейного оператора евклидового пространства в виде композиции ортогонального оператора и самосопряженного оператора называется \textbf{полярным разложением} линейного оператора}.





\section{Унитарное пространство.}
$\bullet$ \textit{Пусть $V$ --- векторное пространство над полем $\mathbb{C}$. Отображение $V \times V\rightarrow \mathbb{C}$,
	ставящее в соответствие упорядоченной паре векторов $(a, b)$ число $ab\in \mathbb{C}$, называется \textbf{скалярным произведением}, если $\forall \alpha \in \mathbb{C}, a, b, c \in V$}\begin{enumerate}
	\item  $ab = \overline{ba}$
	\item $(\alpha a)b = \alpha(ab)$
	\item $a(b + c) = ab + ac$
	\item $\forall  a \neq 0_{v} , a\cdot a \in R$  и $a\cdot a > 0$
\end{enumerate}
$\bullet$ \textit{Комплексное векторное пространство с определенным на нем скалярным произведением называется \textbf{унитарным пространством}.}\\\\
\textit{ \textbf{Простейшие свойства скалярного произведения:}}\begin{enumerate}
	\item $a(\alpha b) = \overline{\alpha}(ab)$, $\quad\forall a, b \in V, \alpha \in \mathbb{C}$.\begin{Proof}
		$a(\alpha b) = \overline{(\alpha b)}a=\overline{\alpha(ba)}=\overline{\alpha}(\overline{ba})= \overline{\alpha}(ab)$.
	\end{Proof}
	\item $a(b+c)=ab+ac,\quad\forall a,b,c\in V$.
	\item $a\cdot 0_v=0_v \cdot a =0_{\mathbb{C}},\quad \forall a \in V$.
\end{enumerate}
Из определения скалярного произведения следует, что для элементов матрицы Грама $G_B(\gamma_{ij})$ произвольной системы векторов $B$ справедливо равенство $\gamma_{ij} = b_ib_j=\overline{b_jb_i} = \overline{\gamma_{ji}}$.\\\\
$\bullet$ \textit{Матрица $B(\beta_{ij})$ называется \textbf{эрмитово сопряженной} для матрицы $A(\alpha_{ij})$, если $\beta_{ij}=\overline{\alpha_{ji}}$ (обозначение $A^*$), то есть $A^* = \overline{A}^T$.\\\\$\bullet$ Матрица $A$ называется \textbf{эрмитовой}, если $A=A^*$.\\\\ $\bullet$ Матрица $A$ называется \textbf{унитарной}, если $A^*\cdot A = A\cdot A^* = E$}.\\\\
Таким образом матрица Грама любой системы векторов и, следовательно, матрица скалярного произведения пространства $V$ является эрмитовой.
\newtheorem*{th14_9_1}{Теорема}\begin{th14_9_1}Если $G_B$ --- матрица скалярного произведения пространства $V$ в базисе $B$, $ X=\begin{pmatrix}
		x_1\\
		\vdots\\
		x_n
	\end{pmatrix}$, 
	$ Y=\begin{pmatrix}
		y_1\\
		\vdots\\
		y_n
	\end{pmatrix}$ --- координатные столбцы векторов $x, y\in V$ в базисе $B$, то $xy=X^T G_B \overline{Y} $
\end{th14_9_1}\begin{Proof}
	$xy=(\sum\limits_{i=1}^{n}{x_ib_i})(\sum\limits_{j=1}^{n}{y_jb_j})=\sum\limits_{i,j=1}^{n}{(x_ib_i)(y_jb_j)}=\sum\limits_{i,j=1}^{n}{(x_i\overline{y_j})(b_ib_j)}=X^TG_B\overline{Y}$
\end{Proof}
\newtheorem*{cor14_9_1}{Следствие}\begin{cor14_9_1}Если $G_A, G_B$ --- матрицы скалярного произведения в базисах $A, B$ соответственно, то $G_B = S^TG_A\overline{S}$, где $S = S_{A\rightarrow B}$.
\end{cor14_9_1}\begin{Proof}
	Для любых векторов $x,y\in V\quad xy = X_A^TG_A\overline{A} = (SX_B)^TG_ASY_B=X_A^T(S^TG_AS)Y_B$. Так как равенство выполняется для всех векторов из пространства $V$, оно выполняется и для всех векторов из базиса $B\Rightarrow G_B = S^TG_A\overline{S}$.
\end{Proof}
\newtheorem*{cor14_9_2}{Следствие}\begin{cor14_9_2}
	Определитель матрицы скалярного произведения равен действительному положительному числу.
\end{cor14_9_2}\begin{Proof}
	Пусть система векторов $\asys$ является ортогональным базисом пространства $V$. 
	Тогда матрица
	$G_A$ диагональная и состоит из действительных чисел, при этом все диагональные элементы положительны. Следовательно, определитель матрицы $G_A$ также положительное действительное число. Так как для любого базиса $B$ выполняется $G_B = S^T G_A \overline{S}$, то
	$det G_B = det(S^T)\cdot det(G_A)\cdot det(\overline{S})= det(S)\cdot det(G_A) \cdot|det(S)|^2\cdot det(G_A) > 0$ 
\end{Proof}\\\\
$\bullet$ \textit{\textbf{Длиной} вектора $a\in V$ такого, что $a\cdot a \in \mathbb{R}$, называется дейтсвительное число $|a|=\sqrt{a \cdot a}\geqslant0$.}\\\\
Если каждый вектор $b_i$ ортогонального базиса $B$ умножить на $|\overline{b}_i|$, то получим ортонормированный базис пространства $V$. Следовательно, в любом унитарном постранстве существует ортонормированный базис.
\newtheorem*{th14_9_2}{Теорема}\begin{th14_9_2}Матрица перехода от ортонормированного базиса к ортонормированному базису является унитарной, то есть $S^*\cdot S=E$.
\end{th14_9_2}\begin{Proof}
	Если системы векторов $A$ и $B$ --- ортонормированные базисы пространства $V$, то $G_A=G_B=E$. При этом, $G_B = S^T G_A \overline{S}$, где $S=S_{A\rightarrow B}$. Следовательно, $E = S^T \cdot E \cdot \overline{S},\ E = E^T =  (S^T\overline{S})^T=\overline{S}^TS=S^*\cdot S$.
\end{Proof}\\\\
$\bullet$ \textit{Линейный оператор унитарного пространства $V$ называется \textbf{унитарным (изометрическим)}, если  $f(x)f(y)=xy$ для всех векторов $x,y \in V$. }\\\\
$\bullet$ \textit{Линеный оператор называется \textbf{самосопряженным}, если $f(x)y = xf(y)$ для всех векторов $x,y \in V$. }\\\\
\textit{ \textbf{Свойства унитарного и самосопряженного оператора:}}
\begin{enumerate}
	\item \textit{В ортонормированном базисе унитарный оператор имеет унитарную матрицу, самосопряженный – эрмитову.}\begin{Proof} Пусть $\bsys$ --- ортонормированный базис пространства $V$, $A(\alpha_{ij})$ --- матрица оператора $f$ в базисе $B$. Тогда \\$f(b_k)b_s = (\alpha_{1k}b_1 + \ldots + \alpha_{nk}b_n)b_s = \alpha_{1k} (b_1 b_s ) + \ldots + \alpha_{nk} (b_n b_s) = \alpha_{sk}$.\\
		$b_k f(b_s) = b_k(\alpha_{1s}b_1 + \ldots + \alpha_{ns}b_n) = \overline{\alpha_{1s}}(b_kb_1)+ \ldots +\overline{\alpha_{ns}}(b_k b_n) = \overline{\alpha_{ks}}$.\\
		Если $f$ --- самосопряженный оператор, то $\alpha_{sk} = \overline{\alpha_{ks}}$, следовательно, матрица $A$ эрмитова.\\ Пусть $f$ --- унитарный оператор. Тогда\\
		$f(b_k)f(b_s) = (\sum\limits_{i=1}^{n}{\alpha_{ik}b_i})(\sum\limits_{j=1}^{n}{\alpha_{js}b_j}) = \sum\limits_{i,j=1}^{n}{(\alpha_{ik}b_i)(\alpha_{js}b_j)}=\sum\limits_{i,j=1}^{n}{(\alpha_{ik}\overline{\alpha_{js}})(b_ib_j)} = \sum\limits_{i,j=1}^{n}(\alpha_{ik}\overline{\alpha_{js}})$.\\	Получаем, что $\alpha_{ik}\overline{\alpha_{js}}$ --- элемент матрицы $A^T\overline{A}$ в $k$-ой строке и $s$-ом столбце. Но $b_kb_s = \begin{cases}
			1, \text{ если } k=s,\\
			0, \text{ если } k\ne s.	\end{cases}$\\ Следовательно, $A^T\overline{A} = E\Rightarrow \overline{A}^T A = E^T = E\Rightarrow A^*\cdot A = E$.\end{Proof}
	\item \textit{Модуль собственного значения унитарного оператора равен 1. Все
		собственные значения самосопряженного оператора --- действительные	числа.}\begin{Proof}
		Пусть $a$ --- собственный вектор оператора $f$, соответствующий собственному значению $\lambda$, то есть $f(a) = \lambda a$. \\\\Если $f$ --- унитарный оператор, то $|a|^2 = a a = f(a)f(a) = (\lambda a)(\lambda a) = \lambda\lambda(aa) = |\lambda|^2|a|^2$.
		Так как вектор $a$ ненулевой, то $|\lambda|^2 = 1 \Rightarrow |\lambda| = 1$.\\\\
		Если $f$ --- самосопряженный оператор, то $f(a) a = \lambda a a = \lambda |a|^2;\, af(a) = a\lambda a =  \overline{\lambda} |a|^2$. Так как $f$ --- самосопряженный оператор, то $\lambda|a|^2 = \overline{\lambda} |a|^2\Rightarrow \lambda = \overline{\lambda}\Rightarrow\lambda\in\mathbb{R}$.
	\end{Proof}
\end{enumerate}
\textit{$\bullet$ Любой линейный оператор унитарного пространства можно представить в виде композиции унитарного и самосопряженного операторов. Любая матрица представима в виде $A = U\cdot H$, где $U$ --- унитарная матрица, а $H$ --- эрмитова. Такое представление называется \textbf{полярным разложением} линейного оператора/матрицы}.






\section{Векторные и матричные нормы.}
Пусть $V$ --- векторное пространство над полем $P$ ($\mathbb{R}$ или $\mathbb{C}$).\\\\
$\bullet$ \textit{Отображение $\left \| \cdot  \right \|:V\rightarrow \mathbb{R}$, ставящее в соответствие каждому вектору $a\in V$ действительное число $\left \|a\right \|$, называется \textbf{векторной нормой}, если}
\begin{enumerate}
	\item $\left \|a\right \| \geqslant 0\ \forall a \in V\backslash\{0\}$, и $\left \|a\right \| = 0 \Leftrightarrow a = 0_v$
	\item $\left \|\alpha a\right \|=|\alpha|\cdot\left \|a\right \|$
	\item  $\left \|a+b\right \| \leqslant \left \|a\right \| + \left \|b\right \|$
\end{enumerate}
$\bullet$ \textit{Векторное пространство с определенной на нем векторной нормой называется \textbf{нормированным}.}\\\\
$\bullet$ \textit{Вектор $b\in V$ называется \textbf{пределом последовательности} векторов} $\{a_n\}^\infty_{n=1}$, если $$\forall\upvarepsilon >0\ \exists N_\upvarepsilon : \forall n > N_\upvarepsilon \Rightarrow \left \| a_n - b \right \| < \upvarepsilon. $$
$\bullet$ \textit{Норма $\left \| \cdot  \right \|_1$ называется \textbf{эквивалентной} норме $\left \| \cdot  \right \|_2$, если $\exists \alpha_1, \alpha_2\in \mathbb{R} : \forall a\in V,\ \alpha_1 \left \| a \right\|_2 \leqslant \left \| a \right\|_1\leqslant \alpha_2 \left \| a \right\|_2$}.\\\\
Если вектор $b$ --- предел последовательности $\{a_n\}^\infty_{n=1}$ по норме $\left \| \cdot \right \|_1$, то он является пределом и по норме $\left \| \cdot \right \|_2$ (по любой эквивалентой норме).\\\\ 
В конечномерном векторном пространстве все векторные нормы эквивалентны.\\\\
\textit{$\bullet$ Отображенрие $\left \| \cdot \right \|:P_{n,n} \rightarrow \mathbb{R}$, ставящее в соответствие каждой матрице $A\in P_{n,n}$ действительное число $\left \| A \right \|$ называется \textbf{матричной нормой}, если}
\begin{enumerate}
	\item $\left \|A\right \| \geqslant 0\ \forall A\ne 0$, и $\left \|A\right \| = 0 \Leftrightarrow A = 0$
	\item $\left \|\alpha A\right \|=|\alpha|\cdot\left \|A\right \|$
	\item  $\left \|A+B\right \| \leqslant \left \|A\right \| + \left \|B\right \|$
	\item $\left \|AB\right \| \leqslant \left \|A\right \|\cdot  \left \|B\right \|$
\end{enumerate}
\textit{$\bullet$ Матричная норма $\left \|\cdot\right \|_M$ называется \textbf{согласованной} с векторной нормой $\left \|\cdot\right \|_B$, если} $$\forall A\in P_{n,n},\quad \forall X \in P_{n,1}\quad \left \|AX\right \|_B\leqslant \left \|A\right \|_M\left \|X\right \|_B.$$
\newtheorem*{th14_10_1}{Теорема}\begin{th14_10_1}Если матричная норма $\left \|\cdot\right \|_M \in P_{n,n}$ является согласованной с векторной $\left \|\cdot\right \|_B$ и $\lambda_0$ --- собственное значение матрицы $A$, то $|\lambda| \leqslant \left \|A\right \|_M$
\end{th14_10_1}\begin{Proof}
	Пусть $\lambda_0$ --- собственное значение матрицы $A$. Тогда $\exists x_0$ --- ненулевой собственный вектор матрицы $A$, соответствующий значению $\lambda_0$. Следовательно\\
	$\begin{cases}
		\left \|Ax_0\right \|_B = \left \|\lambda_0 x_0\right \|_B = |\lambda_0| \left \|x_0\right \|_B,\\
		\left \|A x_0\right \|_B \leqslant \left \|A\right \|_M \left \| x_0\right \|_B.
	\end{cases} \Rightarrow$ так как $x_0\ne 0$, то $\left \| x_0\right \|_B \ne 0\Rightarrow$ поделив обе части неравенства на $\left \| x_0\right \|_B$, получим $|\lambda_0|\leqslant \left \|A\right \|_M$.
\end{Proof}\\\\
\textit{$\bullet$ Если векторная норма $\left \|\cdot\right \|_B\in P_{n,1}$, то отображение, ставящее каждой матрице $A\in P_{n,m}$ в соответствие число $\left \|A\right \|_i = \underset{\left \|X\right \|_B = 1}{\sup} \left \|AX\right \|_B$ является матричной нормой, согласованной с векторной $\left \|\cdot\right \|_B$ называется нормой, \textbf{индуцированной} векторной нормой $\left \|\cdot\right \|_B$.}
\newtheorem*{th14_10_2}{Теорема}\begin{th14_10_2}Любая матричная норма $\left \|\cdot\right \|_M$, согласованная с некоторой векторной нормой $\left \|\cdot\right \|_B$, мажорирует норму $\left \|\cdot\right \|_i$, индуцированную этой векторной нормой, то есть $\forall A\in P_{n,n}\quad \left \|A\right \|_i\leqslant \left \|A\right \|_M$.
\end{th14_10_2}\begin{Proof}
	$\left \|A\right \|_i = \underset{\left \|X\right \|_B = 1}{sup} \left \|AX\right \|_B \leqslant \underset{\left \|X\right \|_B = 1}{\sup} \left \|AX\right \|_M \left \|X\right \|_B = \left \|A\right \|_M$.
\end{Proof}





\section{Псевдорешение линейной системы.}
Рассмотрим линейную систему $$\qquad\qquad AX = B,\ A\in \mathbb{R}_{m,n}, B\in \mathbb{R}_{m,1}, X = \begin{pmatrix}
	x_1\\\vdots\\x_n
\end{pmatrix}.\eqno(14.11.1)$$
Обозначим столбцы матрицы $A$ через $A_i\in\mathbb{R}_{m,1}$. Тогда система (14.11.1) может быть записана в виде $$A_1x_1 + \ldots +A_nx_n = B.$$
Столбцы $A_i$ и $B$ являются элементами пространства $\mathbb{R}_{m,1}$. Определим в этом пространстве скалярное произведение следующим образом: если $X = \begin{pmatrix}
	x_1\\\vdots\\x_m
\end{pmatrix}, Y = \begin{pmatrix}
	y_1\\\vdots\\y_m
\end{pmatrix}$, то $(X,Y) = x_1y_1 + \ldots + x_m y_m = X^TY$.
\newtheorem*{lem14_11_1}{Лемма}\begin{lem14_11_1}Пусть $L=L(A_1,\dots,A_n)$. Тогда $\forall C\in L^\perp,\ A^T C = 0$
\end{lem14_11_1}\begin{Proof}
	$i$-й эл-т $A^TC = (A_i^T,C) = 0$
\end{Proof}\\\\
$\bullet$ \textit{Пусть $y = (y_1,\dots,y_n)\in \mathbb{R}$ --- произвольный элемент пространства $\mathbb{R}^n$. Столбец $d(y) = A_1y_1 + \ldots + A_ny_n - B$ называется \textbf{невязкой} последовательности $y$ для системы $(14.11.1)$.}\\\\
Если последовательность $y\in \mathbb{R}^n$ является решением системы (14.11.1), то ее невязка равна нулевому столбцу.\\\\
Пусть система (14.11.1) несовместна.\\\\
$\bullet$ \textit{\textbf{Псевдорешением} несовместной системы $(14.11.1)$ называется последовательность $y \in \mathbb{R}^n$ имеющая невязку наименьшей длины.}
\newtheorem*{th14_11_1}{Теорема}\begin{th14_11_1}Пусть $L = L(A_1,\dots,A_n),$ $B_1$ --- ортогональная проекция столбца $B$ на подпространство $L$. Тогда множество псевдорешений системы $(14.11.1)$ совпадает с множеством решений системы $$A_1x_1 + \ldots + A_n x_n = B_1.\eqno(14.11.2)$$
\end{th14_11_1}\begin{Proof}
	Так как $B_1$ --- ортогональная проекция столбца $B$ на подпространство $L$, то $B_1\in L$ $\Rightarrow$ столбец $B_1$ представим в виде $B_1 = y_1A_1 + \ldots + y_nA_n$. Следовательно, последовательность, состоящая из коэффициентов этой линейной комбинации, является решением системы (14.11.2), значит, система (14.11.2) совместна.\\\\
	С другой стороны, так как $B_1$ --- ортогональная проекция $B$ на $L$, то столбец $B$ представим в виде $B = \underset{\in L}{B_1} + \underset{\in L^\perp}{B_2}$ $\Rightarrow$ для любой последовательности $y = (y_1,\dots,y_n)$ найдем длину: $|d(y)|^2 = |\underbrace{A_1y_1 + \ldots + A_ny_n -B_1}_{\in L} - \underbrace{B_2}_{\in L^\perp}|^2$ = [по теореме Пифагора] = $|\underbrace{A_1y_1 + \ldots + A_ny_n -B_1}_{\geqslant0}|^2 + |B_2|^2 \geqslant |B_2|^2$, причем равенство возможно, если $A_1y_1 + \ldots + A_ny_n -B_1 = 0$. То есть последовательность $y$ является решением системы (14.11.2). Следовательно, множество псевдорешений системы (14.11.1) совпадает с множеством решений системы (14.11.2).
\end{Proof}
\newtheorem*{th14_11_2}{Теорема}\begin{th14_11_2}Множество псевдорешений системы $(14.11.1)$ совпадает с множеством решений системы $$A^TAX = A^TB. \eqno(14.11.3)$$
\end{th14_11_2}\begin{Proof}
	Покажем, что системы (14.11.2) и (14.11.3) равносильны.\begin{enumerate}
		\item Пусть $x = (x_1,\dots,x_n)$ --- решение системы (14.11.2). Тогда $A_1x_1 + \ldots + A_nx_n = B_1$ домножим на $A^T$ и получим $A^T(A_1x_1 + \ldots + A_nx_n) = A^TB_1.$ Так как $B_1$ --- проекция $B$ на $L$, то $B = \underset{\in L}{B_1} + \underset{\in L^\perp}{B_2}\Rightarrow A^TB = A^T(B_1 + B_2) = A^TB_1 +  \underset{=0}{A^TB_2}\Rightarrow x$ --- решение системы (14.11.3).
		\item Пyсть $x = (x_1,\dots,x_n)$ --- решение системы (14.11.3) $\Rightarrow A^T(A_1x_1+ \ldots +A_nx_n) = A^TB = A^T(B_1+B_2)=A^TB_1 +  \underset{=0}{A^TB_2}$ $\Rightarrow$ $A^T(A_1x_1 + \ldots + A_nx_n - B_1) = 0$ $\Rightarrow$ $i$-й элемент столбца слева из последнего равенства имеет вид $\underbrace{A_i^T(A_1x_1 + \ldots + A_nx_n - B_1) = 0}_{\text{скалярное произведение}}$. Следовательно, столбец $(A_1x_1 + \ldots + A_nx_n - B_1)$ диагонален $\forall A_i$, а значит ортогонален любому вектору из $L$ $\Rightarrow$ $(A_1x_1 + \ldots + A_nx_n - B_1)\in L^\perp$.\\\\
		 С другой стороны, так как этот столбец является линейной комбинацией векторов из $L$, то он принадлежит подпространству $L$. Отсюда следует, что $(A_1x_1 + \ldots + A_nx_n - B_1)\in L\cap L^\perp = \{0\}\Rightarrow A_1x_1 + \ldots + A_nx_n - B_1 = 0\Rightarrow x$ --- решение системы (14.11.2)
	\end{enumerate}
\end{Proof}
\newtheorem*{cor14_11_1}{Следствие}\begin{cor14_11_1}Если система $AX = B$ совместна, то она равносильна системе $A^TAX = A^TB$.
\end{cor14_11_1}\begin{Proof}
	По теореме решения системы (14.11.3) имеют наименьшую невязку относительно системы (14.11.1). Следовательно, если система (14.11.1) совместна, то наименьшую невязку для системы (14.11.1) имеют решения этой системы.
\end{Proof}
\newtheorem*{cor14_11_2}{Следствие}\begin{cor14_11_2}Системы $AX = 0$ и $A^T AX = 0$ равносильны.
\end{cor14_11_2}





\section{Нормальное псевдорешение линейной системы.}
Рассмотрим несовместную линейную систему $$A_1x_1 + \ldots + A_n x_n = B,\ A_i, \in \mathbb{R}_{m,n}, B\in\mathbb{R}_{m,1} \eqno (14.12.1)$$ Обозначим $L_0 \subseteq\mathbb{R}^n$ --- пространство решений приведенной системы $AX = 0$. Определим в этом пространстве скалярное произведение следующим образом: если $x = (x_1,\dots,x_n), y = (y_1,\dots,y_n)$, то $xy = x_1 y_1 + \ldots + x_n y_n$.\\\\
$\bullet$\textit{ \textbf{Нормальным псевдорешением системы} системы $(14.12.1)$ называется её псевдорешение, имеющее наименьшую длину.}
\newtheorem*{th12_1}{Теорема}\begin{th12_1}Нормальное псевдорешение системы $(14.12.1)$ определяется однозначно и равно ортогональной составляющей произвольного псевдорешения системы $(14.12.1)$ относительно подпространства $L_0$.\end{th12_1}\begin{Proof}
	Покажем, что ортогональные составляющие псевдорешений системы (14.12.1) относительно $L_0$ равны.
	Пусть $L_p$ --- множество псевдорешений системы (14.12.1), тогда $L_p$ --- множество решений системы $A_1x_1 + \ldots + A_n x_n = B_1\ (2)$, где столбец $B_1$ --– ортогнальная проекция столбца $B$ на $L = L(A_1, \dots , A_n) \Rightarrow L_p = \{x_0\} + L_0$, где $x_0 $ --- частное решение системы (2), которые является псевдорешением системы (14.12.1). Следовательно, произвольное псевдорешение системы (14.12.1) $x=x_0 + y$, где $y\in L_0$.\\\\
	Обозначим через $x_1$ ортогональную проекцию, а через $x_2$ --- ортогональную составляющую последовательности $x$ на множестве $L_0\subset \mathbb{R}^n$. Тогда $x$ представимо в виде: $x = \underset{\in L_0}{x_1} + \underset{\in L_0}{x_2}\Rightarrow x= x_1 + x_2 + y = \underbrace{(x_1 + y)}_{\in L_0} + \underset{\in L_0}{x_2} \Rightarrow x_2$ --- ортогональная составляющая $x$ относительно $L_0\Rightarrow$ все псевдорешения имеют ортогональную составляющую равную $x_2$.\\\\
	Покажем, что $x_2$ --- нормальное псевдорешение системы (14.12.1).
	Так как $x_1\in L_0 \subset \mathbb{R}^n$, то $-x_1\in L_0$. Следовательно, среди псевдорешений системы (14.12.1) есть последовательность $x = x_0 + y = x_0 - x_1 = (x_1 + x_2)-x_1 = x_2 \Rightarrow x_2 $ --- псевдорешение системы (14.12.1). При этом любое другое псевдорешение системы (14.12.1) представимо в виде: $x' = x_0 + y'$, где $y' \ne -x_1$. Тогда $|x'|^2 = |x_0 + y'|^2 = |(x_1 + x_2) + y'|^2 = |(x_1 + y) + x_2'|^2 =$ [ по теореме Пифагора ] $=|x_1+y'|^2 + |x_2|^2 > |x_2|^2\Rightarrow x_2$ --- нормальное псевдорешние системы (14.12.1), причем единственное.
\end{Proof}
\newtheorem*{cor3_12_1}{Следствие}\begin{cor3_12_1}Нормальное псевдорешение системы $(14.12.1)$ является единственным псевдорешением, которое принадлежит пространству $L_0^{\perp}$.\end{cor3_12_1}
$\bullet$\textit{ Матрица, столбцы которой являются нормальными псевдорешениями систем $AX = E_i$, где $E_i$ --- $i$-ый столбец единичной матрицы, называется \textbf{псевдообратной матрицей} для матрицы $A$.}  (Обозначение: $A^+$)\\\\
Из определения следует, что если $A\in \mathbb{R}_{m,n}$, то $A^+\in \mathbb{R}_{n,m}$ (Если $A$ невырожденная, то $A^+ = A^{-1})$
\newtheorem*{th12_2}{Теорема}\begin{th12_2} Нормальное псевдорешение системы $(14.12.1)$ равно $A^+B$. \end{th12_2}\begin{Proof}
	Пусть $B= \begin{pmatrix}
		\beta_1\\\vdots\\\beta_m
	\end{pmatrix} \Rightarrow AX = \beta_1E_1 + \ldots + \beta_mE_m$.\\ Обозначим через $C_1,\dots, C_m$ столбцы псевдообратной матрицы $A^+$. Тогда $C_i$ --- нормальное псевдорешение системы $AX$ $=$ $E_i$. Следовательно, $C_i$ --- решение системы $A^TAX  =  A^TE_i \Rightarrow$ $A^TAC_i$ $ = $ $A^TE_i \Rightarrow$ $A^TA$($\beta_1C_1$ + $\dots \beta_mC_m$) $ = $ $A^\perp$($\beta_1E_1$ + $\dots$ +$\beta_mE_m$) $\Rightarrow A^+B$ --- псевдорешение системы (14.12.1). При этом, так как $C_i$ --- нормальное псевдорешение, то $C_i\in L_0^\perp\Rightarrow A^+B = \beta_1C_1 + \dots \beta_mC_m \in L^\perp_0 \Rightarrow A^+B$ --- нормальное псевдорешение системы (14.12.1).
\end{Proof}